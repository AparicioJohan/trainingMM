[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This webpage is under development.\nOur goal is to build a collection of resources that use R code and mathematical explanations to make concepts in statistical modeling easier to understand.\nWe will begin with the basics of linear models, and progressively extend toward more advanced mixed models, which are widely applied in plant breeding, animal breeding, and many other fields.\n\n\n\nHands-on examples: R scripts that you can run and adapt.\n\nStep-by-step math: showing how model equations connect to code.\n\nConceptual explanations: to make sense of linear models and linear mixed models.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Authors",
    "section": "",
    "text": "Welcome! We’re building resources to make mixed models and linear models easier to learn—code, math, and practical examples.\n\n\n\n\nJohan Steven Aparicio\n\n\nGraduate Student\n\n\nUniversity of Wisconsin–Madison\n\n\n   GitHub \n\n\n\n\n\nAlejandro Domínguez Rondón\n\n\nGraduate Student\n\n\nUniversidad Politécnica de Madrid\n\n\n   GitHub",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html",
    "href": "pages/00_start_lm.html",
    "title": "Understanding a linear model",
    "section": "",
    "text": "With this article we want to analyze a randomized complete block design (RCBD) to show how a traditional full fixed effects model and ANOVA work.",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html#setup-data",
    "href": "pages/00_start_lm.html#setup-data",
    "title": "Understanding a linear model",
    "section": "1) Setup & data",
    "text": "1) Setup & data\nClewer and Scarisbrick (2001) present a yield trial (t/ha) conducted using a randomized complete block design. The design included 3 blocks and 4 cultivars, resulting in 12 experimental plots.\n\nlibrary(tidyverse)\nlibrary(emmeans)\nlibrary(gt)\nlibrary(kableExtra)\n\n# Read and coerce factors\ndata &lt;- read.csv(\"../data/example_1.csv\") |&gt;\n  mutate(gen = as.factor(gen), block = as.factor(block))\nhead(data)\n\n  block gen yield\n1     1  g1   7.4\n2     2  g1   6.5\n3     3  g1   5.6\n4     1  g2   9.8\n5     2  g2   6.8\n6     3  g2   6.2\n\nstr(data)\n\n'data.frame':   12 obs. of  3 variables:\n $ block: Factor w/ 3 levels \"1\",\"2\",\"3\": 1 2 3 1 2 3 1 2 3 1 ...\n $ gen  : Factor w/ 4 levels \"g1\",\"g2\",\"g3\",..: 1 1 1 2 2 2 3 3 3 4 ...\n $ yield: num  7.4 6.5 5.6 9.8 6.8 6.2 7.3 6.1 6.4 9.5 ...\n\n\n\nQuick visualization\n\ndata |&gt;\n  ggplot(aes(x = gen, y = yield, color = block)) +\n  geom_point(size = 3) +\n  theme_classic(base_size = 15)\n\n\n\n\nYield by genotype colored by block.",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html#linear-model-building-blocks",
    "href": "pages/00_start_lm.html#linear-model-building-blocks",
    "title": "Understanding a linear model",
    "section": "2) Linear model building blocks",
    "text": "2) Linear model building blocks\nWe will progressively build the RCBD model using model.frame() and model.matrix() to see the design matrices explicitly, then solve normal equations. We use:\n\n\\(\\boldsymbol{y} = \\boldsymbol{X\\beta} + \\boldsymbol{\\varepsilon}\\): linear model\n\\(\\boldsymbol{y}\\): response vector (yield)\n\\(\\boldsymbol{X}\\): model matrix\n\\(\\boldsymbol{\\varepsilon} \\sim MVN(\\boldsymbol{0}, \\boldsymbol{I}\\sigma^2)\\)\n\\(E(\\boldsymbol{y}) = \\boldsymbol{X\\beta}; \\; V(\\boldsymbol{y}) = \\boldsymbol{I}\\sigma^2\\)\n\\(\\boldsymbol{\\hat\\beta} = (\\boldsymbol{X^\\top X})^{-1} \\boldsymbol{X^\\top} \\boldsymbol{y}\\)\n\\(V(\\boldsymbol{\\hat{\\beta}}) = (\\boldsymbol{X^\\top X})^{-1} \\sigma^2\\)\nFitted values \\(\\boldsymbol{\\hat y} = \\boldsymbol{X\\hat\\beta}\\)\nErrors \\(\\boldsymbol{\\varepsilon} = \\boldsymbol{y} - \\boldsymbol{\\hat y}\\)\nSum of squared errors \\(\\text{SSE} = \\boldsymbol{\\varepsilon^\\top \\varepsilon}\\)\n\nLet n = 12 observations, n_blks = 3 blocks, n_gens = 4 genotypes.\n\nn &lt;- 12\nn_blks &lt;- 3\nn_gens &lt;- 4\n\n\n2.1 Intercept-only model (overall mean)\n\nMatricesCode\n\n\n\nModel:\n\n\\[\n\\small\n\\boldsymbol{y} = \\boldsymbol{X\\beta} + \\boldsymbol{\\varepsilon} =&gt;\n\\begin{bmatrix}\n7.4 \\\\\n6.5 \\\\\n5.6 \\\\\n9.8 \\\\\n6.8 \\\\\n6.2 \\\\\n7.3 \\\\\n6.1 \\\\\n6.4 \\\\\n9.5 \\\\\n8.0 \\\\\n7.4 \\\\\n\\end{bmatrix} =\n\\overset{\\text{mean}}{\n\\begin{bmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1\n\\end{bmatrix}}\n\\mu + \\boldsymbol{\\varepsilon}\n\\]\n\nBLUE(s):\n\n\\[\n\\small\n\\boldsymbol{\\beta} =\n\\underbrace{\\left(\\begin{bmatrix}\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n\\end{bmatrix}\\right)^{-1}}_{(\\boldsymbol{X'X})^{-1}}\n\\underbrace{\\begin{bmatrix}\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n\\end{bmatrix}\n\\begin{bmatrix}\n7.4 \\\\\n6.5 \\\\\n5.6 \\\\\n9.8 \\\\\n6.8 \\\\\n6.2 \\\\\n7.3 \\\\\n6.1 \\\\\n6.4 \\\\\n9.5 \\\\\n8.0 \\\\\n7.4 \\\\\n\\end{bmatrix}}_{\\boldsymbol{X'y}} =\n\\underbrace{\\frac{1}{12}}_{(\\boldsymbol{X'X})^{-1}}\\cdot \\underbrace{87}_{\\boldsymbol{X'y}} = 7.25\n\\]\n\n\n\n\n\n\nNote\n\n\n\nNotice that the first term, \\(\\left(\\boldsymbol{X'X}\\right)^{-1}\\), corresponds to the inverse of number of observations, while the second term, \\(\\boldsymbol{X'y}\\), gives the sum of phenotypic values.\n\n\nIn this example, there is only a single level, \\(\\mu\\). Therefore, the entire expression simplifies to the sum of the phenotypic values divided by the number of observations, which is simply the mean, as shown below:\n\ndata.frame(mean = \"mean\", beta = mean(data$yield)) |&gt;\n  gt()\n\n\n\n\n\n\n\nmean\nbeta\n\n\n\n\nmean\n7.25\n\n\n\n\n\n\n\n\n\n\nff &lt;- yield ~ 1\nm &lt;- model.frame(ff, data)\nX &lt;- model.matrix(ff, m)\ny &lt;- matrix(data$yield)\n\n# Normal equations components\nXty &lt;- t(X) %*% y\nXtX &lt;- t(X) %*% X\nrank_X &lt;- qr(XtX)$rank\nXtX_inv &lt;- solve(XtX)\n\nbeta_mu &lt;- XtX_inv %*% Xty # overall mean (mu)\ny_hat &lt;- X %*% beta_mu\nerrors &lt;- y - y_hat\nSSE_mu &lt;- t(errors) %*% errors\nSSE_mu &lt;- as.numeric(SSE_mu)\n\nlist(rank = rank_X, beta_mu = drop(beta_mu), SSE_mu = SSE_mu)\n\n$rank\n[1] 1\n\n$beta_mu\n(Intercept) \n       7.25 \n\n$SSE_mu\n[1] 18.81\n\n\n\n\n\n\n\n2.2 Add blocks\nWe will now illustrate what happens if we only include the block factor.\n\nMatricesCode\n\n\n\nModel: \\[\n\\small\n\\boldsymbol{y} = \\boldsymbol{X\\beta} + \\boldsymbol{\\varepsilon} =&gt;\n\\begin{bmatrix}\n7.4 \\\\\n6.5 \\\\\n5.6 \\\\\n9.8 \\\\\n6.8 \\\\\n6.2 \\\\\n7.3 \\\\\n6.1 \\\\\n6.4 \\\\\n9.5 \\\\\n8.0 \\\\\n7.4 \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\overset{\\text{Block 1}}{1} & \\overset{\\text{Block 2}}{0} & \\overset{\\text{Block 3}}{0} \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\nb_3 \\\\\n\\end{bmatrix} + \\boldsymbol{\\varepsilon}\n\\]\nBLUE(s): \\[\n\\small\n\\boldsymbol{\\beta} =\n\\underbrace{\\left(\\begin{bmatrix}\n1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 1 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 1 \\ 0 \\ 0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\\\\n1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\\\\n1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\\\\n1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\\\\n\\end{bmatrix}\\right)^{-1}}_{(\\boldsymbol{X'X})^{-1}}\n\\underbrace{\\begin{bmatrix}\n1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 1 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 1 \\ 0 \\ 0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n7.4 \\\\\n6.5 \\\\\n5.6 \\\\\n9.8 \\\\\n6.8 \\\\\n6.2 \\\\\n7.3 \\\\\n6.1 \\\\\n6.4 \\\\\n9.5 \\\\\n8.0 \\\\\n7.4 \\\\\n\\end{bmatrix}}_{\\boldsymbol{X'y}} =\n\\underbrace{\\begin{bmatrix}\n\\frac{1}{4} \\ 0 \\ 0 \\\\\n0 \\ \\frac{1}{4} \\ 0 \\\\\n0 \\ 0 \\ \\frac{1}{4} \\\\\n\\end{bmatrix}}_{(\\boldsymbol{X'X})^{-1}}\n\\underbrace{\\begin{bmatrix}\n34 \\\\\n27.4 \\\\\n25.6 \\\\\n\\end{bmatrix}}_{\\boldsymbol{X'y}} =\n\\begin{bmatrix}\n8.50 \\\\\n6.85 \\\\\n6.40 \\\\\n\\end{bmatrix}\n\\]\n\nIn this model, there are 3 levels (\\(Block1\\), \\(Block2\\) and \\(Block3\\)). Therefore, the entire expression simplifies to the sum of the phenotypic values divided by the number of observations, which is simply the mean per block.\n\ndata |&gt;\n  group_by(block) |&gt;\n  summarise(beta = mean(yield, na.rm = TRUE)) |&gt;\n  gt()\n\n\n\n\n\n\n\nblock\nbeta\n\n\n\n\n1\n8.50\n\n\n2\n6.85\n\n\n3\n6.40\n\n\n\n\n\n\n\n\n\n\nff &lt;- yield ~ -1 + block\nm &lt;- model.frame(ff, data)\nX &lt;- model.matrix(ff, m)\ny &lt;- matrix(data$yield)\n\nXty &lt;- t(X) %*% y\nXtX &lt;- t(X) %*% X\nrank_X &lt;- qr(XtX)$rank\nXtX_inv &lt;- solve(XtX)\n\nbeta_blk &lt;- XtX_inv %*% Xty\nSSE_blk &lt;- t(y - X %*% beta_blk) %*% (y - X %*% beta_blk)\nSSE_blk &lt;- as.numeric(SSE_blk)\n\nlist(rank = rank_X, beta_blk = drop(beta_blk), SSE_blk = SSE_blk)\n\n$rank\n[1] 3\n\n$beta_blk\nblock1 block2 block3 \n  8.50   6.85   6.40 \n\n$SSE_blk\n[1] 9.03\n\n\n\n\n\n\n\n2.3 Add genotypes\n\nMatricesCode\n\n\n\nModel:\n\n\\[\n\\small\n\\boldsymbol{y} = \\boldsymbol{X\\beta} + \\boldsymbol{\\varepsilon} =&gt;\n\\begin{bmatrix}\n7.4 \\\\\n6.5 \\\\\n5.6 \\\\\n9.8 \\\\\n6.8 \\\\\n6.2 \\\\\n7.3 \\\\\n6.1 \\\\\n6.4 \\\\\n9.5 \\\\\n8.0 \\\\\n7.4 \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\overset{gen1}{1} & \\overset{gen2}{0} & \\overset{gen3}{0} & \\overset{gen4}{0} \\\\\n1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\ng_1 \\\\\ng_2 \\\\\ng_3 \\\\\ng_4 \\\\\n\\end{bmatrix} + \\boldsymbol{\\varepsilon}\n\\]\n\nBLUE(s):\n\n\\[\n\\small\n\\boldsymbol{\\beta} =\n\\underbrace{\n\\left(\\begin{bmatrix}\n1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\ 0 \\ 0 \\ 0 \\\\\n1 \\ 0 \\ 0 \\ 0 \\\\\n1 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 1 \\\\\n0 \\ 0 \\ 0 \\ 1 \\\\\n0 \\ 0 \\ 0 \\ 1 \\\\\n\\end{bmatrix}\\right)^{-1}}_{(\\boldsymbol{X'X})^{-1}}\n\\underbrace{\n\\begin{bmatrix}\n1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n7.4 \\\\\n6.5 \\\\\n5.6 \\\\\n9.8 \\\\\n6.8 \\\\\n6.2 \\\\\n7.3 \\\\\n6.1 \\\\\n6.4 \\\\\n9.5 \\\\\n8.0 \\\\\n7.4 \\\\\n\\end{bmatrix}}_{\\boldsymbol{X'y}} =\n\\underbrace{\\begin{bmatrix}\n\\frac{1}{3}\\ 0 \\ 0 \\ 0 \\\\\n0 \\ \\frac{1}{3} \\ 0 \\ 0 \\\\\n0 \\ 0 \\ \\frac{1}{3} \\ 0 \\\\\n0 \\ 0 \\ 0 \\ \\frac{1}{3} \\\\\n\\end{bmatrix}}_{(\\boldsymbol{X'X})^{-1}}\n\\underbrace{\n\\begin{bmatrix}\n19.5 \\\\\n22.8 \\\\\n19.8 \\\\\n24.9 \\\\\n\\end{bmatrix}\n}_{\\boldsymbol{X'y}} =\n\\begin{bmatrix}\n6.5 \\\\\n7.6 \\\\\n6.6 \\\\\n8.3 \\\\\n\\end{bmatrix}\n\\]\nIn this model, there are 4 levels (\\(g1\\), \\(g2\\), \\(g3\\) and \\(g4\\)). Therefore, the entire expression simplifies to the sum of the phenotypic values divided by the number of observations, which is simply the mean per genotype.\n\ndata |&gt;\n  group_by(gen) |&gt;\n  summarise(beta = mean(yield, na.rm = TRUE)) |&gt;\n  gt()\n\n\n\n\n\n\n\ngen\nbeta\n\n\n\n\ng1\n6.5\n\n\ng2\n7.6\n\n\ng3\n6.6\n\n\ng4\n8.3\n\n\n\n\n\n\n\n\n\n\nff &lt;- yield ~ -1 + gen\nm &lt;- model.frame(ff, data)\nX &lt;- model.matrix(ff, m)\ny &lt;- matrix(data$yield)\n\nXty &lt;- t(X) %*% y\nXtX &lt;- t(X) %*% X\nrank_X &lt;- qr(XtX)$rank\nXtX_inv &lt;- solve(XtX)\n\nbeta_gen &lt;- XtX_inv %*% Xty\nSSE_gen &lt;- t(y - X %*% beta_gen) %*% (y - X %*% beta_gen)\nSSE_gen &lt;- as.numeric(SSE_gen)\n\nm2 &lt;- list(rank = rank_X, beta_gen = drop(beta_gen), SSE_gen = SSE_gen)\n\n\n\n\n\n\n2.4 Full model: intercept + blocks + genotypes\n\nMatricesCode\n\n\n\nModel:\n\n\\[\n\\small\n\\boldsymbol{y} = \\boldsymbol{X\\beta} + \\boldsymbol{\\varepsilon} \\;\\;\\;\\Rightarrow\\;\\;\\;\n\\begin{bmatrix}\n7.4 \\\\\n6.5 \\\\\n5.6 \\\\\n9.8 \\\\\n6.8 \\\\\n6.2 \\\\\n7.3 \\\\\n6.1 \\\\\n6.4 \\\\\n9.5 \\\\\n8.0 \\\\\n7.4 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\overset{\\text{intercept}}{1} &\n\\overset{\\text{block2}}{0} &\n\\overset{\\text{block3}}{0} &\n\\overset{\\text{gen2}}{0} &\n\\overset{\\text{gen3}}{0} &\n\\overset{\\text{gen4}}{0} \\\\\n1 & 1 & 0 & 0 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 1 & 0 \\\\\n1 & 1 & 0 & 0 & 1 & 0 \\\\\n1 & 0 & 1 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 0 & 0 & 1 \\\\\n1 & 1 & 0 & 0 & 0 & 1 \\\\\n1 & 0 & 1 & 0 & 0 & 1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mu \\\\\nb_2 \\\\\nb_3 \\\\\ng_2 \\\\\ng_3 \\\\\ng_4 \\\\\n\\end{bmatrix} + \\boldsymbol{\\varepsilon}\n\\]\n\nBLUE(s):\n\n\\[\n\\small\n\\begin{align*}\n\\boldsymbol{\\beta} &=\n\\underbrace{\n\\left(\n\\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 0 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 1 & 0 \\\\\n1 & 1 & 0 & 0 & 1 & 0 \\\\\n1 & 0 & 1 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 0 & 0 & 1 \\\\\n1 & 1 & 0 & 0 & 0 & 1 \\\\\n1 & 0 & 1 & 0 & 0 & 1\n\\end{bmatrix}\n\\right)^{-1}\n}_{(\\boldsymbol{X'X})^{-1}}\n\\underbrace{\n\\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n7.4 \\\\\n6.5 \\\\\n5.6 \\\\\n9.8 \\\\\n6.8 \\\\\n6.2 \\\\\n7.3 \\\\\n6.1 \\\\\n6.4 \\\\\n9.5 \\\\\n8.0 \\\\\n7.4\n\\end{bmatrix}\n}_{\\boldsymbol{X'y}} \\\\\n&=\n\\underbrace{\n\\begin{bmatrix}\n\\frac{1}{2} & \\frac{-1}{4} & \\frac{-1}{4} & \\frac{-1}{3} & \\frac{-1}{3} & \\frac{-1}{3} \\\\\n\\frac{-1}{4} & \\frac{1}{2} & \\frac{1}{4} & 0 & 0 & 0 \\\\\n\\frac{-1}{4}  & \\frac{1}{4} & \\frac{1}{2} & 1 & 1 & 1 \\\\\n\\frac{-1}{3}  & 0 & 0 & \\frac{2}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\\n\\frac{-1}{3}  & 0 & 0 & \\frac{1}{3} & \\frac{2}{3} & \\frac{1}{3} \\\\\n\\frac{-1}{3}  & 0 & 0 & \\frac{1}{3} & \\frac{1}{3} & \\frac{2}{3}\n\\end{bmatrix}\n}_{(\\boldsymbol{X'X})^{-1}}\n\\underbrace{\n\\begin{bmatrix}\n87.0 \\\\\n27.4 \\\\\n25.6 \\\\\n22.8 \\\\\n19.8 \\\\\n24.9\n\\end{bmatrix}\n}_{\\boldsymbol{X'y}}\n=\n\\begin{bmatrix}\n7.75 \\\\\n-1.65 \\\\\n-2.10 \\\\\n1.10 \\\\\n0.10 \\\\\n1.80\n\\end{bmatrix}\n\\end{align*}\n\\]\n\n\n\nff &lt;- yield ~ 1 + block + gen\nm &lt;- model.frame(ff, data)\nX &lt;- model.matrix(ff, m)\ny &lt;- matrix(data$yield)\n\nXty &lt;- t(X) %*% y\nXtX &lt;- t(X) %*% X\nrank_X &lt;- qr(XtX)$rank\nXtX_inv &lt;- solve(XtX)\n\nbeta &lt;- XtX_inv %*% Xty\nSSE &lt;- t(y - X %*% beta) %*% (y - X %*% beta)\nSSE &lt;- as.numeric(SSE)\n\nlist(rank = rank_X, betas = drop(beta), SSE = SSE)\n\n$rank\n[1] 6\n\n$betas\n(Intercept)      block2      block3       geng2       geng3       geng4 \n       7.75       -1.65       -2.10        1.10        0.10        1.80 \n\n$SSE\n[1] 2.4\n\n\n\n\n\nNotice that the first level of each factor (baseline) is dropped in order to have a full rank \\(\\boldsymbol{X}\\) matrix.\n\n\n2.5 Coefficients and variance-covariances\n\\[ \\boldsymbol{\\hat\\beta} = (\\boldsymbol{X^\\top X})^{-1} \\boldsymbol{X^\\top} \\boldsymbol{y} \\] \\[V(\\boldsymbol{\\hat{\\beta}}) = (\\boldsymbol{X^\\top X})^{-1} \\sigma^2\\]\n\nsigma_2 &lt;- SSE / (n - length(beta))\nvcov_betas &lt;- XtX_inv * sigma_2\nvcov_betas\n\n            (Intercept) block2 block3      geng2      geng3      geng4\n(Intercept)   0.2000000   -0.1   -0.1 -0.1333333 -0.1333333 -0.1333333\nblock2       -0.1000000    0.2    0.1  0.0000000  0.0000000  0.0000000\nblock3       -0.1000000    0.1    0.2  0.0000000  0.0000000  0.0000000\ngeng2        -0.1333333    0.0    0.0  0.2666667  0.1333333  0.1333333\ngeng3        -0.1333333    0.0    0.0  0.1333333  0.2666667  0.1333333\ngeng4        -0.1333333    0.0    0.0  0.1333333  0.1333333  0.2666667\n\n\n\ndata.frame(\n  coefficient = rownames(vcov_betas),\n  solution = beta,\n  variance = diag(vcov_betas),\n  std.error = sqrt(diag(vcov_betas)),\n  row.names = NULL\n) |&gt; gt()\n\n\n\n\n\n\n\ncoefficient\nsolution\nvariance\nstd.error\n\n\n\n\n(Intercept)\n7.75\n0.2000000\n0.4472136\n\n\nblock2\n-1.65\n0.2000000\n0.4472136\n\n\nblock3\n-2.10\n0.2000000\n0.4472136\n\n\ngeng2\n1.10\n0.2666667\n0.5163978\n\n\ngeng3\n0.10\n0.2666667\n0.5163978\n\n\ngeng4\n1.80\n0.2666667\n0.5163978\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLet’s be careful when interpreting the coefficients and the variance of these factor coefficients. In a linear model, when you include a categorical factor like “genotype”, the model needs a baseline. In R it chooses g1 as the reference. Therefore:\n\nThe coefficient for g2 is the estimated difference between g2 and g1 (7.6 - 6.5 = 1.1).\nThe variance for the g2 coefficient is the uncertainty around that difference (g2 - g1).",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html#reduction-in-ss-logic-and-anova-table-by-hand",
    "href": "pages/00_start_lm.html#reduction-in-ss-logic-and-anova-table-by-hand",
    "title": "Understanding a linear model",
    "section": "3) Reduction-in-SS logic and ANOVA table (by hand)",
    "text": "3) Reduction-in-SS logic and ANOVA table (by hand)\nThe partial sums of squares for block and gen can be obtained as reductions from the intercept-only model:\n\nSSE_mu               # intercept-only\n\n[1] 18.81\n\nSSE_mu - SSE_blk     # SS for blocks\n\n[1] 9.78\n\nSSE_mu - SSE_gen     # SS for genotypes\n\n[1] 6.63\n\nSSE                  # residual SS from full model\n\n[1] 2.4\n\n\nWe now assemble an ANOVA table. Residual df is n - p, where p is the number of coefficients in the full model.\n\n# Sigma^2 estimate using the full model (p = length(beta))\nsigma_2 &lt;- SSE / (n - length(beta))\nsigma_2\n\n[1] 0.4\n\nanova_dt &lt;- data.frame(\n  Source = c(\"block\", \"gen\", \"residuals\"),\n  Df     = c(n_blks - 1, n_gens - 1, n - length(beta)),\n  SSq    = c(SSE_mu - SSE_blk, SSE_mu - SSE_gen, SSE)\n) |&gt;\n  mutate(\n    MSq    = SSq / Df,\n    F.value = MSq / MSq[3],\n    `Pr(&gt;F)` = pf(q = F.value, df1 = Df, df2 = Df[3], lower.tail = FALSE),\n    F.value = ifelse(Source == \"residuals\", NA, F.value),\n    `Pr(&gt;F)` = ifelse(Source == \"residuals\", NA, `Pr(&gt;F)`)\n  )\n\nanova_dt\n\n     Source Df  SSq  MSq F.value      Pr(&gt;F)\n1     block  2 9.78 4.89  12.225 0.007650536\n2       gen  3 6.63 2.21   5.525 0.036730328\n3 residuals  6 2.40 0.40      NA          NA\n\n\n::: {.callout-note} Why does this work? In fixed-effects ANOVA, Type-I SS for adding a factor equals the reduction in SSE between nested models. Here we use the intercept-only model as the baseline; adding block or gen reduces SSE by their respective SS. :::",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html#verify-with-lm",
    "href": "pages/00_start_lm.html#verify-with-lm",
    "title": "Understanding a linear model",
    "section": "4) Verify with lm()",
    "text": "4) Verify with lm()\n\nmod &lt;- lm(yield ~ 1 + block + gen, data = data)\nanova(mod)\n\nAnalysis of Variance Table\n\nResponse: yield\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nblock      2   9.78    4.89  12.225 0.007651 **\ngen        3   6.63    2.21   5.525 0.036730 * \nResiduals  6   2.40    0.40                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncoef(mod)\n\n(Intercept)      block2      block3       geng2       geng3       geng4 \n       7.75       -1.65       -2.10        1.10        0.10        1.80 \n\nround(vcov(mod), 5)\n\n            (Intercept) block2 block3    geng2    geng3    geng4\n(Intercept)     0.20000   -0.1   -0.1 -0.13333 -0.13333 -0.13333\nblock2         -0.10000    0.2    0.1  0.00000  0.00000  0.00000\nblock3         -0.10000    0.1    0.2  0.00000  0.00000  0.00000\ngeng2          -0.13333    0.0    0.0  0.26667  0.13333  0.13333\ngeng3          -0.13333    0.0    0.0  0.13333  0.26667  0.13333\ngeng4          -0.13333    0.0    0.0  0.13333  0.13333  0.26667\n\n# Check that (X'X)^{-1} * sigma^2 matches vcov\nround(vcov_betas, 5)\n\n            (Intercept) block2 block3    geng2    geng3    geng4\n(Intercept)     0.20000   -0.1   -0.1 -0.13333 -0.13333 -0.13333\nblock2         -0.10000    0.2    0.1  0.00000  0.00000  0.00000\nblock3         -0.10000    0.1    0.2  0.00000  0.00000  0.00000\ngeng2          -0.13333    0.0    0.0  0.26667  0.13333  0.13333\ngeng3          -0.13333    0.0    0.0  0.13333  0.26667  0.13333\ngeng4          -0.13333    0.0    0.0  0.13333  0.13333  0.26667",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html#interpreting-coefficients-under-treatment-coding",
    "href": "pages/00_start_lm.html#interpreting-coefficients-under-treatment-coding",
    "title": "Understanding a linear model",
    "section": "5) Interpreting coefficients under treatment coding",
    "text": "5) Interpreting coefficients under treatment coding\nWith block and gen as factors and an intercept present, R uses treatment (reference-cell) coding by default. The printed beta therefore contains:\n\nbeta[1]: the intercept (no longer overall mean)\nbeta[2:3]: effects for non-reference blocks\nbeta[4:6]: effects for non-reference genotypes\n\nYou can reconstruct overall mean, genotype cell means, and block means as follows.\n\n# Number of coefficients in full model\nn_coef &lt;- length(beta)\n\n# Overall mean reconstructed from coefficients\nmu_recon &lt;- beta[1] + sum(beta[2:3]) / n_blks + sum(beta[4:6]) / n_gens\n\n# Compare with the intercept-only estimate\nmu_recon - beta_mu[1]\n\n[1] 7.993606e-15\n\n\n\n5.1 Genotype means including the missing (reference) level\n\n# beta currently has: (Intercept), block2, block3, gen2, gen3, gen4\n# Create a named vector for gen effects including the reference level set to 0\nprint(beta)\n\n             [,1]\n(Intercept)  7.75\nblock2      -1.65\nblock3      -2.10\ngeng2        1.10\ngeng3        0.10\ngeng4        1.80\n\ngens &lt;- c(\"geng1\" = 0, beta[4:6, ])\n# Add back the intercept and average block effect\ngens &lt;- beta[1] + sum(beta[2:3]) / n_blks + gens\ngens\n\ngeng1 geng2 geng3 geng4 \n  6.5   7.6   6.6   8.3 \n\n\n\n\n5.2 Block means including the missing (reference) level\n\nprint(beta)\n\n             [,1]\n(Intercept)  7.75\nblock2      -1.65\nblock3      -2.10\ngeng2        1.10\ngeng3        0.10\ngeng4        1.80\n\nblks &lt;- c(\"block1\" = 0, beta[2:3, ])\nblks &lt;- beta[1] + sum(beta[4:6]) / n_gens + blks\nblks\n\nblock1 block2 block3 \n  8.50   6.85   6.40",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html#estimated-marginal-means-and-pairwise-comparisons",
    "href": "pages/00_start_lm.html#estimated-marginal-means-and-pairwise-comparisons",
    "title": "Understanding a linear model",
    "section": "6) Estimated marginal means and pairwise comparisons",
    "text": "6) Estimated marginal means and pairwise comparisons\nemmeans provides adjusted means (marginal over the other factors) and convenient contrasts.\n\n# Genotype adjusted means\nemm_gen &lt;- emmeans(mod, ~gen)\nemm_gen\n\n gen emmean    SE df lower.CL upper.CL\n g1     6.5 0.365  6     5.61     7.39\n g2     7.6 0.365  6     6.71     8.49\n g3     6.6 0.365  6     5.71     7.49\n g4     8.3 0.365  6     7.41     9.19\n\nResults are averaged over the levels of: block \nConfidence level used: 0.95 \n\n# Pairwise genotype comparisons\npairs(emm_gen)\n\n contrast estimate    SE df t.ratio p.value\n g1 - g2      -1.1 0.516  6  -2.130  0.2447\n g1 - g3      -0.1 0.516  6  -0.194  0.9971\n g1 - g4      -1.8 0.516  6  -3.486  0.0486\n g2 - g3       1.0 0.516  6   1.936  0.3066\n g2 - g4      -0.7 0.516  6  -1.356  0.5656\n g3 - g4      -1.7 0.516  6  -3.292  0.0609\n\nResults are averaged over the levels of: block \nP value adjustment: tukey method for comparing a family of 4 estimates \n\n# For a quick hand-check of a simple pairwise SE when balanced:\nsqrt(sigma_2 / 3 + sigma_2 / 3)\n\n[1] 0.5163978\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNotice that the standard errors via \\((\\boldsymbol{X' X})^{-1}\\sigma^2\\) are different to the ones return by emmeans, as expected.",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html#what-you-learned",
    "href": "pages/00_start_lm.html#what-you-learned",
    "title": "Understanding a linear model",
    "section": "7) What you learned",
    "text": "7) What you learned\n\nHow to construct \\(X\\), solve \\((X^\\top X)^{-1} X^\\top y\\), and compute SSE.\nHow reductions in SSE across nested models yield sums of squares for factors.\nHow to rebuild means for reference and non-reference levels under treatment coding.\nHow to validate results with lm() and extract adjusted means and pairwise tests using emmeans.",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html#reproducibility",
    "href": "pages/00_start_lm.html#reproducibility",
    "title": "Understanding a linear model",
    "section": "Reproducibility",
    "text": "Reproducibility\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] kableExtra_1.4.0 emmeans_1.10.7   lubridate_1.9.4  forcats_1.0.0   \n [5] stringr_1.5.1    dplyr_1.1.4      purrr_1.0.2      readr_2.1.5     \n [9] tidyr_1.3.1      tibble_3.3.0     ggplot2_3.5.2    tidyverse_2.0.0 \n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.6         sandwich_3.1-1     generics_0.1.4     xml2_1.3.7        \n [5] stringi_1.8.7      lattice_0.22-6     hms_1.1.3          digest_0.6.37     \n [9] magrittr_2.0.3     evaluate_1.0.3     grid_4.4.2         timechange_0.3.0  \n[13] estimability_1.5.1 RColorBrewer_1.1-3 mvtnorm_1.3-3      fastmap_1.2.0     \n[17] jsonlite_2.0.0     Matrix_1.7-1       survival_3.7-0     multcomp_1.4-28   \n[21] viridisLite_0.4.2  scales_1.4.0       TH.data_1.1-3      codetools_0.2-20  \n[25] cli_3.6.3          rlang_1.1.4        splines_4.4.2      withr_3.0.2       \n[29] yaml_2.3.10        tools_4.4.2        tzdb_0.4.0         coda_0.19-4.1     \n[33] vctrs_0.6.5        R6_2.6.1           zoo_1.8-14         lifecycle_1.0.4   \n[37] htmlwidgets_1.6.4  MASS_7.3-61        pkgconfig_2.0.3    pillar_1.11.0     \n[41] gtable_0.3.6       glue_1.8.0         systemfonts_1.2.3  xfun_0.51         \n[45] tidyselect_1.2.1   rstudioapi_0.17.1  knitr_1.49         xtable_1.8-4      \n[49] farver_2.1.2       htmltools_0.5.8.1  labeling_0.4.3     svglite_2.1.3     \n[53] rmarkdown_2.29     compiler_4.4.2    \n\n\n::: {.callout-tip} Common pitfalls - Mixing character vectors and factors: always coerce to factors for design matrices. - Forgetting that with an intercept, one level per factor is the reference (not shown explicitly in model.matrix()). - Using the wrong residual df: it is n - p where p is the number of estimated coefficients in the full model. :::",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html#reduction-in-ss-and-anova-table-by-hand",
    "href": "pages/00_start_lm.html#reduction-in-ss-and-anova-table-by-hand",
    "title": "Understanding a linear model",
    "section": "3) Reduction-in-SS and ANOVA table (by hand)",
    "text": "3) Reduction-in-SS and ANOVA table (by hand)\nThe partial sums of squares for block and gen can be obtained as reductions from the intercept-only model:\n\nSSE_mu               # intercept-only\n\n[1] 18.81\n\nSSE_mu - SSE_blk     # SS for blocks\n\n[1] 9.78\n\nSSE_mu - SSE_gen     # SS for genotypes\n\n[1] 6.63\n\nSSE                  # residual SS from full model\n\n[1] 2.4\n\n\nWe now assemble an ANOVA table. Residual df is n - p, where p is the number of coefficients in the full model.\n\n# Sigma^2 estimate using the full model (p = length(beta))\nsigma_2 &lt;- SSE / (n - length(beta))\nsigma_2\n\n[1] 0.4\n\nanova_dt &lt;- data.frame(\n  Source = c(\"block\", \"gen\", \"residuals\"),\n  Df     = c(n_blks - 1, n_gens - 1, n - length(beta)),\n  SSq    = c(SSE_mu - SSE_blk, SSE_mu - SSE_gen, SSE)\n) |&gt;\n  mutate(\n    MSq    = SSq / Df,\n    F.value = MSq / MSq[3],\n    `Pr(&gt;F)` = pf(q = F.value, df1 = Df, df2 = Df[3], lower.tail = FALSE),\n    F.value = ifelse(Source == \"residuals\", NA, F.value),\n    `Pr(&gt;F)` = ifelse(Source == \"residuals\", NA, `Pr(&gt;F)`)\n  )\n\nanova_dt\n\n     Source Df  SSq  MSq F.value      Pr(&gt;F)\n1     block  2 9.78 4.89  12.225 0.007650536\n2       gen  3 6.63 2.21   5.525 0.036730328\n3 residuals  6 2.40 0.40      NA          NA\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhy does this work? In fixed-effects ANOVA, Type-I SS for adding a factor equals the reduction in SSE between nested models. Here we use the intercept-only model as the baseline; adding block or gen reduces SSE by their respective SS.",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html#reduction-in-ss-and-anova-table",
    "href": "pages/00_start_lm.html#reduction-in-ss-and-anova-table",
    "title": "Understanding a linear model",
    "section": "3) Reduction-in-SS and ANOVA table",
    "text": "3) Reduction-in-SS and ANOVA table\nPartial Sum of Squares (SS) measures the unique contribution of a predictor variable to a model, accounting for all other variables. It is calculated by comparing the error sum of squares (SSE) of the model without the variable to the SSE of the model with it. A larger reduction in SSE indicates a greater unique contribution of that variable.\n\nsse_df &lt;- data.frame(\n  Model = factor(\n    c(\"Intercept\", \"Add Blocks\", \"Add Genotypes\", \"Full\"),\n    levels = c(\"Intercept\", \"Add Blocks\", \"Add Genotypes\", \"Full\")\n  ),\n  SS = c(SSE_mu, SSE_mu - SSE_blk, SSE_mu - SSE_gen, SSE)\n)\n\nggplot(sse_df, mapping = aes(x = Model, y = SS, group = 0)) +\n  geom_line() +\n  geom_point(size = 3, color = \"#00BFC4\") +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nWe now assemble an ANOVA table. Residual df is\n\nanova_dt &lt;- data.frame(\n  Source = c(\"block\", \"gen\", \"residuals\"), # factors included in the model\n  Df     = c(n_blks - 1, n_gens - 1, n - length(beta)), # `n - p`, where `p` is the number of coefficients\n  SSq    = c(SSE_mu - SSE_blk, SSE_mu - SSE_gen, SSE) # differences between baseline model and nested models\n) |&gt;\n  mutate(\n    MSq = SSq / Df,\n    F.value = MSq / MSq[3],\n    `Pr(&gt;F)` = pf(q = F.value, df1 = Df, df2 = Df[3], lower.tail = FALSE),\n    F.value = ifelse(Source == \"residuals\", NA, F.value),\n    `Pr(&gt;F)` = ifelse(Source == \"residuals\", NA, `Pr(&gt;F)`)\n  )\n\nanova_dt |&gt;\n  gt()\n\n\n\n\n\n\n\nSource\nDf\nSSq\nMSq\nF.value\nPr(&gt;F)\n\n\n\n\nblock\n2\n9.78\n4.89\n12.225\n0.007650536\n\n\ngen\n3\n6.63\n2.21\n5.525\n0.036730328\n\n\nresiduals\n6\n2.40\n0.40\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhy does this work? In fixed-effects ANOVA, Type-I SS for adding a factor equals the reduction in SSE between nested models. Here we use the intercept-only model as the baseline; adding block or gen reduces SSE by their respective SS.",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html#what-we-learned",
    "href": "pages/00_start_lm.html#what-we-learned",
    "title": "Understanding a linear model",
    "section": "7) What we learned",
    "text": "7) What we learned\n\nHow to construct \\(X\\), solve \\((X^\\top X)^{-1} X^\\top y\\), and compute SSE.\nHow reductions in SSE across nested models yield sums of squares for factors.\nHow to rebuild means for reference and non-reference levels under treatment coding.\nHow to validate results with lm() and extract adjusted means and pairwise tests using emmeans.",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "index.html#what-to-expect",
    "href": "index.html#what-to-expect",
    "title": "Home",
    "section": "",
    "text": "Hands-on examples: R scripts that you can run and adapt.\n\nStep-by-step math: showing how model equations connect to code.\n\nConceptual explanations: to make sense of linear models and linear mixed models.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "pages/01_predict_LM.html",
    "href": "pages/01_predict_LM.html",
    "title": "Marginal means",
    "section": "",
    "text": "In the previous chapter, we introduced how linear models are fitted and how coefficients are estimated. However, these coefficients depend on how categorical factors are encoded in the design matrix (for example, using the first level as a reference). As a result, they are not directly expressed in the units of the response variable. In this section, we show in more detail how to recover estimated marginal means (EMMs)—also known as least-squares means—from the model coefficients, along with their variances.",
    "crumbs": [
      "Starting LM",
      "Marginal means (SE)"
    ]
  },
  {
    "objectID": "pages/01_predict_LM.html#loading-data-and-estimating-coefficients",
    "href": "pages/01_predict_LM.html#loading-data-and-estimating-coefficients",
    "title": "Understanding a Linear Model — Marginal Means",
    "section": "1) Loading data and estimating coefficients",
    "text": "1) Loading data and estimating coefficients\n\ndata &lt;- read.csv(\"../data/example_1.csv\") |&gt;\n  mutate(gen = as.factor(gen), block = as.factor(block))\nn &lt;- 12\nn_b &lt;- 3\nn_g &lt;- 4\n\nX &lt;- model.matrix(yield ~ 1 + block + gen, data = data)\ny &lt;- matrix(data[, \"yield\"])\nprint(X)\n\n   (Intercept) block2 block3 geng2 geng3 geng4\n1            1      0      0     0     0     0\n2            1      1      0     0     0     0\n3            1      0      1     0     0     0\n4            1      0      0     1     0     0\n5            1      1      0     1     0     0\n6            1      0      1     1     0     0\n7            1      0      0     0     1     0\n8            1      1      0     0     1     0\n9            1      0      1     0     1     0\n10           1      0      0     0     0     1\n11           1      1      0     0     0     1\n12           1      0      1     0     0     1\nattr(,\"assign\")\n[1] 0 1 1 2 2 2\nattr(,\"contrasts\")\nattr(,\"contrasts\")$block\n[1] \"contr.treatment\"\n\nattr(,\"contrasts\")$gen\n[1] \"contr.treatment\"\n\n\nRecall:\n\nXty &lt;- t(X) %*% y\nXtX &lt;- t(X) %*% X\nXtX_inv &lt;- solve(XtX)\nbeta &lt;- XtX_inv %*% Xty\nbeta\n\n             [,1]\n(Intercept)  7.75\nblock2      -1.65\nblock3      -2.10\ngeng2        1.10\ngeng3        0.10\ngeng4        1.80",
    "crumbs": [
      "Starting LM",
      "Marginal means (SE)"
    ]
  },
  {
    "objectID": "pages/01_predict_LM.html#marginal-means-scalar-form",
    "href": "pages/01_predict_LM.html#marginal-means-scalar-form",
    "title": "Marginal means",
    "section": "2) Marginal means (scalar form)",
    "text": "2) Marginal means (scalar form)\nLet’s start by using a scalar form to derive the estimated marginal means:\n\nEMM_g1 &lt;- beta[1] + sum(beta[2:3]) / 3\nEMM_b1 &lt;- beta[1] + sum(beta[4:6]) / 4\n\nWhich mathematically represent the following operations\n\\[EMM(g_1) = \\mu + \\frac{1}{3}\\hat\\beta_{block_1} + \\frac{1}{3}\\hat\\beta_{block_2} + \\frac{1}{3}\\hat\\beta_{block_3} = 7.75 + 0 - \\frac{1.65}{3} - \\frac{2.10}{3} = 6.5\\]\n\\[EMM(b_1) = \\mu + \\frac{1}{4}\\hat\\beta_{g_1} + \\frac{1}{4}\\hat\\beta_{g_2} + \\frac{1}{4}\\hat\\beta_{g_3} + \\frac{1}{4}\\hat\\beta_{g_4} = 7.75 + 0 + \\frac{1.10}{4} + \\frac{0.10}{4} + \\frac{1.80}{4} = 8.5\\]\nThen, for the remaining levels\n\nEMM_b2 &lt;- beta[1] + sum(beta[4:6]) / 4 + beta[2]\nEMM_b3 &lt;- beta[1] + sum(beta[4:6]) / 4 + beta[3]\n\nEMM_g2 &lt;- beta[1] + sum(beta[2:3]) / 3 + beta[4]\nEMM_g3 &lt;- beta[1] + sum(beta[2:3]) / 3 + beta[5]\nEMM_g4 &lt;- beta[1] + sum(beta[2:3]) / 3 + beta[6]\n\nEMM &lt;- matrix(c(EMM_b1, EMM_b2, EMM_b3, EMM_g1, EMM_g2, EMM_g3, EMM_g4), ncol = 1)\nrownames(EMM) &lt;- c(\"b1\", \"b2\", \"b3\", \"g1\", \"g2\", \"g3\", \"g4\")\nEMM\n\n   [,1]\nb1 8.50\nb2 6.85\nb3 6.40\ng1 6.50\ng2 7.60\ng3 6.60\ng4 8.30",
    "crumbs": [
      "Starting LM",
      "Marginal means (SE)"
    ]
  },
  {
    "objectID": "pages/01_predict_LM.html#marginal-means-matrix-form",
    "href": "pages/01_predict_LM.html#marginal-means-matrix-form",
    "title": "Marginal means",
    "section": "3) Marginal means (matrix form)",
    "text": "3) Marginal means (matrix form)\nNow let’s explore the matrix form. Specifically, we will make use of the \\(\\boldsymbol{L}\\) matrix to compute all desired values simultaneously, so that:\n\\[EMM(\\boldsymbol{g}) = \\boldsymbol{L}\\hat{\\boldsymbol{\\beta}}\\]\nThe \\(\\boldsymbol{L}\\) matrix represents the levels we want to compute. For example, if we want to extract the EMMs of the genotypes, we should include the intercept, the fraction of each block level that we have to add to the mean, and the levels that we want to compute from the genotypes. Notice how \\(\\boldsymbol{L}\\) has as many rows as levels are in the factor to be solved.\n\nL &lt;- cbind(\n  matrix(1, nrow = n_g, ncol = 1), # Intercept\n  matrix(1 / n_b, nrow = n_g, ncol = n_b - 1), # Average block\n  matrix(rbind(0, diag(nrow = n_g - 1)), nrow = n_g, ncol = n_g - 1)\n)\nL\n\n     [,1]      [,2]      [,3] [,4] [,5] [,6]\n[1,]    1 0.3333333 0.3333333    0    0    0\n[2,]    1 0.3333333 0.3333333    1    0    0\n[3,]    1 0.3333333 0.3333333    0    1    0\n[4,]    1 0.3333333 0.3333333    0    0    1\n\n\nNow we can simply multiply this matrix times the estimated coefficients\n\nEMM_g &lt;- L %*% beta\nEMM_g\n\n     [,1]\n[1,]  6.5\n[2,]  7.6\n[3,]  6.6\n[4,]  8.3\n\n\nThe way the EMM are calculated equals the operations shown above, where\n\\[\n\\small\nEMM(\\boldsymbol{g}) = \\boldsymbol{L}\\,\\boldsymbol{\\hat\\beta} =\n\\begin{bmatrix}\n\\overset{\\text{Intercept}}{1} &\n\\overset{\\text{block2}}{0.33} &\n\\overset{\\text{block3}}{0.33} &\n\\overset{\\text{geng2}}{0} &\n\\overset{\\text{geng3}}{0} &\n\\overset{\\text{geng4}}{0} \\\\\n1 & 0.33 & 0.33 & 1 & 0 & 0 \\\\\n1 & 0.33 & 0.33 & 0 & 1 & 0 \\\\\n1 & 0.33 & 0.33 & 0 & 0 & 1 \\\\\n\\end{bmatrix}\n\\,\n\\overset{\\hat\\beta}{\n\\begin{bmatrix}\n7.75 \\\\\n-1.65 \\\\\n-2.10 \\\\\n1.10 \\\\\n0.10 \\\\\n1.80\n\\end{bmatrix}} =\n\\begin{bmatrix}\n6.5 \\\\\n7.6 \\\\\n6.6 \\\\\n8.3\n\\end{bmatrix}\n\\]\nSo that\n\\[\nEMM(\\boldsymbol{g})_{[1]} = 1*7.75 + \\frac{1}{3}*-1.65 + \\frac{1}{3}*-2.10 = 6.5\n\\]\nSimilarly, to obtain the EMM of each block we would do\n\nL2 &lt;- cbind(\n  matrix(1, nrow = n_b, ncol = 1), # Intercept\n  matrix(rbind(0, diag(nrow = n_b - 1)), nrow = n_b, ncol = n_b - 1),\n  matrix(1 / n_g, nrow = n_b, ncol = n_g - 1) # Average genotype\n)\nL2\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    0    0 0.25 0.25 0.25\n[2,]    1    1    0 0.25 0.25 0.25\n[3,]    1    0    1 0.25 0.25 0.25\n\n\n\nEMM_b &lt;- L2 %*% beta\nEMM_b\n\n     [,1]\n[1,] 8.50\n[2,] 6.85\n[3,] 6.40\n\n\nYou can check that the results are the same as the ones computed before:\n\ncbind(EMM, rbind(EMM_b, EMM_g))\n\n   [,1] [,2]\nb1 8.50 8.50\nb2 6.85 6.85\nb3 6.40 6.40\ng1 6.50 6.50\ng2 7.60 7.60\ng3 6.60 6.60\ng4 8.30 8.30",
    "crumbs": [
      "Starting LM",
      "Marginal means (SE)"
    ]
  },
  {
    "objectID": "pages/01_predict_LM.html#variance-of-the-differences",
    "href": "pages/01_predict_LM.html#variance-of-the-differences",
    "title": "Marginal means",
    "section": "6) Variance of the differences",
    "text": "6) Variance of the differences\nBesides that, it allows us to compute the estimates and the variances of the differences between levels, such as genotypes.\n\ndiff_mm &lt;- emmeans(mod, pairwise ~ gen)\ndiff_mm$contrasts\n\n contrast estimate    SE df t.ratio p.value\n g1 - g2      -1.1 0.516  6  -2.130  0.2447\n g1 - g3      -0.1 0.516  6  -0.194  0.9971\n g1 - g4      -1.8 0.516  6  -3.486  0.0486\n g2 - g3       1.0 0.516  6   1.936  0.3066\n g2 - g4      -0.7 0.516  6  -1.356  0.5656\n g3 - g4      -1.7 0.516  6  -3.292  0.0609\n\nResults are averaged over the levels of: block \nP value adjustment: tukey method for comparing a family of 4 estimates \n\nL_diff &lt;- diff_mm$contrasts@linfct\nL_diff # See how L now links more than 1 genotype per row\n\n     (Intercept) block2 block3 geng2 geng3 geng4\n[1,]           0      0      0    -1     0     0\n[2,]           0      0      0     0    -1     0\n[3,]           0      0      0     0     0    -1\n[4,]           0      0      0     1    -1     0\n[5,]           0      0      0     1     0    -1\n[6,]           0      0      0     0     1    -1\n\n\nNote that the columns for the intercept and the blocks don’t contribute to the variance of the differences (all zeros).\n\nBLUEs_diff &lt;- L_diff %*% beta_lm\nrownames(BLUEs_diff) &lt;- diff_mm$contrasts@levels$contrast\n\n\nrownames(L_diff) &lt;- rownames(BLUEs_diff)\nvar_diff &lt;- L_diff %*% C_11_emm %*% t(L_diff)\nvar_diff |&gt; round(4)\n\n        g1 - g2 g1 - g3 g1 - g4 g2 - g3 g2 - g4 g3 - g4\ng1 - g2  0.2667  0.1333  0.1333 -0.1333 -0.1333  0.0000\ng1 - g3  0.1333  0.2667  0.1333  0.1333  0.0000 -0.1333\ng1 - g4  0.1333  0.1333  0.2667  0.0000  0.1333  0.1333\ng2 - g3 -0.1333  0.1333  0.0000  0.2667  0.1333 -0.1333\ng2 - g4 -0.1333  0.0000  0.1333  0.1333  0.2667  0.1333\ng3 - g4  0.0000 -0.1333  0.1333 -0.1333  0.1333  0.2667\n\n# Average standard error of the differences between two genotype means.\nSE &lt;- sqrt(diag(var_diff))\navg_diff &lt;- mean(SE)\navg_diff\n\n[1] 0.5163978\n\n\nImportantly, since \\(\\boldsymbol{L}_{diff}\\) only contains \\(1\\) and \\(-1\\), and as mentioned before, the intercept and the block don’t contribute to the estimation, we can calculate the variance of the differences by simply using the subset of \\(\\boldsymbol{C_{11}}\\) that involves the genotypes (or the factor of interest).\n\nC_11g &lt;- C_11[4:6, 4:6]                # Subset of C\nd &lt;- diag(C_11g)                       # Difference with the baseline level\nD &lt;- outer(d, d, \"+\") - 2 * C_11g      # var(a-b) = var(a) + var(b) - 2cov(a,b)\nvd &lt;- matrix(NA, length(d) + 1, length(d) + 1)\nvd[-1, -1] &lt;- D\nvd[-1, 1] &lt;- vd[1, -1] &lt;- d\ndiag(vd) &lt;- NA\nvd\n\n          [,1]      [,2]      [,3]      [,4]\n[1,]        NA 0.2666667 0.2666667 0.2666667\n[2,] 0.2666667        NA 0.2666667 0.2666667\n[3,] 0.2666667 0.2666667        NA 0.2666667\n[4,] 0.2666667 0.2666667 0.2666667        NA",
    "crumbs": [
      "Starting LM",
      "Marginal means (SE)"
    ]
  },
  {
    "objectID": "pages/01_predict_LM.html#variance-covariance-se",
    "href": "pages/01_predict_LM.html#variance-covariance-se",
    "title": "Marginal means",
    "section": "4) Variance-covariance (SE)",
    "text": "4) Variance-covariance (SE)\nNow let’s compute the variance and the SE of the EMMS. We know that \\(EMMs = \\boldsymbol{L\\hat\\beta}\\), thus\n\\[Var(\\boldsymbol{L\\hat\\beta}) = \\boldsymbol{L}Var(\\boldsymbol{\\hat\\beta}) \\boldsymbol{L^T} = \\boldsymbol{LC_{11} L^T}\\]\nWe will discuss more in detail about the Coefficient Matrix once we start digging into Linear Mixed Models. For now, we will just say that \\(\\boldsymbol{C_{11}}\\) corresponds to \\(Var(\\boldsymbol{\\hat\\beta})\\), so\n\n# Compute the errors to compute sigma^2\ny_hat &lt;- X %*% beta\nerrors &lt;- y - y_hat\nSSE &lt;- sum(errors^2)\nsigma_2 &lt;- SSE / (n - 6)\nC_11 &lt;- solve(t(X) %*% X) * sigma_2\nC_11\n\n            (Intercept) block2 block3      geng2      geng3      geng4\n(Intercept)   0.2000000   -0.1   -0.1 -0.1333333 -0.1333333 -0.1333333\nblock2       -0.1000000    0.2    0.1  0.0000000  0.0000000  0.0000000\nblock3       -0.1000000    0.1    0.2  0.0000000  0.0000000  0.0000000\ngeng2        -0.1333333    0.0    0.0  0.2666667  0.1333333  0.1333333\ngeng3        -0.1333333    0.0    0.0  0.1333333  0.2666667  0.1333333\ngeng4        -0.1333333    0.0    0.0  0.1333333  0.1333333  0.2666667\n\n\nThen the Variance and the SE of the EMMs are\n\nvar_EMMs &lt;- L %*% C_11 %*% t(L)\nvar_EMMs |&gt; round(4)\n\n       [,1]   [,2]   [,3]   [,4]\n[1,] 0.1333 0.0000 0.0000 0.0000\n[2,] 0.0000 0.1333 0.0000 0.0000\n[3,] 0.0000 0.0000 0.1333 0.0000\n[4,] 0.0000 0.0000 0.0000 0.1333\n\n# SE MMEs\nse_EMMs &lt;- sqrt(diag(var_EMMs))\nse_EMMs\n\n[1] 0.3651484 0.3651484 0.3651484 0.3651484\n\ndata.frame(\n  gen = levels(data$gen),\n  EMMs_g = EMM_g,\n  var_EMMs = diag(var_EMMs),\n  se_EMMs = se_EMMs\n)\n\n  gen EMMs_g  var_EMMs   se_EMMs\n1  g1    6.5 0.1333333 0.3651484\n2  g2    7.6 0.1333333 0.3651484\n3  g3    6.6 0.1333333 0.3651484\n4  g4    8.3 0.1333333 0.3651484",
    "crumbs": [
      "Starting LM",
      "Marginal means (SE)"
    ]
  },
  {
    "objectID": "pages/01_predict_LM.html#estimating-coefficients",
    "href": "pages/01_predict_LM.html#estimating-coefficients",
    "title": "Marginal means",
    "section": "1) Estimating coefficients",
    "text": "1) Estimating coefficients\n\ndata &lt;- read.csv(\"../data/example_1.csv\") |&gt;\n  mutate(gen = as.factor(gen), block = as.factor(block))\nn &lt;- 12\nn_b &lt;- 3\nn_g &lt;- 4\nX &lt;- model.matrix(yield ~ 1 + block + gen, data = data)\ny &lt;- matrix(data[, \"yield\"])\nprint(X)\n\n   (Intercept) block2 block3 geng2 geng3 geng4\n1            1      0      0     0     0     0\n2            1      1      0     0     0     0\n3            1      0      1     0     0     0\n4            1      0      0     1     0     0\n5            1      1      0     1     0     0\n6            1      0      1     1     0     0\n7            1      0      0     0     1     0\n8            1      1      0     0     1     0\n9            1      0      1     0     1     0\n10           1      0      0     0     0     1\n11           1      1      0     0     0     1\n12           1      0      1     0     0     1\nattr(,\"assign\")\n[1] 0 1 1 2 2 2\nattr(,\"contrasts\")\nattr(,\"contrasts\")$block\n[1] \"contr.treatment\"\n\nattr(,\"contrasts\")$gen\n[1] \"contr.treatment\"\n\n\n\nXty &lt;- t(X) %*% y\nXtX &lt;- t(X) %*% X\nXtX_inv &lt;- solve(XtX)\nbeta &lt;- XtX_inv %*% Xty\nbeta\n\n             [,1]\n(Intercept)  7.75\nblock2      -1.65\nblock3      -2.10\ngeng2        1.10\ngeng3        0.10\ngeng4        1.80",
    "crumbs": [
      "Starting LM",
      "Marginal means (SE)"
    ]
  },
  {
    "objectID": "pages/01_predict_LM.html#using-lm-and-emmeans",
    "href": "pages/01_predict_LM.html#using-lm-and-emmeans",
    "title": "Marginal means",
    "section": "5) Using lm and emmeans",
    "text": "5) Using lm and emmeans\nWe can also use lm to fit the model and extract the coefficients and emmeans to retrieve the estimated marginal means. The function emmeans does the job automatically but it also returns the \\(\\boldsymbol{L}\\) and \\(\\boldsymbol{C_{11}}\\) matrices used, so we can explore them.\n\nmod &lt;- lm(formula = yield ~ 1 + block + gen, data = data)\nbeta_lm &lt;- coef(mod)\nbeta_lm\n\n(Intercept)      block2      block3       geng2       geng3       geng4 \n       7.75       -1.65       -2.10        1.10        0.10        1.80 \n\n# Marginal means\nmm &lt;- emmeans(mod, ~gen)\nmm\n\n gen emmean    SE df lower.CL upper.CL\n g1     6.5 0.365  6     5.61     7.39\n g2     7.6 0.365  6     6.71     8.49\n g3     6.6 0.365  6     5.71     7.49\n g4     8.3 0.365  6     7.41     9.19\n\nResults are averaged over the levels of: block \nConfidence level used: 0.95 \n\nL_emm &lt;- mm@linfct # L matrix\nC_11_emm &lt;- mm@V # Variance-covariance of coefficients\nBLUE_mod &lt;- L_emm %*% beta_lm # L beta\nvar_BLUEs_emm &lt;- L_emm %*% C_11_emm %*% t(L_emm) # L C L'\nsqrt(diag(var_BLUEs_emm))\n\n[1] 0.3651484 0.3651484 0.3651484 0.3651484",
    "crumbs": [
      "Starting LM",
      "Marginal means (SE)"
    ]
  },
  {
    "objectID": "pages/02_QR_decomp.html",
    "href": "pages/02_QR_decomp.html",
    "title": "OLS via QR decomposition",
    "section": "",
    "text": "We will analyze the same randomized complete block design (RCBD) (4 genotypes and 3 blocks).\n\nlibrary(dplyr)\nlibrary(Matrix)\ndata &lt;- read.csv(\"../data/example_1.csv\") |&gt;\n  mutate(gen = as.factor(gen),\n         block = as.factor(block))\nhead(data)\n\n  block gen yield\n1     1  g1   7.4\n2     2  g1   6.5\n3     3  g1   5.6\n4     1  g2   9.8\n5     2  g2   6.8\n6     3  g2   6.2\n\nstr(data)\n\n'data.frame':   12 obs. of  3 variables:\n $ block: Factor w/ 3 levels \"1\",\"2\",\"3\": 1 2 3 1 2 3 1 2 3 1 ...\n $ gen  : Factor w/ 4 levels \"g1\",\"g2\",\"g3\",..: 1 1 1 2 2 2 3 3 3 4 ...\n $ yield: num  7.4 6.5 5.6 9.8 6.8 6.2 7.3 6.1 6.4 9.5 ...",
    "crumbs": [
      "Starting LM",
      "QR decomposition in LM"
    ]
  },
  {
    "objectID": "pages/02_QR_decomp.html#setup",
    "href": "pages/02_QR_decomp.html#setup",
    "title": "OLS via QR decomposition",
    "section": "",
    "text": "library(dplyr)\nlibrary(Matrix)"
  },
  {
    "objectID": "pages/02_QR_decomp.html#data",
    "href": "pages/02_QR_decomp.html#data",
    "title": "OLS via QR decomposition",
    "section": "",
    "text": "We will analyze the same randomized complete block design (RCBD) (4 genotypes and 3 blocks).\n\nlibrary(dplyr)\nlibrary(Matrix)\ndata &lt;- read.csv(\"../data/example_1.csv\") |&gt;\n  mutate(gen = as.factor(gen),\n         block = as.factor(block))\nhead(data)\n\n  block gen yield\n1     1  g1   7.4\n2     2  g1   6.5\n3     3  g1   5.6\n4     1  g2   9.8\n5     2  g2   6.8\n6     3  g2   6.2\n\nstr(data)\n\n'data.frame':   12 obs. of  3 variables:\n $ block: Factor w/ 3 levels \"1\",\"2\",\"3\": 1 2 3 1 2 3 1 2 3 1 ...\n $ gen  : Factor w/ 4 levels \"g1\",\"g2\",\"g3\",..: 1 1 1 2 2 2 3 3 3 4 ...\n $ yield: num  7.4 6.5 5.6 9.8 6.8 6.2 7.3 6.1 6.4 9.5 ...",
    "crumbs": [
      "Starting LM",
      "QR decomposition in LM"
    ]
  },
  {
    "objectID": "pages/02_QR_decomp.html#model-matrix",
    "href": "pages/02_QR_decomp.html#model-matrix",
    "title": "OLS via QR decomposition",
    "section": "Model Matrix",
    "text": "Model Matrix\nWe fit the additive model\n\\[\\texttt{yield} \\sim 1 + \\texttt{block} + \\texttt{gen}\\]\n\nX &lt;- model.matrix(yield ~ 1 + block + gen, data = data)\ny &lt;- data[[\"yield\"]]\ndim(X)\n\n[1] 12  6\n\nX[1:6, ]\n\n  (Intercept) block2 block3 geng2 geng3 geng4\n1           1      0      0     0     0     0\n2           1      1      0     0     0     0\n3           1      0      1     0     0     0\n4           1      0      0     1     0     0\n5           1      1      0     1     0     0\n6           1      0      1     1     0     0\n\ny\n\n [1] 7.4 6.5 5.6 9.8 6.8 6.2 7.3 6.1 6.4 9.5 8.0 7.4",
    "crumbs": [
      "Starting LM",
      "QR decomposition in LM"
    ]
  },
  {
    "objectID": "pages/02_QR_decomp.html#coefficients-via-normal-equations",
    "href": "pages/02_QR_decomp.html#coefficients-via-normal-equations",
    "title": "OLS via QR decomposition",
    "section": "Coefficients via Normal Equations",
    "text": "Coefficients via Normal Equations\nThe OLS solution solves \\((X^\\top X)\\beta = X^\\top y\\).\n\nXtX &lt;- crossprod(X)      # X'X\nXty &lt;- crossprod(X, y)   # X'y\nqr(XtX)$rank             # check rank of X'X\n\n[1] 6\n\nbeta_normal &lt;- solve(XtX, Xty)\nbeta_normal\n\n             [,1]\n(Intercept)  7.75\nblock2      -1.65\nblock3      -2.10\ngeng2        1.10\ngeng3        0.10\ngeng4        1.80",
    "crumbs": [
      "Starting LM",
      "QR decomposition in LM"
    ]
  },
  {
    "objectID": "pages/02_QR_decomp.html#coefficients-via-qr-decomposition",
    "href": "pages/02_QR_decomp.html#coefficients-via-qr-decomposition",
    "title": "OLS via QR decomposition",
    "section": "Coefficients via QR Decomposition",
    "text": "Coefficients via QR Decomposition\n\n\n\n\n\n\nImportant\n\n\n\nThe QR decomposition (also called the QR factorization) of a matrix is a decomposition of the matrix into an orthogonal matrix and a triangular matrix. A QR decomposition of a real square matrix \\(A\\) is a decomposition of \\(A\\) as \\(A = QR\\), where \\(Q\\) is an orthogonal matrix (i.e. \\(Q^\\top Q = I\\)) and \\(R\\) is an upper triangular matrix. If \\(A\\) is nonsingular, then this factorization is unique. By Yanovsky (UCLA) (PDF) link\n\n\nWe start by factoring the design matrix \\(X\\), such that \\[X = QR\\]\nSubstituting this factorization into the normal equations, \\[X^\\top X \\beta = X^\\top y,\\] we obtain \\[(R^\\top Q^\\top)(Q R)\\beta = R^\\top Q^\\top y\\] Because \\(Q^\\top Q = I\\), this simplifies to \\[R^\\top R \\beta = R^\\top Q^\\top y\\] By pre-multiplying both sides by \\((R^\\top)^{-1}\\), we get \\[R \\beta = Q^\\top y\\] Finally, since \\(R\\) is upper triangular, we can solve for \\(\\beta\\) efficiently using back substitution.\n\nqrX &lt;- qr(X)\nQ   &lt;- qr.Q(qrX)\nR   &lt;- qr.R(qrX)\nbeta_qr &lt;- backsolve(R, t(Q) %*% y)\nbeta_qr\n\n      [,1]\n[1,]  7.75\n[2,] -1.65\n[3,] -2.10\n[4,]  1.10\n[5,]  0.10\n[6,]  1.80\n\n\n\n\n\nround(Q, 3)\n\n        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]\n [1,] -0.289  0.204  0.354  0.167  0.236  0.408\n [2,] -0.289 -0.408  0.000  0.167  0.236  0.408\n [3,] -0.289  0.204 -0.354  0.167  0.236  0.408\n [4,] -0.289  0.204  0.354 -0.500  0.000  0.000\n [5,] -0.289 -0.408  0.000 -0.500  0.000  0.000\n [6,] -0.289  0.204 -0.354 -0.500  0.000  0.000\n [7,] -0.289  0.204  0.354  0.167 -0.471  0.000\n [8,] -0.289 -0.408  0.000  0.167 -0.471  0.000\n [9,] -0.289  0.204 -0.354  0.167 -0.471  0.000\n[10,] -0.289  0.204  0.354  0.167  0.236 -0.408\n[11,] -0.289 -0.408  0.000  0.167  0.236 -0.408\n[12,] -0.289  0.204 -0.354  0.167  0.236 -0.408\n\n\n\n\n\n\nround(R, 3)\n\n  (Intercept) block2 block3  geng2  geng3  geng4\n1      -3.464 -1.155 -1.155 -0.866 -0.866 -0.866\n2       0.000 -1.633  0.816  0.000  0.000  0.000\n3       0.000  0.000 -1.414  0.000  0.000  0.000\n4       0.000  0.000  0.000 -1.500  0.500  0.500\n5       0.000  0.000  0.000  0.000 -1.414  0.707\n6       0.000  0.000  0.000  0.000  0.000 -1.225",
    "crumbs": [
      "Starting LM",
      "QR decomposition in LM"
    ]
  },
  {
    "objectID": "pages/02_QR_decomp.html#check-same-fit-either-way",
    "href": "pages/02_QR_decomp.html#check-same-fit-either-way",
    "title": "OLS via QR decomposition",
    "section": "Check: Same Fit Either Way",
    "text": "Check: Same Fit Either Way\n\nall.equal(as.vector(beta_normal), as.vector(beta_qr))\n\n[1] TRUE",
    "crumbs": [
      "Starting LM",
      "QR decomposition in LM"
    ]
  },
  {
    "objectID": "pages/02_QR_decomp.html#notes",
    "href": "pages/02_QR_decomp.html#notes",
    "title": "OLS via QR decomposition",
    "section": "Notes",
    "text": "Notes\n\nThe normal-equations method uses an explicit \\((X^\\top X)^{-1}\\); it’s concise but can be numerically less stable if \\(X^\\top X\\) is ill-conditioned.\nThe QR method avoids forming \\((X^\\top X)^{-1}\\) directly and is typically more stable; most linear model software uses QR (or related decompositions) under the hood.",
    "crumbs": [
      "Starting LM",
      "QR decomposition in LM"
    ]
  },
  {
    "objectID": "pages/99_404.html",
    "href": "pages/99_404.html",
    "title": "Error 404",
    "section": "",
    "text": "Under construction!",
    "crumbs": [
      "Advanced LMM",
      "Genotype-by-environment"
    ]
  },
  {
    "objectID": "about.html#authors",
    "href": "about.html#authors",
    "title": "About",
    "section": "",
    "text": "Johan Steven Aparicio\nAlejandro Dominguez",
    "crumbs": []
  },
  {
    "objectID": "about.html#how-to-add-another-author",
    "href": "about.html#how-to-add-another-author",
    "title": "Authors",
    "section": "How to add another author",
    "text": "How to add another author\nCopy an author-card block, update the image path (place your photo in images/), name, role, and affiliation. Photos look best as square images (at least 300×300 px) so the circle crops nicely.\n```\nJohan Steven Aparicio Graduate Student UW - Madison\nAlejandro Dominguez Rondon Graduate Student Universidad Politecnica de Madrid",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "pages/04_covariates_LM.html",
    "href": "pages/04_covariates_LM.html",
    "title": "Covariates in LM",
    "section": "",
    "text": "# RCBD example: 4 gens, 3 blocks, 12 observations\ndata &lt;- read.csv(\"../data/example_1.csv\") |&gt;\n  mutate(gen = as.factor(gen), block = as.factor(block))\nhead(data)\n\n  block gen yield\n1     1  g1   7.4\n2     2  g1   6.5\n3     3  g1   5.6\n4     1  g2   9.8\n5     2  g2   6.8\n6     3  g2   6.2\n\n\n\n# A numeric covariate derived from blocks (for illustration only)\ndata$covariate &lt;- as.numeric(data$block)\ndata$covariate_s &lt;- scale(data$covariate, scale = FALSE)",
    "crumbs": [
      "Starting LM",
      "Covariates in LM"
    ]
  },
  {
    "objectID": "pages/04_covariates_LM.html#estimating-marginal-means-with-model-1",
    "href": "pages/04_covariates_LM.html#estimating-marginal-means-with-model-1",
    "title": "Covariates in LM",
    "section": "Estimating Marginal Means with model 1",
    "text": "Estimating Marginal Means with model 1\n\nmm_1 &lt;- emmeans(mod_1, ~gen)\nL_1 &lt;- mm_1@linfct\nprint(L_1)\n\n     (Intercept) covariate geng2 geng3 geng4\n[1,]           1         2     0     0     0\n[2,]           1         2     1     0     0\n[3,]           1         2     0     1     0\n[4,]           1         2     0     0     1\n\nC_1 &lt;- mm_1@V\nprint(round(C_1, 2))\n\n            (Intercept) covariate geng2 geng3 geng4\n(Intercept)        0.40     -0.12 -0.16 -0.16 -0.16\ncovariate         -0.12      0.06  0.00  0.00  0.00\ngeng2             -0.16      0.00  0.32  0.16  0.16\ngeng3             -0.16      0.00  0.16  0.32  0.16\ngeng4             -0.16      0.00  0.16  0.16  0.32\n\nEMM_1 &lt;- L_1 %*% mm_1@bhat\nvar_EMM_1 &lt;- L_1 %*% C_1 %*% t(L_1)\nse_EMM_1 &lt;- sqrt(diag(var_EMM_1))\ndata.frame(EMM_1, var_EMM_1 = diag(var_EMM_1), se_EMM_1)\n\n  EMM_1 var_EMM_1 se_EMM_1\n1   6.5      0.16      0.4\n2   7.6      0.16      0.4\n3   6.6      0.16      0.4\n4   8.3      0.16      0.4"
  },
  {
    "objectID": "pages/04_covariates_LM.html#estimating-marginal-means-with-model-2",
    "href": "pages/04_covariates_LM.html#estimating-marginal-means-with-model-2",
    "title": "Covariates in LM",
    "section": "Estimating Marginal Means with model 2",
    "text": "Estimating Marginal Means with model 2\n\nmm_2 &lt;- emmeans(mod_2, ~gen)\nL_2 &lt;- mm_2@linfct\nprint(L_2)\n\n     (Intercept) covariate_s geng2 geng3 geng4\n[1,]           1           0     0     0     0\n[2,]           1           0     1     0     0\n[3,]           1           0     0     1     0\n[4,]           1           0     0     0     1\n\nC_2 &lt;- mm_2@V\nprint(round(C_2, 2))\n\n            (Intercept) covariate_s geng2 geng3 geng4\n(Intercept)        0.16        0.00 -0.16 -0.16 -0.16\ncovariate_s        0.00        0.06  0.00  0.00  0.00\ngeng2             -0.16        0.00  0.32  0.16  0.16\ngeng3             -0.16        0.00  0.16  0.32  0.16\ngeng4             -0.16        0.00  0.16  0.16  0.32\n\nEMM_2 &lt;- L_2 %*% mm_2@bhat\nvar_EMM_2 &lt;- L_2 %*% C_2 %*% t(L_2)\nse_EMM_2 &lt;- sqrt(diag(var_EMM_2))\ndata.frame(EMM_2, var_EMM_2 = diag(var_EMM_2), se_EMM_2)\n\n  EMM_2 var_EMM_2 se_EMM_2\n1   6.5      0.16      0.4\n2   7.6      0.16      0.4\n3   6.6      0.16      0.4\n4   8.3      0.16      0.4"
  },
  {
    "objectID": "pages/04_covariates_LM.html#marginal-means-with-model-1",
    "href": "pages/04_covariates_LM.html#marginal-means-with-model-1",
    "title": "Covariates in LM",
    "section": "Marginal Means with Model 1",
    "text": "Marginal Means with Model 1\n\nmm_1 &lt;- emmeans(mod_1, ~gen)\nL_1 &lt;- mm_1@linfct\nprint(L_1)\n\n     (Intercept) covariate geng2 geng3 geng4\n[1,]           1         2     0     0     0\n[2,]           1         2     1     0     0\n[3,]           1         2     0     1     0\n[4,]           1         2     0     0     1"
  },
  {
    "objectID": "pages/04_covariates_LM.html#marginal-means-with-model-2",
    "href": "pages/04_covariates_LM.html#marginal-means-with-model-2",
    "title": "Covariates in LM",
    "section": "Marginal Means with Model 2",
    "text": "Marginal Means with Model 2\n\nmm_2 &lt;- emmeans(mod_2, ~gen)\nL_2 &lt;- mm_2@linfct\nprint(L_2)\n\n     (Intercept) covariate_s geng2 geng3 geng4\n[1,]           1           0     0     0     0\n[2,]           1           0     1     0     0\n[3,]           1           0     0     1     0\n[4,]           1           0     0     0     1"
  },
  {
    "objectID": "pages/04_covariates_LM.html#marginal-means-with-model-1-1",
    "href": "pages/04_covariates_LM.html#marginal-means-with-model-1-1",
    "title": "Covariates in LM",
    "section": "Marginal Means with Model 1",
    "text": "Marginal Means with Model 1\n\nmm_1 &lt;- emmeans(mod_1, ~gen)\nL_1 &lt;- mm_1@linfct\nprint(L_1)\n\n     (Intercept) covariate geng2 geng3 geng4\n[1,]           1         2     0     0     0\n[2,]           1         2     1     0     0\n[3,]           1         2     0     1     0\n[4,]           1         2     0     0     1\n\nC_1 &lt;- mm_1@V\nprint(round(C_1, 2))\n\n            (Intercept) covariate geng2 geng3 geng4\n(Intercept)        0.40     -0.12 -0.16 -0.16 -0.16\ncovariate         -0.12      0.06  0.00  0.00  0.00\ngeng2             -0.16      0.00  0.32  0.16  0.16\ngeng3             -0.16      0.00  0.16  0.32  0.16\ngeng4             -0.16      0.00  0.16  0.16  0.32\n\nEMM_1 &lt;- L_1 %*% mm_1@bhat\nvar_EMM_1 &lt;- L_1 %*% C_1 %*% t(L_1)\nse_EMM_1 &lt;- sqrt(diag(var_EMM_1))\ndata.frame(EMM_1, var_EMM_1 = diag(var_EMM_1), se_EMM_1)\n\n  EMM_1 var_EMM_1 se_EMM_1\n1   6.5      0.16      0.4\n2   7.6      0.16      0.4\n3   6.6      0.16      0.4\n4   8.3      0.16      0.4"
  },
  {
    "objectID": "pages/04_covariates_LM.html#marginal-means-with-model-2-1",
    "href": "pages/04_covariates_LM.html#marginal-means-with-model-2-1",
    "title": "Covariates in LM",
    "section": "Marginal Means with Model 2",
    "text": "Marginal Means with Model 2\n\nmm_2 &lt;- emmeans(mod_2, ~gen)\nL_2 &lt;- mm_2@linfct\nprint(L_2)\n\n     (Intercept) covariate_s geng2 geng3 geng4\n[1,]           1           0     0     0     0\n[2,]           1           0     1     0     0\n[3,]           1           0     0     1     0\n[4,]           1           0     0     0     1\n\nC_2 &lt;- mm_2@V\nprint(round(C_2, 2))\n\n            (Intercept) covariate_s geng2 geng3 geng4\n(Intercept)        0.16        0.00 -0.16 -0.16 -0.16\ncovariate_s        0.00        0.06  0.00  0.00  0.00\ngeng2             -0.16        0.00  0.32  0.16  0.16\ngeng3             -0.16        0.00  0.16  0.32  0.16\ngeng4             -0.16        0.00  0.16  0.16  0.32\n\nEMM_2 &lt;- L_2 %*% mm_2@bhat\nvar_EMM_2 &lt;- L_2 %*% C_2 %*% t(L_2)\nse_EMM_2 &lt;- sqrt(diag(var_EMM_2))\ndata.frame(EMM_2, var_EMM_2 = diag(var_EMM_2), se_EMM_2)\n\n  EMM_2 var_EMM_2 se_EMM_2\n1   6.5      0.16      0.4\n2   7.6      0.16      0.4\n3   6.6      0.16      0.4\n4   8.3      0.16      0.4"
  },
  {
    "objectID": "pages/04_covariates_LM.html#two-models-raw-vs-centered-covariate",
    "href": "pages/04_covariates_LM.html#two-models-raw-vs-centered-covariate",
    "title": "Covariates in LM",
    "section": "Two models: raw vs centered covariate",
    "text": "Two models: raw vs centered covariate\n\nmod_1 &lt;- lm(formula = yield ~ 1 + covariate + gen, data = data)\nmod_2 &lt;- lm(formula = yield ~ 1 + covariate_s + gen, data = data)\n\n\ntibble(\n  term = names(coef(mod_1)),\n  coef_1 = unname(coef(mod_1)), se_1 = sqrt(diag(vcov(mod_1))),\n  coef_2 = unname(coef(mod_2)), se_2 = sqrt(diag(vcov(mod_2)))\n) |&gt;\n  mutate_if(is.numeric, round, 2) |&gt;\n  gt() |&gt;\n  fmt_number(columns = c(coef_1, coef_2), decimals = 3) |&gt;\n  tab_header(title = \"Coefficient estimates and SEs\")\n\n\n\n\n\n\n\nCoefficient estimates and SEs\n\n\nterm\ncoef_1\nse_1\ncoef_2\nse_2\n\n\n\n\n(Intercept)\n8.600\n0.63\n6.500\n0.40\n\n\ncovariate\n−1.050\n0.24\n−1.050\n0.24\n\n\ngeng2\n1.100\n0.57\n1.100\n0.57\n\n\ngeng3\n0.100\n0.57\n0.100\n0.57\n\n\ngeng4\n1.800\n0.57\n1.800\n0.57",
    "crumbs": [
      "Starting LM",
      "Covariates in LM"
    ]
  },
  {
    "objectID": "pages/04_covariates_LM.html#estimated-marginal-means-emms-for-genotypes",
    "href": "pages/04_covariates_LM.html#estimated-marginal-means-emms-for-genotypes",
    "title": "Covariates in LM",
    "section": "Estimated marginal means (EMMs) for genotypes",
    "text": "Estimated marginal means (EMMs) for genotypes\n\n\n\nMarginal means with model 1\n\nmm_1 &lt;- emmeans(mod_1, ~gen)\nL_1 &lt;- mm_1@linfct\nrownames(L_1) &lt;- paste0(\"gen_\", 1:4)\nprint(L_1)\n\n      (Intercept) covariate geng2 geng3 geng4\ngen_1           1         2     0     0     0\ngen_2           1         2     1     0     0\ngen_3           1         2     0     1     0\ngen_4           1         2     0     0     1\n\n\n\n\n\n\n\nMarginal means with model 2\n\nmm_2 &lt;- emmeans(mod_2, ~gen)\nL_2 &lt;- mm_2@linfct\nrownames(L_2) &lt;- paste0(\"gen_\", 1:4)\nprint(L_2)\n\n      (Intercept) covariate_s geng2 geng3 geng4\ngen_1           1           0     0     0     0\ngen_2           1           0     1     0     0\ngen_3           1           0     0     1     0\ngen_4           1           0     0     0     1\n\n\n\n\n\nWe can clearly see the differences in the construction of the \\(L\\) matrix in order to get proper estimations of the marginal means. In mod_1, the covariate column equals the mean of the uncentered covariate (here, 2) for each gen row. In mod_2, the covariate column is 0 because we centered it.\n\n\n\nC_1 &lt;- mm_1@V\nprint(round(C_1, 2))\n\n            (Intercept) covariate geng2 geng3 geng4\n(Intercept)        0.40     -0.12 -0.16 -0.16 -0.16\ncovariate         -0.12      0.06  0.00  0.00  0.00\ngeng2             -0.16      0.00  0.32  0.16  0.16\ngeng3             -0.16      0.00  0.16  0.32  0.16\ngeng4             -0.16      0.00  0.16  0.16  0.32\n\n\n\n\n\n\nC_2 &lt;- mm_2@V\nprint(round(C_2, 2))\n\n            (Intercept) covariate_s geng2 geng3 geng4\n(Intercept)        0.16        0.00 -0.16 -0.16 -0.16\ncovariate_s        0.00        0.06  0.00  0.00  0.00\ngeng2             -0.16        0.00  0.32  0.16  0.16\ngeng3             -0.16        0.00  0.16  0.32  0.16\ngeng4             -0.16        0.00  0.16  0.16  0.32\n\n\n\n\nThe variance–covariance matrix of the model coefficients is also influenced by the scaling of the covariate, as we observed earlier through the standard errors. When the covariate is centered, the covariance between the intercept and the covariate becomes zero, reflecting the removal of linear dependence between the intercept and the covariate.\n\n\n\nRelationship\nBefore centering\nAfter centering\n\n\n\n\nIntercept–covariate\nCorrelated\nUncorrelated (0)\n\n\nCovariate–genotypes\n0\n0\n\n\nGenotype–genotype\nunchanged\nunchanged\n\n\n\n\n\n\nEMM_1 &lt;- L_1 %*% mm_1@bhat\nvar_EMM_1 &lt;- L_1 %*% C_1 %*% t(L_1)\nse_EMM_1 &lt;- sqrt(diag(var_EMM_1))\ndata.frame(\n  Gen = rownames(EMM_1),\n  EMM_1 = EMM_1, \n  var_EMM_1 = diag(var_EMM_1),\n  se_EMM_1\n) |&gt; gt()\n\n\n\n\n\n\n\nGen\nEMM_1\nvar_EMM_1\nse_EMM_1\n\n\n\n\ngen_1\n6.5\n0.16\n0.4\n\n\ngen_2\n7.6\n0.16\n0.4\n\n\ngen_3\n6.6\n0.16\n0.4\n\n\ngen_4\n8.3\n0.16\n0.4\n\n\n\n\n\n\n\n\n\n\n\nEMM_2 &lt;- L_2 %*% mm_2@bhat\nvar_EMM_2 &lt;- L_2 %*% C_2 %*% t(L_2)\nse_EMM_2 &lt;- sqrt(diag(var_EMM_2))\ndata.frame(\n  Gen = rownames(EMM_2),\n  EMM_2 = EMM_2, \n  var_EMM_2 = diag(var_EMM_2),\n  se_EMM_2\n) |&gt; gt()\n\n\n\n\n\n\n\nGen\nEMM_2\nvar_EMM_2\nse_EMM_2\n\n\n\n\ngen_1\n6.5\n0.16\n0.4\n\n\ngen_2\n7.6\n0.16\n0.4\n\n\ngen_3\n6.6\n0.16\n0.4\n\n\ngen_4\n8.3\n0.16\n0.4\n\n\n\n\n\n\n\n\n\nThis vignette outlines practical considerations for incorporating covariates in linear models, including the impact of centering on the intercept, implications for standard errors, and consequences for the estimation of marginal means.",
    "crumbs": [
      "Starting LM",
      "Covariates in LM"
    ]
  },
  {
    "objectID": "pages/04_covariates_LM.html#data",
    "href": "pages/04_covariates_LM.html#data",
    "title": "Covariates in LM",
    "section": "",
    "text": "# RCBD example: 4 gens, 3 blocks, 12 observations\ndata &lt;- read.csv(\"../data/example_1.csv\") |&gt;\n  mutate(gen = as.factor(gen), block = as.factor(block))\nhead(data)\n\n  block gen yield\n1     1  g1   7.4\n2     2  g1   6.5\n3     3  g1   5.6\n4     1  g2   9.8\n5     2  g2   6.8\n6     3  g2   6.2\n\n\n\n# A numeric covariate derived from blocks (for illustration only)\ndata$covariate &lt;- as.numeric(data$block)\ndata$covariate_s &lt;- scale(data$covariate, scale = FALSE)",
    "crumbs": [
      "Starting LM",
      "Covariates in LM"
    ]
  },
  {
    "objectID": "pages/03_LMM.html",
    "href": "pages/03_LMM.html",
    "title": "Linear Mixed Models",
    "section": "",
    "text": "A mixed model, mixed-effects model or mixed error-component model is a statistical model containing both fixed effects and random effects\n\nFirst, let’s load our data\n\ndata &lt;- read.csv(\"../data/example_1.csv\") |&gt;\n  mutate(gen = as.factor(gen), block = as.factor(block))\nhead(data)\n\n  block gen yield\n1     1  g1   7.4\n2     2  g1   6.5\n3     3  g1   5.6\n4     1  g2   9.8\n5     2  g2   6.8\n6     3  g2   6.2\n\nn &lt;- 12\nn_blks &lt;- 3\nn_gens &lt;- 4\n\nThe most common way to obtain BLUEs and BLUPs is to solve the Henderson’s Equations. To do so, first, we need to estimate the variances of the random components. Later on, we will show how to do so using Restricted Maximum Likelihood (REML), but for now we will use the methods of moments. So, let’s assume the following model\n\\[y = \\mu + b + g + \\epsilon, \\text{where } g \\sim N(0, I\\sigma^2_g) \\text{ and } \\epsilon \\sim N(0, I\\sigma^2_e)\\]\nwhich can be also expressed as:\n\\[y = X\\beta + Zu + \\epsilon = \\begin{bmatrix}\n1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\\\\n1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\\\\n1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\\\\n1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\\\\n\\end{bmatrix} \\beta +\n  \\begin{bmatrix}\n1 \\ 0 \\ 0 \\ 0 \\\\\n1 \\ 0 \\ 0 \\ 0 \\\\\n1 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 1 \\\\\n0 \\ 0 \\ 0 \\ 1 \\\\\n0 \\ 0 \\ 0 \\ 1 \\\\\n\\end{bmatrix}u + \\epsilon \\]\n\n\nIn the methods of moments, the variances can be calculated\n\\[\\sigma^2_g = \\frac{MSE - \\sigma^2_\\epsilon}{n_{blocks}}\\]\n\nmod &lt;- lm(formula = yield ~ 1 + block + gen, data = data)\naov_table &lt;- as.data.frame(anova(mod))\naov_table\n\n          Df Sum Sq Mean Sq F value      Pr(&gt;F)\nblock      2   9.78    4.89  12.225 0.007650536\ngen        3   6.63    2.21   5.525 0.036730328\nResiduals  6   2.40    0.40      NA          NA\n\nmse_g &lt;- aov_table[\"gen\", \"Mean Sq\"]\nvar_e &lt;- aov_table[\"Residuals\", \"Mean Sq\"]\nvar_g &lt;- (mse_g - var_e) / n_blks\nvar_g\n\n[1] 0.6033333\n\n\nNext, we need to build the design matrices and the variance covariances \\(\\mathbf{G}, \\mathbf{R} \\text{ and } \\mathbf{V}\\), where:\n\\[G  = \\sigma^2_g I = \\sigma^2_g\n\\begin{bmatrix}\n1 \\ 0 \\ 0 \\ 0\\\\\n0 \\ 1 \\ 0 \\ 0\\\\\n0 \\ 0 \\ 1 \\ 0\\\\\n0 \\ 0 \\ 0 \\ 1\\\\\n\\end{bmatrix}\\]\n\\[R  = \\sigma^2_\\epsilon I = \\sigma^2_\\epsilon\n\\begin{bmatrix}\n1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\\\\n\\end{bmatrix}\\]\n\\[V  = Var(y) = Var(X\\beta + Zu + \\epsilon) = V(Zu) + V(\\epsilon) = ZV(u)Z^T + R = ZGZ^T + R\\]\nThus, we can express the model as\n\\[y \\sim N(X\\beta, ZGZ^T + R) \\text{ or } y \\sim MVN(\\mu, V)\\]\n\n\n\n\nX &lt;- model.matrix(yield ~ 1 + block, data)\nZ &lt;- model.matrix(yield ~ -1 + gen, data)\ny &lt;- matrix(data[, \"yield\"]) |&gt; na.omit()\nprint(X)\n\n   (Intercept) block2 block3\n1            1      0      0\n2            1      1      0\n3            1      0      1\n4            1      0      0\n5            1      1      0\n6            1      0      1\n7            1      0      0\n8            1      1      0\n9            1      0      1\n10           1      0      0\n11           1      1      0\n12           1      0      1\nattr(,\"assign\")\n[1] 0 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$block\n[1] \"contr.treatment\"\n\nprint(y)\n\n      [,1]\n [1,]  7.4\n [2,]  6.5\n [3,]  5.6\n [4,]  9.8\n [5,]  6.8\n [6,]  6.2\n [7,]  7.3\n [8,]  6.1\n [9,]  6.4\n[10,]  9.5\n[11,]  8.0\n[12,]  7.4\n\nG &lt;- diag(x = var_g, nrow = n_gens)\nR &lt;- diag(x = var_e, nrow = n)\nV &lt;- Z %*% G %*% t(Z) + R\n\n\n\n\nNow we have all we need to solve:\n\\[\n  \\underbrace{\\begin{bmatrix}\n    X^T R^{-1} X & X^T R^{-1} Z \\\\\n    Z^T R^{-1} X & Z^T R^{-1} Z + G^{-1}\n    \\end{bmatrix}}_{\\boldsymbol{C}} \\begin{bmatrix}\n\\hat\\beta\\\\ \\hat u\n\\end{bmatrix} = \\underbrace{\\begin{bmatrix}\n  X^T R^{-1} y \\\\\n  Z^T R^{-1} y\n  \\end{bmatrix}}_{\\boldsymbol{RHS}}\n\\]\nAnd therefore\n\\[\n  \\begin{bmatrix}\n\\hat\\beta\\\\ \\hat u\n\\end{bmatrix} = C^{-1} RHS, \\text{ where } C = \\begin{bmatrix}\nC_{11} & C_{12} \\\\\nC_{21} & C_{22}\n\\end{bmatrix}\n\\]\n\n# Mixed Model Equations\nC11 &lt;- t(X) %*% chol2inv(chol(R)) %*% X\nC12 &lt;- t(X) %*% chol2inv(chol(R)) %*% Z\nC21 &lt;- t(Z) %*% chol2inv(chol(R)) %*% X\nC22 &lt;- t(Z) %*% chol2inv(chol(R)) %*% Z + solve(G)\n\n# Coefficient matrix (LHS)\nC &lt;- as.matrix(\n  rbind(\n    cbind(C11, C12),\n    cbind(C21, C22)\n  )\n)\n\n# RHS\nrhs &lt;- rbind(\n  t(X) %*% solve(R) %*% y,\n  t(Z) %*% solve(R) %*% y\n)\n\n# Solution\nC_inv &lt;- chol2inv(chol(C))\nrownames(C_inv) &lt;- colnames(C_inv) &lt;- rownames(C)\nprint(round(C_inv, 4))\n\n            (Intercept) block2 block3   geng1   geng2   geng3   geng4\n(Intercept)      0.2508   -0.1   -0.1 -0.1508 -0.1508 -0.1508 -0.1508\nblock2          -0.1000    0.2    0.1  0.0000  0.0000  0.0000  0.0000\nblock3          -0.1000    0.1    0.2  0.0000  0.0000  0.0000  0.0000\ngeng1           -0.1508    0.0    0.0  0.2327  0.1235  0.1235  0.1235\ngeng2           -0.1508    0.0    0.0  0.1235  0.2327  0.1235  0.1235\ngeng3           -0.1508    0.0    0.0  0.1235  0.1235  0.2327  0.1235\ngeng4           -0.1508    0.0    0.0  0.1235  0.1235  0.1235  0.2327\n\nans &lt;- C_inv %*% rhs\nans\n\n                  [,1]\n(Intercept)  8.5000000\nblock2      -1.6500000\nblock3      -2.1000000\ngeng1       -0.6142534\ngeng2        0.2866516\ngeng3       -0.5323529\ngeng4        0.8599548\n\n\nBLUEs and BLUPs can also be calculated separately as follows:\n\\[\n  \\hat \\beta = (X^TV^{-1}X)^{-1}X^TV^{-1}y\n\\]\nThese are the generalized forms. Let’s show how would \\(\\hat \\beta\\) be calculated using \\(V = \\sigma^2_\\epsilon I\\)\n\\[\n\\beta = \\sigma^2_\\epsilon(X^TX)^{-1} \\frac{X^Ty}{\\sigma^2_\\epsilon} = (X^TX)^{-1}X^Ty\n\\]\nThis looks familiar right? It’s pure Ordinary Least Squares.\nRespectively, for BLUPs:\n\\[\n  \\hat u = GZ^TV^{-1}(y-X\\hat\\beta)\n\\]\n\nV_inv &lt;- solve(V)\nbetas &lt;- solve(t(X) %*% V_inv %*% X) %*% t(X) %*% V_inv %*% y\nu &lt;- G %*% t(Z) %*% V_inv %*% (y - X %*% betas)\nrownames(u) &lt;- colnames(Z)\nm &lt;- rbind(betas, u)\nrownames(m) &lt;- c(rownames(betas), rownames(u))\nm \n\n                  [,1]\n(Intercept)  8.5000000\nblock2      -1.6500000\nblock3      -2.1000000\ngeng1       -0.6142534\ngeng2        0.2866516\ngeng3       -0.5323529\ngeng4        0.8599548",
    "crumbs": [
      "Starting LMM",
      "Mixed model by hand"
    ]
  },
  {
    "objectID": "pages/03_LMM.html#solving-mixed-models-equations",
    "href": "pages/03_LMM.html#solving-mixed-models-equations",
    "title": "Linear Mixed Models",
    "section": "",
    "text": "A mixed model, mixed-effects model or mixed error-component model is a statistical model containing both fixed effects and random effects\n\nFirst, let’s load our data\n\ndata &lt;- read.csv(\"../data/example_1.csv\") |&gt;\n  mutate(gen = as.factor(gen), block = as.factor(block))\nhead(data)\n\n  block gen yield\n1     1  g1   7.4\n2     2  g1   6.5\n3     3  g1   5.6\n4     1  g2   9.8\n5     2  g2   6.8\n6     3  g2   6.2\n\nn &lt;- 12\nn_blks &lt;- 3\nn_gens &lt;- 4\n\nThe most common way to obtain BLUEs and BLUPs is to solve the Henderson’s Equations. To do so, first, we need to estimate the variances of the random components. Later on, we will show how to do so using Restricted Maximum Likelihood (REML), but for now we will use the methods of moments. So, let’s assume the following model\n\\[y = \\mu + b + g + \\epsilon, \\text{where } g \\sim N(0, I\\sigma^2_g) \\text{ and } \\epsilon \\sim N(0, I\\sigma^2_e)\\]\nwhich can be also expressed as:\n\\[y = X\\beta + Zu + \\epsilon = \\begin{bmatrix}\n1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\\\\n1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\\\\n1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\\\\n1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\\\\n\\end{bmatrix} \\beta +\n  \\begin{bmatrix}\n1 \\ 0 \\ 0 \\ 0 \\\\\n1 \\ 0 \\ 0 \\ 0 \\\\\n1 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 1 \\\\\n0 \\ 0 \\ 0 \\ 1 \\\\\n0 \\ 0 \\ 0 \\ 1 \\\\\n\\end{bmatrix}u + \\epsilon \\]\n\n\nIn the methods of moments, the variances can be calculated\n\\[\\sigma^2_g = \\frac{MSE - \\sigma^2_\\epsilon}{n_{blocks}}\\]\n\nmod &lt;- lm(formula = yield ~ 1 + block + gen, data = data)\naov_table &lt;- as.data.frame(anova(mod))\naov_table\n\n          Df Sum Sq Mean Sq F value      Pr(&gt;F)\nblock      2   9.78    4.89  12.225 0.007650536\ngen        3   6.63    2.21   5.525 0.036730328\nResiduals  6   2.40    0.40      NA          NA\n\nmse_g &lt;- aov_table[\"gen\", \"Mean Sq\"]\nvar_e &lt;- aov_table[\"Residuals\", \"Mean Sq\"]\nvar_g &lt;- (mse_g - var_e) / n_blks\nvar_g\n\n[1] 0.6033333\n\n\nNext, we need to build the design matrices and the variance covariances \\(\\mathbf{G}, \\mathbf{R} \\text{ and } \\mathbf{V}\\), where:\n\\[G  = \\sigma^2_g I = \\sigma^2_g\n\\begin{bmatrix}\n1 \\ 0 \\ 0 \\ 0\\\\\n0 \\ 1 \\ 0 \\ 0\\\\\n0 \\ 0 \\ 1 \\ 0\\\\\n0 \\ 0 \\ 0 \\ 1\\\\\n\\end{bmatrix}\\]\n\\[R  = \\sigma^2_\\epsilon I = \\sigma^2_\\epsilon\n\\begin{bmatrix}\n1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\\\\n\\end{bmatrix}\\]\n\\[V  = Var(y) = Var(X\\beta + Zu + \\epsilon) = V(Zu) + V(\\epsilon) = ZV(u)Z^T + R = ZGZ^T + R\\]\nThus, we can express the model as\n\\[y \\sim N(X\\beta, ZGZ^T + R) \\text{ or } y \\sim MVN(\\mu, V)\\]\n\n\n\n\nX &lt;- model.matrix(yield ~ 1 + block, data)\nZ &lt;- model.matrix(yield ~ -1 + gen, data)\ny &lt;- matrix(data[, \"yield\"]) |&gt; na.omit()\nprint(X)\n\n   (Intercept) block2 block3\n1            1      0      0\n2            1      1      0\n3            1      0      1\n4            1      0      0\n5            1      1      0\n6            1      0      1\n7            1      0      0\n8            1      1      0\n9            1      0      1\n10           1      0      0\n11           1      1      0\n12           1      0      1\nattr(,\"assign\")\n[1] 0 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$block\n[1] \"contr.treatment\"\n\nprint(y)\n\n      [,1]\n [1,]  7.4\n [2,]  6.5\n [3,]  5.6\n [4,]  9.8\n [5,]  6.8\n [6,]  6.2\n [7,]  7.3\n [8,]  6.1\n [9,]  6.4\n[10,]  9.5\n[11,]  8.0\n[12,]  7.4\n\nG &lt;- diag(x = var_g, nrow = n_gens)\nR &lt;- diag(x = var_e, nrow = n)\nV &lt;- Z %*% G %*% t(Z) + R\n\n\n\n\nNow we have all we need to solve:\n\\[\n  \\underbrace{\\begin{bmatrix}\n    X^T R^{-1} X & X^T R^{-1} Z \\\\\n    Z^T R^{-1} X & Z^T R^{-1} Z + G^{-1}\n    \\end{bmatrix}}_{\\boldsymbol{C}} \\begin{bmatrix}\n\\hat\\beta\\\\ \\hat u\n\\end{bmatrix} = \\underbrace{\\begin{bmatrix}\n  X^T R^{-1} y \\\\\n  Z^T R^{-1} y\n  \\end{bmatrix}}_{\\boldsymbol{RHS}}\n\\]\nAnd therefore\n\\[\n  \\begin{bmatrix}\n\\hat\\beta\\\\ \\hat u\n\\end{bmatrix} = C^{-1} RHS, \\text{ where } C = \\begin{bmatrix}\nC_{11} & C_{12} \\\\\nC_{21} & C_{22}\n\\end{bmatrix}\n\\]\n\n# Mixed Model Equations\nC11 &lt;- t(X) %*% chol2inv(chol(R)) %*% X\nC12 &lt;- t(X) %*% chol2inv(chol(R)) %*% Z\nC21 &lt;- t(Z) %*% chol2inv(chol(R)) %*% X\nC22 &lt;- t(Z) %*% chol2inv(chol(R)) %*% Z + solve(G)\n\n# Coefficient matrix (LHS)\nC &lt;- as.matrix(\n  rbind(\n    cbind(C11, C12),\n    cbind(C21, C22)\n  )\n)\n\n# RHS\nrhs &lt;- rbind(\n  t(X) %*% solve(R) %*% y,\n  t(Z) %*% solve(R) %*% y\n)\n\n# Solution\nC_inv &lt;- chol2inv(chol(C))\nrownames(C_inv) &lt;- colnames(C_inv) &lt;- rownames(C)\nprint(round(C_inv, 4))\n\n            (Intercept) block2 block3   geng1   geng2   geng3   geng4\n(Intercept)      0.2508   -0.1   -0.1 -0.1508 -0.1508 -0.1508 -0.1508\nblock2          -0.1000    0.2    0.1  0.0000  0.0000  0.0000  0.0000\nblock3          -0.1000    0.1    0.2  0.0000  0.0000  0.0000  0.0000\ngeng1           -0.1508    0.0    0.0  0.2327  0.1235  0.1235  0.1235\ngeng2           -0.1508    0.0    0.0  0.1235  0.2327  0.1235  0.1235\ngeng3           -0.1508    0.0    0.0  0.1235  0.1235  0.2327  0.1235\ngeng4           -0.1508    0.0    0.0  0.1235  0.1235  0.1235  0.2327\n\nans &lt;- C_inv %*% rhs\nans\n\n                  [,1]\n(Intercept)  8.5000000\nblock2      -1.6500000\nblock3      -2.1000000\ngeng1       -0.6142534\ngeng2        0.2866516\ngeng3       -0.5323529\ngeng4        0.8599548\n\n\nBLUEs and BLUPs can also be calculated separately as follows:\n\\[\n  \\hat \\beta = (X^TV^{-1}X)^{-1}X^TV^{-1}y\n\\]\nThese are the generalized forms. Let’s show how would \\(\\hat \\beta\\) be calculated using \\(V = \\sigma^2_\\epsilon I\\)\n\\[\n\\beta = \\sigma^2_\\epsilon(X^TX)^{-1} \\frac{X^Ty}{\\sigma^2_\\epsilon} = (X^TX)^{-1}X^Ty\n\\]\nThis looks familiar right? It’s pure Ordinary Least Squares.\nRespectively, for BLUPs:\n\\[\n  \\hat u = GZ^TV^{-1}(y-X\\hat\\beta)\n\\]\n\nV_inv &lt;- solve(V)\nbetas &lt;- solve(t(X) %*% V_inv %*% X) %*% t(X) %*% V_inv %*% y\nu &lt;- G %*% t(Z) %*% V_inv %*% (y - X %*% betas)\nrownames(u) &lt;- colnames(Z)\nm &lt;- rbind(betas, u)\nrownames(m) &lt;- c(rownames(betas), rownames(u))\nm \n\n                  [,1]\n(Intercept)  8.5000000\nblock2      -1.6500000\nblock3      -2.1000000\ngeng1       -0.6142534\ngeng2        0.2866516\ngeng3       -0.5323529\ngeng4        0.8599548",
    "crumbs": [
      "Starting LMM",
      "Mixed model by hand"
    ]
  },
  {
    "objectID": "pages/03_LMM.html#fixed-and-random-factors-implications",
    "href": "pages/03_LMM.html#fixed-and-random-factors-implications",
    "title": "Linear Mixed Models",
    "section": "2) Fixed and Random Factors Implications",
    "text": "2) Fixed and Random Factors Implications\nNext, we will explore these variance covariance matrices to underline the implications of modeling effects either as random or as fixed. Specifically, we will fit three different models:\n\nThe genotype as Random\nThe genotype as Random and the Block as Fixed\nBoth genotype and Block as Random\n\n\nmm_1 &lt;- lmer(formula = yield ~ 1 + (1 | gen), data = data)\nmm_2 &lt;- lmer(formula = yield ~ 1 + block + (1 | gen), data = data)\nmm_3 &lt;- lmer(formula = yield ~ 1 + (1 | block) + (1 | gen), data = data)\n\nans &lt;- map(list(mm_1, mm_2, mm_3), \\(x) h_cullis(model = x, genotype = \"gen\", re_MME = TRUE))\ncol_pallete &lt;- c(\"#440154\", \"#21908C\", \"#FDE725\")\n\nFirst, let’s take a look at the \\(\\mathbf{V}\\) matrices\n\nVs &lt;- map(ans, \\(x) as.matrix(x$Z %*% x$G %*% t(x$Z) + x$R))\ntitles &lt;- c(\n  \"y ~ (1|gen)\",\n  \"y ~ block + (1|gen)\",\n  \"y ~ (1|block) + (1|gen)\"\n)\nmedian &lt;- median(unlist(Vs))\nmax &lt;- max(unlist(Vs))\n\nplot_function &lt;- function(matrix, title, median, max){\n  covcor_heat(matrix, corr = FALSE, size = 3) +\n    scale_fill_gradient2(\n      low = col_pallete[1],\n      high = col_pallete[3],\n      mid = col_pallete[2],\n      midpoint = median,\n      limit = c(0, max + 0.02),\n      space = \"Lab\"\n    ) +\n    theme(legend.position = \"top\") +\n    labs(title = title)\n}\n\nVs_plots &lt;- map2(Vs, titles, \\(V, t) plot_function(V, t, median, max))\ngrid.arrange(grobs = Vs_plots, ncol = 3)\n\n\n\n\n\n\n\n\nNotice that in the first model we only find correlation between observations that share the same genotype, while no correlation is observed among different genotypes. When block information is added, the correlation pattern remains only within genotypes, but we can clearly see how the variances decreases notably, favouring the covariances.Finally, when both block and genotype are included as random effects, we observe that observations within the same block also share information. Let’s focus now on the \\(C_{22}\\) matrices\n\nC22 &lt;- map(ans, \\(x) as.matrix(x$C22.g))\nmedian &lt;- median(unlist(C22))\nmax &lt;- max(unlist(C22))\nC22_plots &lt;- map2(C22, titles, \\(C, t) plot_function(C, t, median, max))\ngrid.arrange(grobs = C22_plots, ncol = 3)",
    "crumbs": [
      "Starting LMM",
      "Mixed models"
    ]
  },
  {
    "objectID": "pages/05_MVN.html",
    "href": "pages/05_MVN.html",
    "title": "Multivariate Normal Distributions",
    "section": "",
    "text": "In probability theory and statistics, the multivariate normal distribution, multivariate Gaussian distribution, or joint normal distribution is a generalization of the one-dimensional (univariate) normal distribution to higher dimensions.",
    "crumbs": [
      "Starting LMM",
      "Multivariate Normal"
    ]
  },
  {
    "objectID": "pages/05_MVN.html#generalized-case",
    "href": "pages/05_MVN.html#generalized-case",
    "title": "Multivariate Normal Distributions",
    "section": "1) Generalized Case",
    "text": "1) Generalized Case\nA Bivariate Normal Distribution follows the distribution:\n\\[\n\\begin{bmatrix}\n\\boldsymbol{a}\\\\\n\\boldsymbol{b}\n\\end{bmatrix} \\sim\nMVN(\n  \\mu_{ab},\n  \\Sigma_{ab}\n),\n\\text{where }\n\\mu_{ab} = \\begin{bmatrix}\n\\boldsymbol{\\mu_a}\\\\\n\\boldsymbol{\\mu_b}\n\\end{bmatrix}\n\\text{ and }\n\\boldsymbol{\\Sigma_{ab}} = \\begin{bmatrix}\n\\boldsymbol{\\Sigma_{aa}}\\ \\boldsymbol{\\Sigma_{ab}}\\\\\n\\boldsymbol{\\Sigma_{ba}}\\ \\boldsymbol{\\Sigma_{bb}}\\\\\n\\end{bmatrix}\n\\]\nOne interesting property of these kind of distribution is that allows us to compute the mean and the variance of the conditional distributions \\(b \\mid a\\) (or \\(a \\mid b\\)) as follows\n\\[\n\\mathbf{b}\\mid\\mathbf{a}\\ \\sim\\\nN\\left(\n  \\underbrace{\\boldsymbol{\\mu}_b + \\boldsymbol{\\Sigma}_{ba}\\,\\boldsymbol{\\Sigma}_{aa}^{-1}\\,(\\mathbf{a}-\\boldsymbol{\\mu}_a)}_{\\,\\mathbb{E}(\\mathbf{b}\\mid\\mathbf{a})\\,},\n  \\underbrace{\\boldsymbol{\\Sigma}_{bb} - \\boldsymbol{\\Sigma}_{ba}\\,\\boldsymbol{\\Sigma}_{aa}^{-1}\\,\\boldsymbol{\\Sigma}_{ab}}_{\\operatorname{Var}(\\mathbf{b}\\mid\\mathbf{a})}\n\\right).\n\\]\nTherefore, if we focus solely on the conditional mean and variances\n\\[\nE(b \\mid a) = \\mu_b + \\Sigma_{ba}(\\Sigma_{aa})^{-1}(a - \\mu_a)\n\\]\n\\[\nvar(b \\mid a) = \\Sigma_{bb} - \\Sigma_{ab} (\\Sigma_{aa})^{-1}\\Sigma_{ba}\n\\]",
    "crumbs": [
      "Starting LMM",
      "Multivariate Normal"
    ]
  },
  {
    "objectID": "pages/05_MVN.html#implications-in-quantitative-genetics",
    "href": "pages/05_MVN.html#implications-in-quantitative-genetics",
    "title": "Multivariate Normal Distributions",
    "section": "2) Implications in Quantitative Genetics",
    "text": "2) Implications in Quantitative Genetics\n\n1) Conditional \\(\\mathbf{E[u \\mid y]}\\)\nLet’s illustrate this with our model where we fit a single variable (i.e, block) as fixed and another one (i.e, genotype) as random. Therefore:\n\\[\ny = X\\beta + Zu + \\epsilon, \\text{ where } u \\sim N(0,\\underbrace{I\\sigma^2_u}_{G}) \\text{ and } \\epsilon\\sim N(0, \\underbrace{I\\sigma^2_e}_{R})\n\\]\nTherefore, we could write the Multivariate Normal distribution of \\(y\\) and \\(u\\) as follows.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   \n\n\\[\n\\begin{bmatrix}\ny\\\\\nu\n\\end{bmatrix} \\sim\nMVN(\n  \\begin{bmatrix}\n  X\\beta\\\\\n  0\n  \\end{bmatrix},\n  \\begin{bmatrix}\n  V \\ ZG \\\\\n  GZ'\\ G \\\\\n\\end{bmatrix}\n)\n\\]\nFollowing the previous equations, the expected value of \\(u\\) is\n\\[\n\\begin{align*}\nE[u \\mid y]\n  &= \\mu_u + \\Sigma_{uy}(\\Sigma_{yy})^{-1}(y - \\mu_y) \\\\\n  &= GZ'V^{-1}(y - X\\beta)\n\\end{align*}\n\\]\nGiven that we don’t have the true \\(\\beta\\) but the estimated ones (\\(\\hat \\beta\\)), then the expression for the estimator and predictors becomes\n\\[\n\\begin{align*}\nE[\\hat u \\mid y]\n  &= GZ'V^{-1}(y - X\\hat{\\beta}) \\\\\n&= GZ'V^{-1}\\big(y - X(X'V^{-1}X)^{-1}X'V^{-1}y\\big) \\\\\n  &= GZ'V^{-1}\\big(I - X(X'V^{-1}X)^{-1}X'V^{-1}\\big)y \\\\\n                   &= GZ'\\big(V^{-1} - V^{-1}X(X'V^{-1}X)^{-1}X'V^{-1}\\big)y \\\\\n  &= GZ'Py \\\\\n& \\text{where }\nP = V^{-1} - V^{-1}X(X'V^{-1}X)^{-1}X'V^{-1}\n\\end{align*}\n\\]\nSo, briefly\n\\[\nBLUPs = \\hat u = GZ'Py\n\\]\nThis BLUP definition is very useful to derive some variance and covariances that we might need in future chapters.\n\n\n1) Variance of \\(\\mathbf{\\hat u}\\)\n\\[\n\\begin{align*}\n\\mathrm{var}(\\hat u)\n  &= var(GZ'Py) \\\\\n&= GZ'Pvar(y)PZG \\\\\n  &= GZ'PVPZG \\\\\n&= GZ'PZG\n\\end{align*}\n\\]\nThis is possible thanks to one of the many useful properties of \\(P\\), that fulfills that \\(PVP = P\\)\n\n\n2) \\(\\mathbf{Cov(\\hat{u}, u)}\\)\n\\[\n\\begin{align*}\n\\mathrm{cov}(\\hat u, u)\n  &= cov(GZ'Py, u) \\\\\n&= GZ'Pcov(y, u) \\\\\n  &= GZ'PZG\n\\end{align*}\n\\]\nInterestingly\n\\[\n  \\begin{align*}\n\\mathrm{cov}(u, \\hat u)\n&= cov(\\hat u, u)' \\\\\n  &= GZ'PZG\n\\end{align*}\n\\]\nThen \\(\\mathrm{cov}(u, \\hat u) = \\mathrm{cov}(\\hat u, u)\\)\n\n\n3) \\(\\mathbf{Var(u - \\hat{u})}\\)\nFollowing the properties of the variances\n\\[\n\\begin{align*}\n  \\mathrm{var}(u - \\hat{u})\n  &= E\\!\\left[(u - \\hat{u} - E(u - \\hat{u}))(u - \\hat{u} - E(u - \\hat{u}))'\\right] \\\\\n  &= E\\!\\left[(u - \\hat{u})(u - \\hat{u})'\\right] \\\\\n  &= E(uu') - E(u\\hat{u}') - E(\\hat{u}u') + E(\\hat{u}\\hat{u}') \\\\\n  &= \\mathrm{var}(u) - \\mathrm{cov}(u, \\hat{u}) - \\mathrm{cov}(\\hat{u}, u) + \\mathrm{var}(\\hat{u})\n  \\end{align*}\n\\]\nThen, using the previously computed variances and covariances we can solve for\n\\[\n\\begin{align*}\n  \\operatorname{var}(u - \\hat{u})\n  &= G - GZ'PZG - GZ'PGZ + GZ'PZG \\\\\n  &= G - GZ'PZG\n\\end{align*}\n\\]\nRemember that this differences are represented in the so called Prediction Error Variance Matrix, so\n\\[\n    C_{22} = PEV =  G - GZ'PZG\n\\]\nThis is helpful since now we can prove that the marginal variance \\(var(\\hat{u})\\) equals\n\\[\n\\begin{align*}\n\\operatorname{var}(\\hat{u})\n  &= G - C_{22} \\\\\n  &= G - (G - GZ'PZG) \\\\\n&= GZ'PZG\n\\end{align*}\n\\]\n\n\n2) Conditional \\(\\mathbf{\\hat{u}, u}\\)\nStarting from the joint distribution\n\\[\n\\begin{bmatrix}\n\\hat{u} \\\\[4pt]\nu\n\\end{bmatrix}\n\\sim\n\\text{MVN}\\!\\left(\n\\begin{bmatrix}\n0 \\\\[4pt]\n0\n\\end{bmatrix},\n\\begin{bmatrix}\nGZ'PZG & GZ'PZG \\\\[4pt]\nGZ'PZG & G\n\\end{bmatrix}\n\\right)\n\\]\nWe can derive \\(\\mathrm{var}(u \\mid \\hat{u})\\) as\n\\[\n\\begin{align*}\n\\mathrm{var}(u \\mid \\hat{u})\n&= \\Sigma_{uu} - \\Sigma_{\\hat{u}u} (\\Sigma_{\\hat{u}\\hat{u}})^{-1}\\Sigma_{u\\hat{u}} \\\\\n&= G - GZ'PZG(GZ'PZG)^{-1}GZ'PZG \\\\\n  &= G - GZ'PZG\n\\end{align*}\n\\]\nWe have just proved then that\n\\[\n\\mathrm{var}(u \\mid \\hat{u}) = \\operatorname{var}(u - \\hat{u})\n\\]\nBesides that, if we solve for the mean of the conditional distribution, we get that\n\\[\n\\begin{align*}\n  E[u \\mid \\hat{u}]\n  &= \\mu_u + \\Sigma_{u\\hat{u}}(\\Sigma_{\\hat{u}\\hat{u}})^{-1}(\\hat{u} - \\mu_\\hat{u}) \\\\\n  &= GZ'PZG(GZ'PZG)^{-1}(\\hat{u}) \\\\\n&= \\hat{u}\n\\end{align*}\n\\]\nSo that\n\\[\nE[u \\mid \\hat{u}] = E[u \\mid y] = \\hat{u} = BLUP\n\\]",
    "crumbs": [
      "Starting LMM",
      "Multivariate Normal"
    ]
  },
  {
    "objectID": "pages/06_Heritabilities.html",
    "href": "pages/06_Heritabilities.html",
    "title": "Heritability",
    "section": "",
    "text": "Heritability is a key concept in Quantitative Genetics and Plant Breeding. It has been largely used for different purposes, such as estimating the response to selection, estimate model accuracy or determine how difficult is to make progress for a trait. Because of its relevance, many researchers have studied how to improve its estimation, yielding multiple methods to do calculate this paremeter. In this chapter, we will cover some of the these approaches and explain their differences and similarities.\n\n1) Standard\nThe easiest interpretation of the heritability defines it as the ratio of genetic variance to phenotypic variance\n\\[\nH^2 = \\frac{\\sigma^2_g}{\\sigma^2_p}\n\\]\nThe issue with this approach is that, typically, when genotypes are tested in Mutienvironmental Trials (MET) the purely environmental variance (i.e, the block effect) is not incorporated to the phenotypic variance, neglecting those effects in the heritability calculation\n\n\n2) Cullis Heritability\nA different approach focuses on differences between genotypes as the source of heritability. The reasoning behind this is that rankings are uniquely determined by pairwise differences, making them more informative than individual genotypic effects. Heritability based on genotype differences can be calculated from a model by fitting the genotypes as either random effects or a combination of random and fixed effects. For simplicity, we will describe how to compute it using the first approach.\nThe pairwise heritability can be defined as the squared correlation between the true difference and its predictor/estimator. To compute it, we will need to calculate the covariance between the differences as well as their standard deviations\n\\[\nH^2_{\\Delta 12} =\n\\left(\n\\frac{\\operatorname{cov}(u_1 - u_2, \\hat{u}_1 - \\hat{u}_2)}\n{\\sqrt{\\operatorname{var}(u_1 - u_2)} \\, \\sqrt{\\operatorname{var}(\\hat{u}_1 - \\hat{u}_2)}}\n\\right)^2\n\\]\nAssuming independence with constant variance, the variance of the difference between the true values would be\n\\[\n\\operatorname{var}(u_1 - u_2) = 2\\sigma^2_g - \\cancel{2\\operatorname{cov}(u_1, u_2)} = 2\\sigma^2_g\n\\]\nTo compute the variance of the estimated values we will use the marginal variance of \\(\\hat{u}^{BLUP}\\), which as wxplained in the previous chapter equals\n\\[\nvar(\\hat{u}^{BLUP}) = G_{(g)} - C_{22(g)}\n\\]\nLet’s illustrate that \\(G - C_{22}\\) matrix that contains the marginal variance of each BLUP in the diagonals and the covariances between BLUPs in the off-diagonals.\n\\[\n\\operatorname{var}(\\hat{u}) =\n\\underbrace{\n\\begin{bmatrix}\n\\sigma_g^2 & 0 & \\cdots & 0 \\\\\n0 & \\sigma_g^2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\sigma_g^2\n\\end{bmatrix}\n}_{G = I} -\n\\,\n\\underbrace{\n\\begin{bmatrix}\n\\operatorname{var}(u_1 - \\hat{u}_1) & \\cdot & \\cdots & \\cdot \\\\\n\\operatorname{cov}(u_1 - \\hat{u}_1, u_2 - \\hat{u}_2) & \\operatorname{var}(u_2 - \\hat{u}_2) & \\cdots & \\cdot \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\operatorname{cov}(u_1 - \\hat{u}_1, u_n - \\hat{u}_n) & \\operatorname{cov}(u_2 - \\hat{u}_2, u_n - \\hat{u}_n) & \\cdots & \\operatorname{var}(u_n - \\hat{u}_n)\n\\end{bmatrix}\n}_{C_{22}}\n\\]\nWhen assuming independence between estimates, the off-diagonals in G equals 0, as shown above, and thus \\(cov(\\hat{u}^{BLUP}_1, \\hat{u}^{BLUP}_2) = C_{22[1, 2]}\\). The variances and covariances would then be alculated as follows\n\\[\n\\begin{aligned}\n\\operatorname{var}(\\hat{u}_{1})\n&= G_{[1,1]} - C_{22[1,1]}\n= \\sigma_g^2 + \\operatorname{var}(u_1 - \\hat{u}_1) \\\\[6pt]\n\\operatorname{cov}(\\hat{u}_{1}, \\hat{u}_{2})\n&= \\cancel{G_{[1,2]}} - C_{22[1,2]}\n= -\\,\\operatorname{cov}(u_1 - \\hat{u}_1,\\, u_2 - \\hat{u}_2)\n\\end{aligned}\n\\]\nSo, with this information we can now calculate the variance of the difference between two BLUPs, which under the assumption of independent genotypes would equal\n\\[\n\\begin{align*}\n\\operatorname{var}(\\hat{u}^{BLUP}_1 - \\hat{u}^{BLUP}_2)\n&= \\operatorname{var}(\\hat{u}^{BLUP}_1) + \\operatorname{var}(\\hat{u}^{BLUP}_2) - 2\\operatorname{cov}(\\hat{u}^{BLUP}_1, \\hat{u}^{BLUP}_2)\\\\\n&= \\sigma^2_g - \\operatorname{var}(u_1 - \\hat{u}_1) + \\sigma^2_g - \\operatorname{var}(u_2 - \\hat{u}_2) + 2\\operatorname{cov}(u_1 - \\hat{u}_1, u_2 - \\hat{u}_2) \\\\\n&= 2\\sigma^2_g - (\\operatorname{var}(u_1 - \\hat{u}_1) + \\operatorname{var}(u_2 - \\hat{u}_2) - 2\\operatorname{cov}(u_1 - \\hat{u}_1, u_2 - \\hat{u}_2)) \\\\\n&= 2\\sigma^2_g - \\operatorname{var}((u_1 - \\hat{u}_1) - (u_1 - \\hat{u}_1))\\\\\n&= 2\\sigma^2_g - v^{BLUP}_{\\Delta_{12}}\n\\end{align*}\n\\]\nThe last component left is then to compute the covariance between differences\n\\[\n\\begin{align*}\n\\operatorname{cov}(u_{1} - u_2, \\hat{u}_1 - \\hat{u}_2)\n&= \\operatorname{cov}(u_{1}, \\hat{u}_1) - \\operatorname{cov}(u_{1}, \\hat{u}_2) - \\operatorname{cov}(u_{2}, \\hat{u}_1) + \\operatorname{cov}(u_{2}, \\hat{u}_2)\\\\\n&= (G-C_{22})_{[1,1]} - (G-C_{22})_{[1,2]} - (G-C_{22})_{[2,1]} + (G-C_{22})_{[2,2]} \\\\\n&= \\sigma^2_g - C_{22[1,1]} + C_{22[1,2]} + C_{22[1,2]}  + \\sigma^2_g - C_{22[2,2]} \\\\\n&= 2\\sigma^2_g - v^{BLUP}_{\\Delta_{12}}\n\\end{align*}\n\\]\nFinally, we can solve the Heritability as the square correlation of the differences between the true values and the predictors\n\\[\nH^2_{{\\Delta 12}_{BLUP}} =\n\\left(\n\\frac{\\operatorname{cov}(u_1 - u_2, \\hat{u}_1 - \\hat{u}_2)}\n{\\sqrt{\\operatorname{var}(u_1 - u_2)} \\, \\sqrt{\\operatorname{var}(\\hat{u}_1 - \\hat{u}_2)}}\n\\right)^2 =\n\\frac{(2\\sigma_g^2 - v^{\\text{BLUP}}_{\\Delta_{12}})^2}\n{2\\sigma_g^2 \\left( 2\\sigma_g^2 - v^{\\text{BLUP}}_{\\Delta_{12}} \\right)}\n=\n\\frac{2\\sigma_g^2 - v^{\\text{BLUP}}_{\\Delta_{12}}}{2\\sigma_g^2}\n\\]\nThis expression can be simplified to the Cullis Heritability (Cullis et al, 2006), where \\(\\overline v_{\\Delta_{ii}}^{BLUP}\\) represent the mean variance of a difference of two BLUPs for the genotypic effects\n\\[\nH^2_{Cullis} = 1 - \\frac{\\overline v_{\\Delta_{ii}}^{BLUP}}{2\\sigma^2_g}\n\\]\nIt is important to notice that this way to compute heritability only holds under the assumption that the genotypes are independent. Otherwise, a more generalized approach, as described by (Schmidt et al, 2019) should be used\n\\[\n\\begin{align*}\nH^2_{{\\Delta 12}_{BLUP}}\n&= \\frac{\\operatorname{var}(g_1 - g_2) - v^{BLUP}_{\\Delta 12}}{\\operatorname{var}(g_1 - g_2)}\\\\\n&= \\frac{\\sigma^2_{g(1,1)} + \\sigma^2_{g(2,2)} - 2\\sigma^2_{g(1,2)} -  v^{BLUP}_{\\Delta 12}}{\\sigma^2_{g(1,1)} + \\sigma^2_{g(2,2)} - 2\\sigma^2_{g(1,2)}}\n\\end{align*}\n\\]\n\n\n3) Oakey\nIn this approach the contrast between the true and the predicted genotypic effects is defined as the square correlation of \\(c^Tg\\) and \\(c^T\\hat{g}^{BLUP}\\), where c represent any linear combination of genotypic effects where the elements of \\(c\\) sum to 0. The variances and covariances needed to estimate that correlations are\n\\[\n\\operatorname{cov}(c^Tg, c^T\\hat{g}^{BLUP}) = c^T\\operatorname{cov}(g, \\hat{g}^{BLUP})c = c^TGZ'PZGc\n\\]\n\\[\n\\operatorname{var}(c^Tg) = c^T\\operatorname{var}(g)c = c^TGc\n\\]\n\\[\n\\operatorname{var}(c^T\\hat{g}^{BLUP}) = c^T\\operatorname{var}(\\hat{g}^{BLUP})c = c^TGZ'PZGc\n\\]\nThen we can compute Oakey’s Heritability as\n\\[\n\\begin{align*}\nH^2_{c}\n&= \\left(\n  \\frac{\\operatorname{cov}(c^T g,\\; c^T \\hat{g}^{BLUP})}\n       {\\sqrt{\\operatorname{var}(c^T g)} \\sqrt{\\operatorname{var}(c^T \\hat{g}^{BLUP})}}\n  \\right)^{2} \\\\\n&= \\frac{(c^T G Z' P Z G c)^2}{c^T G c \\, (c^T G Z' P Z G c)} \\\\\n&= \\frac{c^T G Z' P Z G c}{c^T G c}\n\\end{align*}\n\\]\nThe idea is to maximize \\(H^2_{c}\\) subject to \\(c^T G c = 1\\), using the Lagrangian product \\(L_c = c^T G Z' P Z G c - \\lambda(c^T G c - 1)\\). In summary, the vector the maximizes \\(H^2_{c}\\) is an eigenvector of \\(Z' P Z G\\) with associated eigenvalue \\(\\lambda\\) (\\(\\max\\limits_{c} H^2_{c} = \\lambda\\)). Therefore, the full set of eigenvalues (\\(\\lambda_1, \\lambda_2, ..., \\lambda_n\\)) characterizes the full heritability as follows, where \\(s\\) represent the number of eigenvalues equals 0.\n\\[\nH^2_{Oakey} = \\frac{\\sum_{i = 1}^m \\lambda_i}{m - s}\n\\]\nAn interesting simplification is that, \\(Z' P Z G = I - G^{-1}C_{22} = D\\). Following the properties of the trace, this \\(D\\) matrix fulfills\n\\[\ntrace(D) = trace(Q\\Lambda Q^{-T}) = trace(QQ^{-T}\\Lambda) = trace(\\Lambda) = \\sum \\lambda_i\n\\]\nBriefly, the full Heritability can be approximated without the need of calculating the eigenvalues of the matrix. This is particularly useful for larger problems, although it ignores the possibility of zero eigenvalues.\n\\[\nH^2_{Approx} = 1 - \\frac{\\operatorname{tr}(G^{-1}C_{22})}{m}\n\\] # 4) Reliability\nIn the context of anial breeding, reliability is a popular stastistic expressing the squared corrrelation between predicted an true breeding value. For a particular entry, it can be computed as\n\\[\nr^2_1 = 1 - \\frac{\\operatorname{var}(\\hat{g}^{BLUP}_1)}{\\operatorname{var}(g_1)}\n\\]\nAnd thus, the mean reliability is computed as the average of the reliability of the entire population\n\\[\n\\bar{r}^2_. = \\frac{1}{n_g}\\sum_{i = 1}^{n_g}r^2_i\n\\]\n\n\n5) Simulated",
    "crumbs": [
      "Starting LMM",
      "Heritability"
    ]
  },
  {
    "objectID": "pages/06_Heritabilities.html#assuming-genotypes-as-random-effects",
    "href": "pages/06_Heritabilities.html#assuming-genotypes-as-random-effects",
    "title": "Untitled",
    "section": "Assuming Genotypes as Random Effects",
    "text": "Assuming Genotypes as Random Effects\nWe have the following model where the genotype has been fit as Random\n\\[\ny \\sim \\mu + block + (1|gen) + \\varepsilon\n\\]\nLet’s first extract the variance component using ANOVA\n\nmod &lt;- lm(formula = yield ~ 1 + block + gen, data = data)\naov_table &lt;- as.data.frame(anova(mod))\n\nmse_g &lt;- aov_table[\"gen\", \"Mean Sq\"]\nvar_e &lt;- aov_table[\"Residuals\", \"Mean Sq\"]\nvar_g &lt;- (mse_g - var_e) / n_blks\n\nNow we can build the required matrices\n\nones &lt;- model.matrix(yield ~ 1, data)\nX &lt;- cbind(ones, model.matrix(yield ~ -1 + block, data))\nZ &lt;- model.matrix(yield ~ -1 + gen, data)\ny &lt;- matrix(data[, \"yield\"]) |&gt; na.omit()\n\nG &lt;- diag(x = var_g, nrow = n_gens)\nR &lt;- diag(x = var_e, nrow = n)\nV &lt;- Z %*% G %*% t(Z) + R\n\nAnd solve the Henderson Equation to extract the BLUEs and BLUPs\n\nC11 &lt;- t(X) %*% chol2inv(chol(R)) %*% X\nC12 &lt;- t(X) %*% chol2inv(chol(R)) %*% Z\nC21 &lt;- t(Z) %*% chol2inv(chol(R)) %*% X\nC22 &lt;- t(Z) %*% chol2inv(chol(R)) %*% Z + solve(G)\n\nC &lt;- as.matrix(\n  rbind(\n    cbind(C11, C12),\n    cbind(C21, C22)\n  )\n)\n\nrhs &lt;- rbind(\n  t(X) %*% solve(R) %*% y,\n  t(Z) %*% solve(R) %*% y\n)\n\nC_inv &lt;- ginv(C)\nrownames(C_inv) &lt;- colnames(C_inv) &lt;- rownames(C)\nprint(C_inv)\n\n            (Intercept)      block1      block2      block3       geng1\n(Intercept)  0.10359375  0.03453125  0.03453125  0.03453125 -0.11312500\nblock1       0.03453125  0.07817708 -0.02182292 -0.02182292 -0.03770833\nblock2       0.03453125 -0.02182292  0.07817708 -0.02182292 -0.03770833\nblock3       0.03453125 -0.02182292 -0.02182292  0.07817708 -0.03770833\ngeng1       -0.11312500 -0.03770833 -0.03770833 -0.03770833  0.23273379\ngeng2       -0.11312500 -0.03770833 -0.03770833 -0.03770833  0.12353318\ngeng3       -0.11312500 -0.03770833 -0.03770833 -0.03770833  0.12353318\ngeng4       -0.11312500 -0.03770833 -0.03770833 -0.03770833  0.12353318\n                  geng2       geng3       geng4\n(Intercept) -0.11312500 -0.11312500 -0.11312500\nblock1      -0.03770833 -0.03770833 -0.03770833\nblock2      -0.03770833 -0.03770833 -0.03770833\nblock3      -0.03770833 -0.03770833 -0.03770833\ngeng1        0.12353318  0.12353318  0.12353318\ngeng2        0.23273379  0.12353318  0.12353318\ngeng3        0.12353318  0.23273379  0.12353318\ngeng4        0.12353318  0.12353318  0.23273379\n\nans &lt;- C_inv %*% rhs\nans\n\n                  [,1]\n(Intercept)  5.4375000\nblock1       3.0625000\nblock2       1.4125000\nblock3       0.9625000\ngeng1       -0.6142534\ngeng2        0.2866516\ngeng3       -0.5323529\ngeng4        0.8599548\n\n\nAdditionally, we can extract the Marginal Means of each block as follows\n\nL &lt;- cbind(\n  matrix(1, nrow = n_blks, ncol = 1), # Intercept\n  diag(n_blks) # blocks\n)\nL\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    1    0    0\n[2,]    1    0    1    0\n[3,]    1    0    0    1\n\nL %*% ans[1:4]\n\n     [,1]\n[1,] 8.50\n[2,] 6.85\n[3,] 6.40\n\nL %*% C_inv[1:4, 1:4] %*% t(L) |&gt;\n  diag() |&gt;\n  sqrt()\n\n[1] 0.5008326 0.5008326 0.5008326\n\n\nAs explained in the previous chapter, the conditional variance or prediction error variance of the predicted BLUPs given the data is \\(var(\\hat{u}^{BLUP} \\mid g) = C_{22}\\). However, notice that the marginal variance of \\(\\hat{u}^{BLUP}\\) is \\(var(\\hat{u}^{BLUP}) = G_{(g)} - C_{22(g)}\\). Then, the variance of the difference between two BLUPs, assuming independence, is\n\\[\n\\begin{align*}\n\\operatorname{var}(\\hat{u}^{BLUP}_1 - \\hat{u}^{BLUP}_2)\n&= \\operatorname{var}(\\hat{u}^{BLUP}_1) + \\operatorname{var}(\\hat{u}^{BLUP}_2) - 2\\operatorname{cov}(\\hat{u}^{BLUP}_1, \\hat{u}^{BLUP}_2)\n\\end{align*}\n\\]\nLet’s illustrate the symmetric \\(G - C_{22}\\) matrix that contains the marginal variance of each BLUP in the diagonals and the covariances between BLUPs in the off-diagonals.\n\\[\n\\operatorname{var}(\\hat{u}) =\n\\underbrace{\n\\begin{bmatrix}\n\\sigma_g^2 & 0 & \\cdots & 0 \\\\\n0 & \\sigma_g^2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\sigma_g^2\n\\end{bmatrix}\n}_{G = I} -\n\\,\n\\underbrace{\n\\begin{bmatrix}\n\\operatorname{var}(u_1 - \\hat{u}_1) & \\cdot & \\cdots & \\cdot \\\\\n\\operatorname{cov}(u_1 - \\hat{u}_1, u_2 - \\hat{u}_2) & \\operatorname{var}(u_2 - \\hat{u}_2) & \\cdots & \\cdot \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\operatorname{cov}(u_1 - \\hat{u}_1, u_n - \\hat{u}_n) & \\operatorname{cov}(u_2 - \\hat{u}_2, u_n - \\hat{u}_n) & \\cdots & \\operatorname{var}(u_n - \\hat{u}_n)\n\\end{bmatrix}\n}_{C_{22}}\n\\]\nWhen assuming independence between markers, the off-diagonals in G equals 0, as shown above, and thus \\(cov(\\hat{u}^{BLUP}_1, \\hat{u}^{BLUP}_2) = C_{22[i, j]}\\). The variances and covarainces would then be\n\\[\n\\begin{aligned}\n\\operatorname{var}(\\hat{u}_{i})\n&= G_{[i,i]} - C_{22[i,i]}\n= \\sigma_g^2 + \\operatorname{var}(u_1 - \\hat{u}_1) \\\\[6pt]\n\\operatorname{cov}(\\hat{u}_{i}, \\hat{u}_{j})\n&= \\cancel{G_{[i,j]}} - C_{22[i,j]}\n= -\\,\\operatorname{cov}(u_1 - \\hat{u}_1,\\, u_2 - \\hat{u}_2)\n\\end{aligned}\n\\]\nWe can continue to solve the variance of the differences as\n\\[\n\\begin{align*}\n\\operatorname{var}(\\hat{u}^{BLUP}_1 - \\hat{u}^{BLUP}_2)\n&= \\operatorname{var}(\\hat{u}^{BLUP}_1) + \\operatorname{var}(\\hat{u}^{BLUP}_2) - 2\\operatorname{cov}(\\hat{u}^{BLUP}_1, \\hat{u}^{BLUP}_2)\\\\\n&= \\sigma^2_g - \\operatorname{var}(u_1 - \\hat{u}_1) + \\sigma^2_g - \\operatorname{var}(u_2 - \\hat{u}_2) + 2\\operatorname{cov}(u_1 - \\hat{u}_1, u_2 - \\hat{u}_2) \\\\\n&= 2\\sigma^2_g - (\\operatorname{var}(u_1 - \\hat{u}_1) + \\operatorname{var}(u_2 - \\hat{u}_2) - 2\\operatorname{cov}(u_1 - \\hat{u}_1, u_2 - \\hat{u}_2)) \\\\\n&= 2\\sigma^2_g - \\operatorname{var}((u_1 - \\hat{u}_1) - (u_1 - \\hat{u}_1))\\\\\n&= 2\\sigma^2_g - v^{BLUP}_{\\Delta_{ij}}\n\\end{align*}\n\\]\nThen, we can compute the Matrix of differences between BLUPs as\n\nC_22g &lt;- C_inv[5:8, 5:8]\nvd_BLUP_mat &lt;- function(C) {\n  d &lt;- diag(C)\n  vd &lt;- outer(d, d, \"+\") - 2 * C\n  vd[vd &lt; 0 & abs(vd) &lt; 1e-12] &lt;- 0\n  diag(vd) &lt;- NA\n  return(vd)\n}\nvar_diff_mat &lt;- vd_BLUP_mat(C_22g)\nvar_diff_mat\n\n          geng1     geng2     geng3     geng4\ngeng1        NA 0.2184012 0.2184012 0.2184012\ngeng2 0.2184012        NA 0.2184012 0.2184012\ngeng3 0.2184012 0.2184012        NA 0.2184012\ngeng4 0.2184012 0.2184012 0.2184012        NA\n\n\nCullis et al, 2006 proposed to compute Broad-Sense Heritability using the following formula\n\\[\nH^2_{Cullis} = 1 - \\frac{\\overline v_{\\Delta_{ii}}^{BLUP}}{2\\sigma^2_g}\n\\]\nWhere \\(\\overline v_{\\Delta_{ii}}^{BLUP}\\) represent the mean variance of a difference of two BLUPs for the genotypic effects. Therefore\n\nvdBLUPavg &lt;- mean(var_diff_mat[upper.tri(var_diff_mat, diag = FALSE)])\ncullis &lt;- 1 - vdBLUPavg / (2 * var_g)\ncullis\n\n[1] 0.8190045"
  },
  {
    "objectID": "pages/06_Heritabilities.html#assuming-genotypes-as-fixed-effects",
    "href": "pages/06_Heritabilities.html#assuming-genotypes-as-fixed-effects",
    "title": "Untitled",
    "section": "Assuming Genotypes as Fixed Effects",
    "text": "Assuming Genotypes as Fixed Effects\nNow, instead, we have a model in which the genotype has been fit as Fixed\n\\[\ny \\sim \\mu + block + gen + \\varepsilon\n\\]\nWe can now follow a similar approach to the one explained above to compute the average differences between genotypes fitted as fixed. Thus, we will make use of the \\(C_{11(g)}\\) matrix"
  },
  {
    "objectID": "pages/06_Heritabilities.html#genotypes-as-random-effects",
    "href": "pages/06_Heritabilities.html#genotypes-as-random-effects",
    "title": "Untitled",
    "section": "Genotypes as Random Effects",
    "text": "Genotypes as Random Effects\nWe have the following model where the genotype has been fit as Random\n\\[\ny \\sim \\mu + block + (1|gen) + \\varepsilon\n\\]\nLet’s first extract the variance component using ANOVA\n\nmod &lt;- lm(formula = yield ~ 1 + block + gen, data = data)\naov_table &lt;- as.data.frame(anova(mod))\n\nmse_g &lt;- aov_table[\"gen\", \"Mean Sq\"]\nvar_e &lt;- aov_table[\"Residuals\", \"Mean Sq\"]\nvar_g &lt;- (mse_g - var_e) / n_blks\n\nNow we can build the required matrices\n\nones &lt;- model.matrix(yield ~ 1, data)\nX &lt;- cbind(ones, model.matrix(yield ~ -1 + block, data))\nZ &lt;- model.matrix(yield ~ -1 + gen, data)\ny &lt;- matrix(data[, \"yield\"]) |&gt; na.omit()\n\nG &lt;- diag(x = var_g, nrow = n_gens)\nR &lt;- diag(x = var_e, nrow = n)\nV &lt;- Z %*% G %*% t(Z) + R\n\nAnd solve the Henderson Equation to extract the BLUEs and BLUPs\n\nC11 &lt;- t(X) %*% chol2inv(chol(R)) %*% X\nC12 &lt;- t(X) %*% chol2inv(chol(R)) %*% Z\nC21 &lt;- t(Z) %*% chol2inv(chol(R)) %*% X\nC22 &lt;- t(Z) %*% chol2inv(chol(R)) %*% Z + solve(G)\n\nC &lt;- as.matrix(\n  rbind(\n    cbind(C11, C12),\n    cbind(C21, C22)\n  )\n)\n\nrhs &lt;- rbind(\n  t(X) %*% solve(R) %*% y,\n  t(Z) %*% solve(R) %*% y\n)\n\nC_inv &lt;- ginv(C)\nrownames(C_inv) &lt;- colnames(C_inv) &lt;- rownames(C)\nprint(C_inv)\n\n            (Intercept)      block1      block2      block3       geng1\n(Intercept)  0.10359375  0.03453125  0.03453125  0.03453125 -0.11312500\nblock1       0.03453125  0.07817708 -0.02182292 -0.02182292 -0.03770833\nblock2       0.03453125 -0.02182292  0.07817708 -0.02182292 -0.03770833\nblock3       0.03453125 -0.02182292 -0.02182292  0.07817708 -0.03770833\ngeng1       -0.11312500 -0.03770833 -0.03770833 -0.03770833  0.23273379\ngeng2       -0.11312500 -0.03770833 -0.03770833 -0.03770833  0.12353318\ngeng3       -0.11312500 -0.03770833 -0.03770833 -0.03770833  0.12353318\ngeng4       -0.11312500 -0.03770833 -0.03770833 -0.03770833  0.12353318\n                  geng2       geng3       geng4\n(Intercept) -0.11312500 -0.11312500 -0.11312500\nblock1      -0.03770833 -0.03770833 -0.03770833\nblock2      -0.03770833 -0.03770833 -0.03770833\nblock3      -0.03770833 -0.03770833 -0.03770833\ngeng1        0.12353318  0.12353318  0.12353318\ngeng2        0.23273379  0.12353318  0.12353318\ngeng3        0.12353318  0.23273379  0.12353318\ngeng4        0.12353318  0.12353318  0.23273379\n\nans &lt;- C_inv %*% rhs\nans\n\n                  [,1]\n(Intercept)  5.4375000\nblock1       3.0625000\nblock2       1.4125000\nblock3       0.9625000\ngeng1       -0.6142534\ngeng2        0.2866516\ngeng3       -0.5323529\ngeng4        0.8599548\n\n\nAdditionally, we can extract the Marginal Means of each block as follows\n\nL &lt;- cbind(\n  matrix(1, nrow = n_blks, ncol = 1), # Intercept\n  diag(n_blks) # blocks\n)\nL\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    1    0    0\n[2,]    1    0    1    0\n[3,]    1    0    0    1\n\nL %*% ans[1:4]\n\n     [,1]\n[1,] 8.50\n[2,] 6.85\n[3,] 6.40\n\nL %*% C_inv[1:4, 1:4] %*% t(L) |&gt;\n  diag() |&gt;\n  sqrt()\n\n[1] 0.5008326 0.5008326 0.5008326\n\n\nThe pairwise heritability can be calculated as the squared correltion between the true difference and its predictor/estimator. To compute that correlation using its general formula, we will need to know the covariance between the differences as well as the standard deviations o each one separately.\n\\[\nH^2_{\\Delta 12} =\n\\left(\n\\frac{\\operatorname{cov}(u_1 - u_2, \\hat{u}_1 - \\hat{u}_2)}\n{\\sqrt{\\operatorname{var}(u_1 - u_2)} \\, \\sqrt{\\operatorname{var}(\\hat{u}_1 - \\hat{u}_2)}}\n\\right)^2\n\\]\nAssuming independence with constant variance, the variance of the difference between the true values would be\n\\[\n\\operatorname{var}(u_1 - u_2) = 2\\sigma^2_g - \\cancel{2\\operatorname{cov}(u_1, u_2)} = 2\\sigma^2_g\n\\]\nTo compute the variance of the estimated values we will use the marginal variance of \\(\\hat{u}^{BLUP}\\), which as wxplained in the previous chapter equals\n\\[\nvar(\\hat{u}^{BLUP}) = G_{(g)} - C_{22(g)}\n\\]\nLet’s illustrate that \\(G - C_{22}\\) matrix that contains the marginal variance of each BLUP in the diagonals and the covariances between BLUPs in the off-diagonals.\n\\[\n\\operatorname{var}(\\hat{u}) =\n\\underbrace{\n\\begin{bmatrix}\n\\sigma_g^2 & 0 & \\cdots & 0 \\\\\n0 & \\sigma_g^2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\sigma_g^2\n\\end{bmatrix}\n}_{G = I} -\n\\,\n\\underbrace{\n\\begin{bmatrix}\n\\operatorname{var}(u_1 - \\hat{u}_1) & \\cdot & \\cdots & \\cdot \\\\\n\\operatorname{cov}(u_1 - \\hat{u}_1, u_2 - \\hat{u}_2) & \\operatorname{var}(u_2 - \\hat{u}_2) & \\cdots & \\cdot \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\operatorname{cov}(u_1 - \\hat{u}_1, u_n - \\hat{u}_n) & \\operatorname{cov}(u_2 - \\hat{u}_2, u_n - \\hat{u}_n) & \\cdots & \\operatorname{var}(u_n - \\hat{u}_n)\n\\end{bmatrix}\n}_{C_{22}}\n\\]\nWhen assuming independence between estimates, the off-diagonals in G equals 0, as shown above, and thus \\(cov(\\hat{u}^{BLUP}_1, \\hat{u}^{BLUP}_2) = C_{22[1, 2]}\\). The variances and covariances would then be alculated as follows\n\\[\n\\begin{aligned}\n\\operatorname{var}(\\hat{u}_{1})\n&= G_{[1,1]} - C_{22[1,1]}\n= \\sigma_g^2 + \\operatorname{var}(u_1 - \\hat{u}_1) \\\\[6pt]\n\\operatorname{cov}(\\hat{u}_{1}, \\hat{u}_{2})\n&= \\cancel{G_{[1,2]}} - C_{22[1,2]}\n= -\\,\\operatorname{cov}(u_1 - \\hat{u}_1,\\, u_2 - \\hat{u}_2)\n\\end{aligned}\n\\]\nSo, with this information we can now calculate the variance of the difference between two BLUPs, which under the assumption of independent genotypes would equal\n\\[\n\\begin{align*}\n\\operatorname{var}(\\hat{u}^{BLUP}_1 - \\hat{u}^{BLUP}_2)\n&= \\operatorname{var}(\\hat{u}^{BLUP}_1) + \\operatorname{var}(\\hat{u}^{BLUP}_2) - 2\\operatorname{cov}(\\hat{u}^{BLUP}_1, \\hat{u}^{BLUP}_2)\\\\\n&= \\sigma^2_g - \\operatorname{var}(u_1 - \\hat{u}_1) + \\sigma^2_g - \\operatorname{var}(u_2 - \\hat{u}_2) + 2\\operatorname{cov}(u_1 - \\hat{u}_1, u_2 - \\hat{u}_2) \\\\\n&= 2\\sigma^2_g - (\\operatorname{var}(u_1 - \\hat{u}_1) + \\operatorname{var}(u_2 - \\hat{u}_2) - 2\\operatorname{cov}(u_1 - \\hat{u}_1, u_2 - \\hat{u}_2)) \\\\\n&= 2\\sigma^2_g - \\operatorname{var}((u_1 - \\hat{u}_1) - (u_1 - \\hat{u}_1))\\\\\n&= 2\\sigma^2_g - v^{BLUP}_{\\Delta_{12}}\n\\end{align*}\n\\]\nThen, we can compute the Matrix of differences between BLUPs as\n\nC_22g &lt;- C_inv[5:8, 5:8]\nvd_BLUP_mat &lt;- function(C) {\n  d &lt;- diag(C)\n  vd &lt;- outer(d, d, \"+\") - 2 * C\n  vd[vd &lt; 0 & abs(vd) &lt; 1e-12] &lt;- 0\n  diag(vd) &lt;- NA\n  return(vd)\n}\nvar_diff_mat &lt;- vd_BLUP_mat(C_22g)\nvar_diff_mat\n\n          geng1     geng2     geng3     geng4\ngeng1        NA 0.2184012 0.2184012 0.2184012\ngeng2 0.2184012        NA 0.2184012 0.2184012\ngeng3 0.2184012 0.2184012        NA 0.2184012\ngeng4 0.2184012 0.2184012 0.2184012        NA\n\n\nThe last component left is then to compute the covariance between differences\n\\[\n\\begin{align*}\n\\operatorname{cov}(u_{1} - u_2, \\hat{u}_1 - \\hat{u}_2)\n&= \\operatorname{cov}(u_{1}, \\hat{u}_1) - \\operatorname{cov}(u_{1}, \\hat{u}_2) - \\operatorname{cov}(u_{2}, \\hat{u}_1) + \\operatorname{cov}(u_{2}, \\hat{u}_2)\\\\\n&= (G-C_{22})_{[1,1]} - (G-C_{22})_{[1,2]} - (G-C_{22})_{[2,1]} + (G-C_{22})_{[2,2]} \\\\\n&= \\sigma^2_g - C_{22[1,1]} + C_{22[1,2]} + C_{22[1,2]}  + \\sigma^2_g - C_{22[2,2]} \\\\\n&= 2\\sigma^2_g - v^{BLUP}_{\\Delta_{12}}\n\\end{align*}\n\\]\nFinally, we can solve the Heritability as the square correlation of the differences between the true values and the predictors\n\\[\nH^2_{{\\Delta 12}_{BLUP}} =\n\\left(\n\\frac{\\operatorname{cov}(u_1 - u_2, \\hat{u}_1 - \\hat{u}_2)}\n{\\sqrt{\\operatorname{var}(u_1 - u_2)} \\, \\sqrt{\\operatorname{var}(\\hat{u}_1 - \\hat{u}_2)}}\n\\right)^2 =\n\\frac{(2\\sigma_g^2 - v^{\\text{BLUP}}_{\\Delta_{12}})^2}\n{2\\sigma_g^2 \\left( 2\\sigma_g^2 - v^{\\text{BLUP}}_{\\Delta_{12}} \\right)}\n=\n\\frac{2\\sigma_g^2 - v^{\\text{BLUP}}_{\\Delta_{12}}}{2\\sigma_g^2}\n\\]\nThis expression can be simplified to the Cullis Heritability (Cullis et al, 2006), where \\(\\overline v_{\\Delta_{ii}}^{BLUP}\\) represent the mean variance of a difference of two BLUPs for the genotypic effects\n\\[\nH^2_{Cullis} = 1 - \\frac{\\overline v_{\\Delta_{ii}}^{BLUP}}{2\\sigma^2_g}\n\\]\n\nvdBLUPavg &lt;- mean(var_diff_mat[upper.tri(var_diff_mat, diag = FALSE)])\ncullis &lt;- 1 - vdBLUPavg / (2 * var_g)\ncullis\n\n[1] 0.8190045\n\n\nIt is important to notice that this way to compute heritability only holds under the assumption that the genotypes are independent. Otherwise, a more generalized approach, as described by (Schmidt et al, 2019) should be used\n\\[\n\\begin{align*}\nH^2_{{\\Delta 12}_{BLUP}}\n&= \\frac{\\operatorname{var}(g_1 - g_2) - v^{BLUP}_{\\Delta 12}}{\\operatorname{var}(g_1 - g_2)}\\\\\n&= \\frac{\\sigma^2_{g(1,1)} + \\sigma^2_{g(2,2)} - 2\\sigma^2_{g(1,2)} -  v^{BLUP}_{\\Delta 12}}{\\sigma^2_{g(1,1)} + \\sigma^2_{g(2,2)} - 2\\sigma^2_{g(1,2)}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "pages/08_Parametrizations.html",
    "href": "pages/08_Parametrizations.html",
    "title": "Parametrizations",
    "section": "",
    "text": "In this chapter we will go through the derivation and assumptions of the two most common parametrizations to model additivity and dominance for both diploids and autotetraploid species. Please, refer to Vitezica et al, 2013 and Endelman et al, 2018 for further explanations.",
    "crumbs": [
      "Parametrizations",
      "Breeding vs Genotypic"
    ]
  },
  {
    "objectID": "pages/08_Parametrizations.html#breeding-values-u",
    "href": "pages/08_Parametrizations.html#breeding-values-u",
    "title": "Parametrizations",
    "section": "Breeding Values (\\(u\\))",
    "text": "Breeding Values (\\(u\\))\nWhere \\(u\\) is the Breeding Value and \\(v\\) is the dominance deviation. The \\(u\\) of an individual is defined as a function of the additive substitution effect \\(\\alpha\\). This parameter is computed as the difference of the alleles substitution effects, which represent the mean deviation from the population mean of those individuals which received this particular allele from one parent and the second allele at random from the population. Therefore\n\\[\n\\alpha_A = \\mu_A - \\mu\n\\] \\[\n\\alpha_a = \\mu_a - \\mu\n\\]\nConsider the average effect of Allele \\(A\\)\n\n\n\nGenotypic Values For a Loxus\n\n\nAllele.from.other.Parent\nProbability\nGenotype\nValue\n\n\n\n\nA\n$p$\n$Aa$\n$a$\n\n\na\n$q$\n$aa$\n$d$\n\n\n\n\n\n\n\nAnd the average effect of Allele \\(a\\)\n\n\n\nGenotypic Values For a Loxus\n\n\nAllele.from.other.Parent\nProbability\nGenotype\nValue\n\n\n\n\nA\n$p$\n$Aa$\n$d$\n\n\na\n$q$\n$aa$\n$-a$\n\n\n\n\n\n\n\nThen\n\\[\n\\mu_A = dp + dq\n\\] \\[\n\\mu_a = dp - aq\n\\]\nThus\n\\[\n\\begin{align*}\n\\alpha_A &= ap + dq - (a(p - q) + 2pqd) \\\\\n         &= ap + dq - ap + aq - 2pqd \\\\\n         &= q(a - 2pd + d) \\\\\n         &= q(a + d(-2p + 1)) \\\\\n         &= q(a + d(-2p + p + q)) \\\\\n         &= q(a + d(q - p))\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n\\alpha_a &= ap + dq - (a(p - q) + 2pqd) \\\\\n         &= dp - aq - ap + aq - 2pqd \\\\\n         &= p(-a + d(-2q + 1)) \\\\\n         &= -p(a + d(2q - (p + q))) \\\\\n         &= -p(a + d(q - p))\n\\end{align*}\n\\]\nNow we can solve\n\\[\n\\begin{align*}\n\\alpha &= q(a + d(q - p)) - (-p(a - d(q + p)))\\\\\n       &= a + d(q - p)\n\\end{align*}\n\\]\nWe can also express the allele substitution effects of each allele in terms of \\(\\alpha\\)\n\\[\n\\alpha_A = q(a + d(q - p)) = q\\alpha\n\\] \\[\n\\alpha_a = -p(a + d(q - p)) = -p\\alpha\n\\]\nThus, for a single locus with two alleles\n\\[\n\\mathbf{u} =\n\\begin{cases}\n    (2 - 2p)\\alpha = 2q\\alpha\\\\\n    (1 - 2p)\\alpha = (q - p)\\alpha\\\\\n    (0 - 2p)\\alpha = -2p\\alpha\n\\end{cases}\n\\]\nSo, the breeding values of a set of individuals is \\(\\mathbf{u} = \\mathbf{Z}\\alpha = \\mathbf{Z}a + \\mathbf{Z}(q - p)d\\) where \\(\\mathbf{Z}\\) is the centered marker matrix\n\\[\n\\mathbf{Z} =\n\\begin{cases}\n    2q, & \\text{for } A_1A_1\\\\\n    q - p, & \\text{for } A_1A_2\\\\\n    -2p, & \\text{for } A_2A_2\n\\end{cases}\n\\]",
    "crumbs": [
      "Parametrizations",
      "Breeding vs Genotypic"
    ]
  },
  {
    "objectID": "pages/08_Parametrizations.html#dominance-deviations-v",
    "href": "pages/08_Parametrizations.html#dominance-deviations-v",
    "title": "Parametrizations",
    "section": "Dominance Deviations (\\(v\\))",
    "text": "Dominance Deviations (\\(v\\))\nAlternatively, the dominance deviations can be computed as the difference between the actual genotypic value and the genotypic value predicted from the sum of the average effects\n\\[\n\\begin{align*}\nG_{ij} &= \\mu_G + \\alpha_i + \\alpha_j + v{ij} \\\\\n\\hat{G}_{ij} &= \\mu_G + \\alpha_i + \\alpha_j \\\\\nv_{ij} &= G_{ij} - \\hat{G}_{ij} \\\\\n             &= G_{ij} - (\\mu_G + \\alpha_i + \\alpha_j) \\\\\n             &= G_{ij} - \\mu_G - u_{ij}\n\\end{align*}\n\\] So, assuming that\n\n\n\nGenotypic Values For a Loxus\n\n\nGenotypes\nFrequency\nValue\nu\n\n\n\n\naa\nq^2\nm - a\n$2q\\alpha$\n\n\nAa\n2pq\nm + d\n$(q - p)\\alpha$\n\n\nAA\np^2\nm + a\n$-2p\\alpha$\n\n\n\n\n\n\n\nWe can solve that\n\\[\n\\begin{align*}\nv_{A_1, A_1}\n&= a - \\big(a(p-q) + 2pq d\\big) -2q\\alpha \\\\\n&= a -ap + aq -2pqd -2q(a+ d(q-p))\\\\\n&= a -ap + aq -2pqd -2aq -2q^2d + 2pqd \\\\\n&= a - ap -aq -2q^2d \\\\\n&= -2q^2d\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\nv_{A_1, A_2}\n&= d - \\big(a(p-q) + 2pq d\\big) - (q-p)\\big(a + d(q-p)\\big) \\\\\n&= d - a(p-q) - 2pq d - (q-p)a - d(q-p)^2 \\\\\n&= d - 2pq d - d(q-p)^2 \\\\\n&= d\\Big(1 - 2pq - (q-p)^2\\Big) \\\\\n&= d\\big(1 - p^2 - q^2\\big) \\\\\n&= d(2pq) \\\\\n&= 2pq\\,d\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\nv_{A_2, A_2} &= -a - (a(p - q) + 2pqd) - (-2p\\alpha) \\\\\n    &= -a -ap + aq - 2pqd + 2p(a + d(q - p )) \\\\\n    &= -a -ap + aq - 2pqd + 2pa + 2pdq - 2p^2d \\\\\n    &= -a + aq + pa - 2p^2d \\\\\n    &= -a + a(p + q) - 2p^2d \\\\\n    &= - 2p^2d \\\\\n\\end{align*}\n\\]\nThen\n\\[\nv =\n\\begin{cases}\n    -2q^2d\\\\\n    2pqd\\\\\n    - 2p^2d\n\\end{cases}\n\\]\nIn summary, \\(v = Wd\\), where \\(W\\) is the centered marker matrix\n\\[\nW =\n\\begin{cases}\n    -2q^2,  & \\text{for } A_1A_1\\\\\\\\\n    2pq,  & \\text{for } A_1A_2\\\\\\\\\n    - 2p^2,  & \\text{for } A_2A_2\\\\\n\\end{cases}\n\\]\nBesides that, it is important to note two things\n\\[\n\\begin{align*}\nE(u) &= p^2(2q\\alpha) + 2pq(q-p)\\alpha + q^2(-2p\\alpha) \\\\\n  &= 2p^2q\\alpha + 2pq^2\\alpha - 2p^2q\\alpha -2pq^2\\alpha \\\\\n  &= 0\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\nE(v) &= p^2(-2q^2d) + 2pq2pqd + q^2(-2p^2d) \\\\\n  &= -2p^2q^2d + 4p^2q^2d -2p^2q^2d\\\\\n  &= 0\n\\end{align*}\n\\]\nTherefore, we can described genetic variance as\n\\[\n\\operatorname{var}(G) = \\operatorname{var}(A) + \\operatorname{var}(D)  \n\\] \\[\n\\begin{align*}\n\\operatorname{var}(A) &= \\operatorname{var}(u) = E[(u−E(u))^2]=E(u^2)− \\cancel{[E(u)]^2} \\\\\n  &= p^2(2q\\alpha)^2 + 2pq((q-p)\\alpha)^2 + q^2(-2p\\alpha)^2\\\\\n  &= 4p^2q^2\\alpha^2 + 2pq(q-p)^2\\alpha^2 +4 p^2q^2\\alpha^2 \\\\\n  &= 2pq\\alpha^2(2pq + (q-p)^2 + 2pq) \\\\\n  &= 2pq\\alpha^2(2pq + p^2 + q^2 -2pq + 2pq) \\\\\n  &= 2pq\\alpha^2(p^2 + q^2 + 2pq) \\\\\n  &= 2pq\\alpha^2 = 2pq[a + d(1-p)]^2\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n\\operatorname{var}(D) &= \\operatorname{var}(v) = E[(v−E(v))^2]=E(v^2)−\\cancel{[E(v)]^2} \\\\\n  &= p^2(-2q^2d)^2 + 2pq(2pqd)^2 + q^2(-2p^2d) ^2 \\\\\n  &= 4p^2q^4d^2 + 8p^3q^3d^2 + 4p^4q^2d^2 \\\\\n  &= 4p^2q^2d^2(q^2 + 2pq + p^2) \\\\\n  &= 4p^2q^2d^2 = (2pqd)^2\n\\end{align*}\n\\]\nTherefore\n\\[\n\\operatorname{var}(G) = 2pq[a + d(1-p)]^2 + (2pqd)^2\n\\]\nAs you can see, this parametrization assumes that there is no covariance between the additive and dominance, that is, that they are independent. Besides that, if we assume that \\(a \\sim N(0, \\sigma^2_a)\\) and \\(d \\sim N(0, \\sigma^2_d)\\), then\n\\[\n\\begin{align*}\n\\operatorname{cov}(u) &= \\operatorname{var}(Z\\alpha) \\\\\n  &= \\operatorname{var}(Z(a + d(p-q))) \\\\\n  &= ZZ'\\operatorname{var}(a) + ZZ'(p-q)^2\\operatorname{var}(d) \\\\\n  &=  ZZ'\\sigma^2_a + ZZ'(p-q)^2\\sigma^2_d\n\\end{align*}\n\\]\nAnd\n\\[\n\\begin{aligned}\n\\sigma_A^2 &= \\operatorname{Var}(u)\n= E\\big[(z a + z(q - p)d)^2\\big] \\\\\n&= E[z^2]\\,E[a^2] + 2(q - p)E[z^2]\\,E[a d] + (q - p)^2 E[z^2]\\,E[d^2] \\\\\n&= E[z^2]\\big(\\sigma_a^2 + (q - p)^2\\sigma_d^2\\big) \\\\\n\\end{aligned}\n\\]\nGiven that\n\\[\n\\begin{aligned}\nE[z^2] &= p^2(2q)^2 + 2pq(q - p)^2 + q^2(2p)^2 \\\\[6pt]\n&= 4p^2q^2 + 2pq(q^2 - 2pq + p^2) + 4p^2q^2 \\\\[6pt]\n&= 8p^2q^2 + 2pq(p^2 + q^2 - 2pq) \\\\[6pt]\n&= 2pq(p^2 + q^2 + 4pq - 2pq) = 2pq \\\\[6pt]\n\\end{aligned}\n\\]\nTherefore\n\\[\n\\sigma_A^2 = 2pq\\big(\\sigma_a^2 + (q - p)^2\\sigma_d^2\\big) ;\\\\\n\\frac{\\sigma_A^2}{2pq} = (\\sigma_a^2 + (q - p)^2\\sigma_d^2\\big)\n\\]\nSo,\n\\[\n\\begin{align*}\n\\operatorname{cov}(u) &= ZZ'(\\sigma^2_a + (p-q)^2\\sigma^2_d) \\\\\n  &= ZZ'\\frac{\\sigma_A^2}{2pq} \\\\\n  &= \\frac{ZZ'}{2pq}\\sigma_A^2 \\\\\n  &= G\\sigma^2_A\n\\end{align*}\n\\]\nA similar Derivation can be made for dominance\n\\[\n\\operatorname{\\sigma^2_D} = (2pq)^2\\sigma^2_d\n\\]\n\\[\n\\begin{align*}\n\\operatorname{cov}(v) &= \\operatorname{var}(Wd) \\\\\n  &= WW'\\sigma^2_d \\\\\n  &= \\frac{WW'}{(2pq)^2}\\sigma^2_D\n\\end{align*}\n\\]"
  },
  {
    "objectID": "pages/08_Parametrizations.html#dominance-deviations-operatornamev",
    "href": "pages/08_Parametrizations.html#dominance-deviations-operatornamev",
    "title": "Parametrizations",
    "section": "Dominance Deviations (\\(\\operatorname{v}\\))",
    "text": "Dominance Deviations (\\(\\operatorname{v}\\))\nAlternatively, the dominance deviations can be computed as the difference between the actual genotypic value and the genotypic value predicted from the sum of the average effects\n\\[\n\\begin{align*}\nG_{ij} &= \\mu_G + \\alpha_i + \\alpha_j + v{ij} \\\\\n\\hat{G}_{ij} &= \\mu_G + \\alpha_i + \\alpha_j \\\\\nv_{ij} &= G_{ij} - \\hat{G}_{ij} \\\\\n             &= G_{ij} - (\\mu_G + \\alpha_i + \\alpha_j) \\\\\n             &= G_{ij} - \\mu_G - u_{ij}\n\\end{align*}\n\\] So, assuming that\n\n\n\nGenotypic Values For a Loxus\n\n\nGenotypes\nFrequency\nValue\nu\n\n\n\n\naa\nq^2\nm - a\n$2q\\alpha$\n\n\nAa\n2pq\nm + d\n$(q - p)\\alpha$\n\n\nAA\np^2\nm + a\n$-2p\\alpha$\n\n\n\n\n\n\n\nWe can solve that\n\\[\n\\begin{align*}\nv_{A_1, A_1}\n&= a - \\big(a(p-q) + 2pq d\\big) -2q\\alpha \\\\\n&= a -ap + aq -2pqd -2q(a+ d(q-p))\\\\\n&= a -ap + aq -2pqd -2aq -2q^2d + 2pqd \\\\\n&= a - ap -aq -2q^2d \\\\\n&= -2q^2d\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\nv_{A_1, A_2}\n&= d - \\big(a(p-q) + 2pq d\\big) - (q-p)\\big(a + d(q-p)\\big) \\\\\n&= d - a(p-q) - 2pq d - (q-p)a - d(q-p)^2 \\\\\n&= d - 2pq d - d(q-p)^2 \\\\\n&= d(1 - 2pq - (q-p)^2) \\\\\n&= d\\big(1 - p^2 - q^2\\big) \\\\\n&= d(2pq) \\\\\n&= 2pq\\,d\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\nv_{A_2, A_2} &= -a - (a(p - q) + 2pqd) - (-2p\\alpha) \\\\\n    &= -a -ap + aq - 2pqd + 2p(a + d(q - p )) \\\\\n    &= -a -ap + aq - 2pqd + 2pa + 2pdq - 2p^2d \\\\\n    &= -a + aq + pa - 2p^2d \\\\\n    &= -a + a(p + q) - 2p^2d \\\\\n    &= - 2p^2d \\\\\n\\end{align*}\n\\]\nThen\n\\[\n\\mathbf{v} =\n\\begin{cases}\n    -2q^2d\\\\\n    2pqd\\\\\n    - 2p^2d\n\\end{cases}\n\\]\nIn summary, \\(\\mathbf{v} = \\mathbf{W}d\\), where \\(W\\) is the centered marker matrix\n\\[\nW =\n\\begin{cases}\n    -2q^2,  & \\text{for } A_1A_1\\\\\\\\\n    2pq,  & \\text{for } A_1A_2\\\\\\\\\n    - 2p^2,  & \\text{for } A_2A_2\\\\\n\\end{cases}\n\\]\nBesides that, it is important to note that\n\\[\n\\begin{align*}\nE(u) &= p^2(2q\\alpha) + 2pq(q-p)\\alpha + q^2(-2p\\alpha) \\\\\n  &= 2p^2q\\alpha + 2pq^2\\alpha - 2p^2q\\alpha -2pq^2\\alpha \\\\\n  &= 0\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\nE(v) &= p^2(-2q^2d) + 2pq2pqd + q^2(-2p^2d) \\\\\n  &= -2p^2q^2d + 4p^2q^2d -2p^2q^2d\\\\\n  &= 0\n\\end{align*}\n\\]\nTherefore, we can described genetic variance as\n\\[\n\\operatorname{var}(G) = \\operatorname{var}(A) + \\operatorname{var}(D)  \n\\] \\[\n\\begin{align*}\n\\operatorname{var}(A) &= \\operatorname{var}(u) = E[(u−E(u))^2]=E(u^2)− \\cancel{[E(u)]^2} \\\\\n  &= p^2(2q\\alpha)^2 + 2pq((q-p)\\alpha)^2 + q^2(-2p\\alpha)^2\\\\\n  &= 4p^2q^2\\alpha^2 + 2pq(q-p)^2\\alpha^2 +4 p^2q^2\\alpha^2 \\\\\n  &= 2pq\\alpha^2(2pq + (q-p)^2 + 2pq) \\\\\n  &= 2pq\\alpha^2(2pq + p^2 + q^2 -2pq + 2pq) \\\\\n  &= 2pq\\alpha^2(p^2 + q^2 + 2pq) \\\\\n  &= 2pq\\alpha^2 = 2pq[a + d(1-p)]^2\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n\\operatorname{var}(D) &= \\operatorname{var}(v) = E[(v−E(v))^2]=E(v^2)−\\cancel{[E(v)]^2} \\\\\n  &= p^2(-2q^2d)^2 + 2pq(2pqd)^2 + q^2(-2p^2d) ^2 \\\\\n  &= 4p^2q^4d^2 + 8p^3q^3d^2 + 4p^4q^2d^2 \\\\\n  &= 4p^2q^2d^2(q^2 + 2pq + p^2) \\\\\n  &= 4p^2q^2d^2 = (2pqd)^2\n\\end{align*}\n\\]\nTherefore\n\\[\n\\operatorname{var}(G) = 2pq[a + d(1-p)]^2 + (2pqd)^2\n\\]\nAs you can see, this parametrization assumes that there is no covariance between the additive and dominance, that is, that they are independent. Besides that, if we assume that \\(a \\sim N(0, \\sigma^2_a)\\) and \\(d \\sim N(0, \\sigma^2_d)\\), then\n\\[\n\\begin{align*}\n\\operatorname{cov}(\\mathbf{u}) &= \\operatorname{var}(\\mathbf{Z}\\alpha) \\\\\n  &= \\operatorname{var}(\\mathbf{Z}(\\mathbf{a} + \\mathbf{d}(p-q))) \\\\\n  &= \\mathbf{ZZ'}\\operatorname{var}(\\mathbf{a}) + \\mathbf{ZZ'}(p-q)^2\\operatorname{var}(\\mathbf{d}) \\\\\n  &=  \\mathbf{ZZ'}\\sigma^2_a + \\mathbf{ZZ'}(p-q)^2\\sigma^2_d\n\\end{align*}\n\\]\nTaking into account that\n\\[\n\\begin{aligned}\n\\sigma_A^2 &= \\operatorname{Var}(\\mathbf{u})\n= E\\big[(\\mathbf{Z} a + \\mathbf{Z}(q - p)d)^2\\big] \\\\\n&= E[\\mathbf{Z}^2]\\,E[\\mathbf{a}^2] + 2(q - p)E[\\mathbf{Z}^2]\\,E[\\mathbf{a} \\mathbf{d}] + (q - p)^2 E[\\mathbf{Z}^2]\\,E[\\mathbf{d}^2] \\\\\n&= E[\\mathbf{Z}^2]\\big(\\sigma_a^2 + (q - p)^2\\sigma_d^2\\big) \\\\\n\\end{aligned}\n\\]\nAnd given that\n\\[\n\\begin{align*}\nE[\\mathbf{Z}^2] &= p^2(2q)^2 + 2pq(q - p)^2 + q^2(2p)^2 \\\\[6pt]\n&= 4p^2q^2 + 2pq(q^2 - 2pq + p^2) + 4p^2q^2 \\\\[6pt]\n&= 8p^2q^2 + 2pq(p^2 + q^2 - 2pq) \\\\[6pt]\n&= 2pq(p^2 + q^2 + 4pq - 2pq) = 2pq \\\\[6pt]\n\\end{align*}\n\\]\nWe can solve\n\\[\n\\sigma_A^2 = 2pq\\big(\\sigma_a^2 + (q - p)^2\\sigma_d^2\\big) ;\\\\\n\\frac{\\sigma_A^2}{2pq} = (\\sigma_a^2 + (q - p)^2\\sigma_d^2\\big)\n\\]\nSo that finally\n\\[\n\\begin{align*}\n\\operatorname{cov}(\\mathbf{u})\n&= \\mathbf{ZZ'} \\big(\\sigma_a^2 + (p-q)^2 \\sigma_d^2\\big) \\\\\n&= \\mathbf{ZZ'} \\frac{\\sigma_A^2}{2pq} \\\\\n&= \\frac{\\mathbf{ZZ'}}{2pq} \\, \\sigma_A^2 \\\\\n&= \\mathbf{G} \\, \\sigma_A^2\n\\end{align*}\n\\]\nA similar Derivation can be made for dominance\n\\[\n\\operatorname{\\sigma^2_D} = (2pq)^2\\sigma^2_d\n\\]\n\\[\n\\begin{align*}\n\\operatorname{cov}(\\mathbf{v})\n&= \\operatorname{var}(\\mathbf{W}\\mathbf{d}) \\\\\n&= \\mathbf{WW'} \\, \\sigma_d^2 \\\\\n&= \\frac{\\mathbf{WW'}}{(2pq)^2} \\, \\sigma_D^2 \\\\\n&= \\mathbf{D} \\, \\sigma_D^2\n\\end{align*}\n\\]",
    "crumbs": [
      "Parametrizations",
      "Breeding vs Genotypic"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html#reconstructing-means",
    "href": "pages/00_start_lm.html#reconstructing-means",
    "title": "Understanding a linear model",
    "section": "5) Reconstructing means",
    "text": "5) Reconstructing means\nWith block and gen as factors and an intercept present, R uses treatment (baseline) coding by default. The printed beta therefore contains:\n\nbeta[1]: the intercept (no longer overall mean)\nbeta[2:3]: effects for non-reference blocks\nbeta[4:6]: effects for non-reference genotypes\n\nYou can reconstruct overall mean, genotype means, and block means as follows.\n\n# Number of coefficients in full model\nn_coef &lt;- length(beta)\n\n# Overall mean reconstructed from coefficients\nmu_recon &lt;- beta[1] + sum(beta[2:3]) / n_blks + sum(beta[4:6]) / n_gens\n\n# Compare with the intercept-only estimate\nmu_recon - beta_mu[1]\n\n[1] 7.993606e-15\n\n\n\n5.1 Genotype means including the missing (baseline) level\n\n# beta currently has: (Intercept), block2, block3, gen2, gen3, gen4\n# Create a named vector for gen effects including the reference level set to 0\nprint(beta)\n\n             [,1]\n(Intercept)  7.75\nblock2      -1.65\nblock3      -2.10\ngeng2        1.10\ngeng3        0.10\ngeng4        1.80\n\ngens &lt;- c(\"geng1\" = 0, beta[4:6, ])\n# Add back the intercept and average block effect\ngens &lt;- beta[1] + sum(beta[2:3]) / n_blks + gens\ngens\n\ngeng1 geng2 geng3 geng4 \n  6.5   7.6   6.6   8.3 \n\n\n\n\n5.2 Block means including the missing (baseline) level\n\nprint(beta)\n\n             [,1]\n(Intercept)  7.75\nblock2      -1.65\nblock3      -2.10\ngeng2        1.10\ngeng3        0.10\ngeng4        1.80\n\nblks &lt;- c(\"block1\" = 0, beta[2:3, ])\nblks &lt;- beta[1] + sum(beta[4:6]) / n_gens + blks\nblks\n\nblock1 block2 block3 \n  8.50   6.85   6.40",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "pages/08_Parametrizations.html#breeding-parametrization",
    "href": "pages/08_Parametrizations.html#breeding-parametrization",
    "title": "Parametrizations",
    "section": "1) Breeding Parametrization",
    "text": "1) Breeding Parametrization\n\nTheoryProofs\n\n\nThis approach separate the Genotypic Value (\\(G\\)) of an individual into the breeding value (\\(u\\)) and dominance deviations (\\(v\\))\n\\[\nG = \\mu_G + u + v\n\\]\n\nBreeding Values (\\(u\\))\nThis value is defined as a function of the additive substitution effect \\(\\alpha\\). This parameter is computed as the difference of the alleles substitution effects, which represent the mean deviation from the population mean of the individuals receiving this particular allele from one parent and the second allele at random from the population.\n\\[\n\\begin{align*}\n\\alpha &=  a + d(q - p)\n\\end{align*}\n\\]\nThus, for a single locus with two alleles\n\\[\nu =\n\\begin{cases}\n    (0 - 2p)\\alpha = -2p\\alpha\\\\\n    (1 - 2p)\\alpha = (q - p)\\alpha\\\\\n    (2 - 2p)\\alpha = 2q\\alpha\\\\\n\\end{cases}\n\\]\nSo, the breeding values of a set of individuals is \\(\\mathbf{u} = \\mathbf{Z}\\alpha = \\mathbf{Z}a + \\mathbf{Z}(q - p)d\\) where \\(\\mathbf{Z}\\) is the centered marker matrix, so that \\(\\mathbf{Z} = (z_1...z_n)\\) including all markers is identical to \\(\\mathbf{T}\\) but centered. Then\n\\[\nz_i =\n\\begin{cases}\n    -2p, & \\text{for } A_1A_1\\\\\n    q - p, & \\text{for } A_1A_2\\\\\n     2q, & \\text{for } A_2A_2\n\\end{cases}\n\\] #### Dominance Deviations\nAlternatively, the dominance deviations can be computed as the difference between the actual genotypic value and the genotypic value predicted from the sum of the average effects\n\\[\n\\begin{align*}\nG_{ij} &= \\mu_G + \\alpha_i + \\alpha_j + v{ij} \\\\\n\\hat{G}_{ij} &= \\mu_G + \\alpha_i + \\alpha_j \\\\\nv_{ij} &= G_{ij} - \\hat{G}_{ij} \\\\\n             &= G_{ij} - (\\mu_G + \\alpha_i + \\alpha_j) \\\\\n             &= G_{ij} - \\mu_G - u_{ij}\n\\end{align*}\n\\]\nThen\n\\[\nv =\n\\begin{cases}\n    - 2p^2d \\\\\n    2pqd\\\\\n    - 2q^2d\n\\end{cases}\n\\]\nIn summary, \\(\\mathbf{v} = \\mathbf{W}d\\), where \\(W = (w_1...w_n)\\) is not \\(\\mathbf{X}\\) in the model and\n\\[\nw_i =\n\\begin{cases}\n    - 2p^2,  & \\text{for } A_1A_1\\\\\\\\\n    2pq,  & \\text{for } A_1A_2\\\\\\\\\n    -2q^,  & \\text{for } A_2A_2\\\\\n\\end{cases}\n\\]\nBesides that, it is important to note that the expected Value of both \\(u\\) and \\(v\\) equals 0. Under this parametrization, we will assume that there is no covariance between additive and dominance, and therefore we can also express the genetic variance for a single locus simply as\n\\[\n\\operatorname{var}(G) = \\operatorname{var}(A) + \\operatorname{var}(D)  \n\\]\nWhere\n\\[\n\\operatorname{var}(A) = 2pq[a + d(1-p)]^2 + (2pqd)^2\n\\]\n\\[\n\\operatorname{var}(D) = (2pqd)^2\n\\]\nBesides that, if we assume that \\(a \\sim N(0, \\sigma^2_a)\\) and \\(d \\sim N(0, \\sigma^2_d)\\), and that \\(\\sigma_A^2 = 2pq\\big(\\sigma_a^2 + (q - p)^2\\sigma_d^2\\big)\\)\n\\[\n\\begin{align*}\n\\operatorname{cov}(\\mathbf{u})\n&=\\operatorname{cov}(\\mathbf{Z}\\alpha) \\\\\n&= \\operatorname{var}(\\mathbf{Z}(\\mathbf{a} + \\mathbf{d}(p-q))) \\\\\n&= \\mathbf{ZZ'}\\operatorname{var}(\\mathbf{a}) + \\mathbf{ZZ'}(p-q)^2\\operatorname{var}(\\mathbf{d}) \\\\\n  &=  \\mathbf{ZZ'}\\sigma^2_a + \\mathbf{ZZ'}(p-q)^2\\sigma^2_d \\\\\n&= \\mathbf{ZZ'} \\big(\\sigma_a^2 + (p-q)^2 \\sigma_d^2\\big) \\\\\n&= \\mathbf{ZZ'} \\frac{\\sigma_A^2}{2pq} \\\\\n&= \\frac{\\mathbf{ZZ'}}{2pq} \\, \\sigma_A^2 \\\\\n&= \\mathbf{G} \\, \\sigma_A^2\n\\end{align*}\n\\]\nWhich equals the VanRaden method to compute the Additive Genomic Relationship Matrix. A similar Derivation can be made for dominance deviations, considering that \\(\\operatorname{\\sigma^2_D} = (2pq)^2\\sigma^2_d\\)\n\\[\n\\begin{align*}\n\\operatorname{cov}(\\mathbf{v})\n&= \\operatorname{var}(\\mathbf{W}\\mathbf{d}) \\\\\n&= \\mathbf{WW'} \\, \\sigma_d^2 \\\\\n&= \\frac{\\mathbf{WW'}}{(2pq)^2} \\, \\sigma_D^2 \\\\\n&= \\mathbf{D} \\, \\sigma_D^2\n\\end{align*}\n\\]\n\n\n\n\nAllele substitution effects\n\nThe allele substitution are by definition the differrence between the mean of the population carrying the allele and the overall population mean. Thus\n\\[\n\\alpha_{A_1} = \\mu_{A_1} - \\mu\n\\] \\[\n\\alpha_{A_2} = \\mu_{A_2} - \\mu\n\\]\nSo we need to compute \\(\\mu_{A_1}\\) and \\(\\mu_{A_2}\\)\nLet’s consider then the average effect of Allele \\({A_1}\\)\n\n\n\nGenotypic Values For a Loxus\n\n\nAllele.from.other.Parent\nProbability\nGenotype\nValue\n\n\n\n\n$A_1$\n$p$\n$A_1A_1$\n$a$\n\n\n$A_2$\n$q$\n$A_1A_2$\n$d$\n\n\n\n\n\n\n\nAnd the average effect of Allele \\(A_2\\)\n\n\n\nGenotypic Values For a Loxus\n\n\nAllele.from.other.Parent\nProbability\nGenotype\nValue\n\n\n\n\n$A_1$\n$p$\n$A_1A_2$\n$d$\n\n\n$A_2$\n$q$\n$A_2A_2$\n$-a$\n\n\n\n\n\n\n\nWe can simply compute the subpopulations means as\n\\[\n\\mu_{A_1} = ap + dq\n\\] \\[\n\\mu_{A_2} = dp - aq\n\\]\nWe can solve now the allele substitution effects taking into account that the population mean \\(\\mu = a(p - q) + 2pqd\\)\n\\[\n\\begin{align*}\n\\alpha_{A_1} &= ap + dq - (a(p - q) + 2pqd) \\\\\n         &= ap + dq - ap + aq - 2pqd \\\\\n         &= q(a - 2pd + d) \\\\\n         &= q(a + d(-2p + 1)) \\\\\n         &= q(a + d(-2p + p + q)) \\\\\n         &= q(a + d(q - p))\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n\\alpha_{A_2} &= ap + dq - (a(p - q) + 2pqd) \\\\\n         &= dp - aq - ap + aq - 2pqd \\\\\n         &= p(-a + d(-2q + 1)) \\\\\n         &= -p(a + d(2q - (p + q))) \\\\\n         &= -p(a + d(q - p))\n\\end{align*}\n\\]\nNow, if \\(\\alpha = \\alpha_{A_1} - \\alpha_{A_2}\\), then\n\\[\n\\begin{align*}\n\\alpha &= q(a + d(q - p)) - (-p(a - d(q + p)))\\\\\n       &= a + d(q - p)\n\\end{align*}\n\\]\nFinally, we can express the allele substitution effects of each allele in terms of \\(\\alpha\\)\n\\[\n\\alpha_{A_1} = q(a + d(q - p)) = q\\alpha\n\\]\n\\[\n\\alpha_{A_2} = -p(a + d(q - p)) = -p\\alpha\n\\]\n\nDominance Deviations\n\nConsider first that \\(v = G - (\\mu + u)\\). Then, following the information shown below\n\n\n\nGenotypic Values For a Loxus\n\n\nGenotypes\nFrequency\nValue\nu\n\n\n\n\n$A_1A_1$\nq^2\n- a\n$-2p\\alpha$\n\n\n$A_1A_2$\n2pq\nd\n$(q - p)\\alpha$\n\n\n$A_2A_2$\np^2\na\n$2q\\alpha$\n\n\n\n\n\n\n\nWe can solve that\n\\[\n\\begin{align*}\nv_{A_1, A_1}\n&= -a - (a(p - q) + 2pqd) - (-2p\\alpha) \\\\\n    &= -a -ap + aq - 2pqd + 2p(a + d(q - p )) \\\\\n    &= -a -ap + aq - 2pqd + 2pa + 2pdq - 2p^2d \\\\\n    &= -a + aq + pa - 2p^2d \\\\\n    &= -a + a(p + q) - 2p^2d \\\\\n    &= - 2p^2d \\\\\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\nv_{A_1, A_2}\n&= d - \\big(a(p-q) + 2pq d\\big) - (q-p)\\big(a + d(q-p)\\big) \\\\\n&= d - a(p-q) - 2pq d - (q-p)a - d(q-p)^2 \\\\\n&= d - 2pq d - d(q-p)^2 \\\\\n&= d(1 - 2pq - (q-p)^2) \\\\\n&= d\\big(1 - p^2 - q^2\\big) \\\\\n&= d(2pq) \\\\\n&= 2pq\\,d\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\nv_{A_2, A_2} &= a - \\big(a(p-q) + 2pq d\\big) -2q\\alpha \\\\\n&= a -ap + aq -2pqd -2q(a+ d(q-p))\\\\\n&= a -ap + aq -2pqd -2aq -2q^2d + 2pqd \\\\\n&= a - ap -aq -2q^2d \\\\\n&= -2q^2d\n\\end{align*}\n\\]\n\nExpectations\n\nLet’s prove why the expectations of the breeding values and the dominance deviations equals 0.\n\\[\n\\begin{align*}\nE(u) &= p^2(2q\\alpha) + 2pq(q-p)\\alpha + q^2(-2p\\alpha) \\\\\n  &= 2p^2q\\alpha + 2pq^2\\alpha - 2p^2q\\alpha -2pq^2\\alpha \\\\\n  &= 0\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\nE(v) &= p^2(-2q^2d) + 2pq2pqd + q^2(-2p^2d) \\\\\n  &= -2p^2q^2d + 4p^2q^2d -2p^2q^2d\\\\\n  &= 0\n\\end{align*}\n\\]\n\nVariances\n\nFollowing the properties of the variance, we can define the additive and dominance variance as follows\n\\[\n\\begin{align*}\n\\operatorname{var}(A) &= \\operatorname{var}(u) = E[(u−E(u))^2]=E(u^2)− \\cancel{[E(u)]^2} \\\\\n  &= p^2(2q\\alpha)^2 + 2pq((q-p)\\alpha)^2 + q^2(-2p\\alpha)^2\\\\\n  &= 4p^2q^2\\alpha^2 + 2pq(q-p)^2\\alpha^2 +4 p^2q^2\\alpha^2 \\\\\n  &= 2pq\\alpha^2(2pq + (q-p)^2 + 2pq) \\\\\n  &= 2pq\\alpha^2(2pq + p^2 + q^2 -2pq + 2pq) \\\\\n  &= 2pq\\alpha^2(p^2 + q^2 + 2pq) \\\\\n  &= 2pq\\alpha^2 = 2pq[a + d(1-p)]^2\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n\\operatorname{var}(D) &= \\operatorname{var}(v) = E[(v−E(v))^2]=E(v^2)−\\cancel{[E(v)]^2} \\\\\n  &= p^2(-2q^2d)^2 + 2pq(2pqd)^2 + q^2(-2p^2d) ^2 \\\\\n  &= 4p^2q^4d^2 + 8p^3q^3d^2 + 4p^4q^2d^2 \\\\\n  &= 4p^2q^2d^2(q^2 + 2pq + p^2) \\\\\n  &= 4p^2q^2d^2 = (2pqd)^2\n\\end{align*}\n\\]",
    "crumbs": [
      "Parametrizations",
      "Breeding vs Genotypic"
    ]
  },
  {
    "objectID": "pages/08_Parametrizations.html#genotypic-parametrization",
    "href": "pages/08_Parametrizations.html#genotypic-parametrization",
    "title": "Parametrizations",
    "section": "2) Genotypic Parametrization",
    "text": "2) Genotypic Parametrization\n\nTheoryProofs\n\n\n\nPure additive values\nA different approach is to define the genotypic value as\n\\[\nG = E(G) + u^* + v^*\n\\]\nWhere \\(u^*\\) and \\(v^*\\) are no longer the breeding values and dominance deviations but pure additive and pure additive values. Given that we aim to partition the effects in pure additive and pure dominance effects, we will no longer have dominance with \\(u^*\\). An easy way to summarize this is to remove \\(d\\) from \\(\\alpha\\), so that \\(\\alpha = a\\). then, we can substitute to obtain\n\\[\nu^* =\n\\begin{cases}\n    -2pa\\\\\n    (q-p)a\\\\\n    2qa\n\\end{cases}\n\\]\nSee how still, the covariate equals the \\(Z\\) matrix as in the Breeding parametrization. Then, we can express \\(u^* = \\mathbf{Z}a\\).\n\n\nPure dominance values\nThe pure dominance values are derived by substracting the population mean, and the pure additive values to the genotypic values. Therefore\n\\[\nv^* =\n\\begin{cases}\n    -2pqd\\\\\n    d(1 -2pq)\\\\\n    -2pqd\n\\end{cases}\n\\]\nSo that \\(\\mathbf{v^*} = \\mathbf{H}d\\) where\n\\[\n\\mathbf{H} =\n\\begin{cases}\n    -2pqd,  & \\text{for } A_1A_1\\\\\\\\\n    (1 -2pq)d,  & \\text{for } A_1A_2\\\\\\\\\n    -2pqd,  & \\text{for } A_2A_2\\\\\n\\end{cases}\n\\]\nIt is important to notice that \\(H \\neq W\\). However, it can be proved that the expected values of \\(u^*\\) and \\(v^*\\) is still \\(0\\). Now, the genetic variance equals\n\\[\n\\operatorname{var}(G) = 2pqa^2 + 2pq[ 1 - 2pq ]d^2\n\\]\nAnd the covariances between additive values are computed as\n\\[\n\\begin{align*}\n\\operatorname{cov}(\\mathbf{u^*}) &= \\mathbf{ZZ'}\\sigma^2_{a} \\\\\n  &=  \\frac{\\mathbf{ZZ'}}{2pq}\\sigma^2_{A^*} \\\\\n  &= \\mathbf{G}\\sigma^2_{A^*}\n\\end{align*}\n\\]\nNotice that the Additive GRM follows the same definition for both parametrizations. Let’s show how this is not the case for the Dominance GRM\n\\[\n\\begin{align*}\n\\operatorname{cov}(\\mathbf{v^*}) &= \\mathbf{HH'}\\sigma^2_{d} \\\\\n  &=  \\frac{\\mathbf{HH'}}{2pq(1 - 2pq)}\\sigma^2_{D^*} \\\\\n  &= \\mathbf{D^*}\\sigma^2_{D^*}\n\\end{align*}\n\\]\n\n\n\n\nPure Dominance Values\n\nUnder the genotypic parametrization, \\(v^*\\) are computed as \\(v\\) in the breeding parametrization, although now, by definition \\(v^* = G - (\\mu + u^*)\\). Despite the population mean remains the same, now \\(u^* \\neq u\\), so\n\\[\n  \\begin{align*}\nv^*_{A_1A_1}\n&= -a - \\big(a(p-q) + 2pq d\\big) + 2pa \\\\\n&= -a -ap + aq -2pqd + 2pa \\\\\n&= -a + a(p + q) -2pqd \\\\\n&= -2pqd\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\nv^*_{A_1A_2}\n&= d - \\big(a(p-q) + 2pq d\\big) -(q-p)a) \\\\\n&= d -ap + aq -2pqd -aq + ap \\\\\n&= d(1 -2pq)\n\\end{align*}\n\\]\n\\[\n  \\begin{align*}\nv^*_{A_2A_2}\n&= a - \\big(a(p-q) + 2pq d\\big) -2qa \\\\\n&= a - ap + aq -2pqd -2qa \\\\\n&= a -ap -2pqd -qa \\\\\n&= a -a(p + 1) -2pqd \\\\\n&= -2pqd\n\\end{align*}\n\\]\n\nExpectations\n\nThe expectation of \\(u^*\\) and \\(v^*\\) still equal 0\n\\[\n\\begin{align*}\nE(u^*) &= p^2(2qa) + 2pq(q-p)a + q(-2pa) \\\\\n&= 2p^2qa + 2pq^2a - 2p^2qa -2pq^2a\\\\\n&= 0\\\\\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\nE(v^*) &= p^2(-2pqd) + 2pq((1-2pq)d) + q(-2pqd) \\\\\n&= -2p^2qd + 2pqd - 4p^2q^2d -2pq^2d\\\\\n&= 2pqd(-p^2 + 1 - 2pq - q^2)\\\\\n&= 2pqd(-1 + 1) = 0\n\\end{align*}\n\\]\n\nVariances\n\nBy definition, the variances under the genotypic parametrization can be defined as\n\\[\n  \\begin{align*}\nVar(A^*) = \\sigma^2_{A^*} &= E(u^{*2}) - \\cancel{E(u^*)^2} \\\\\n&= p^2(2qa)^2 + 2pq((q-p)a)^2 + q^2(-2pa)^2 \\\\\n&= 4p^2q^2a^2 + 2pq(q-p)^2a^2 + 4p^2q^2a^2 \\\\\n&= 2pqa^2(2pq + (q-p)^2 + 2pq) \\\\\n&= 2pqa^2(q^2 + p^2 +2pq)\\\\\n&= 2pqa^2\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\nVar(D^*) = \\sigma^2_{D^*} &= E(v^{*2}) - \\cancel{E(v^*)^2} \\\\\n&= p^2\\big(-2pq\\,d)^2 \\;+\\; 2pq((1-2pq)\\,d)^2 \\;+\\; q^2(-2pq\\,d)^2 \\\\\n&= d^2[ p^2( -2pq )^2 \\;+\\; 2pq(1-2pq)^2 \\;+\\; q^2( -2pq )^2\\\\\n        &= d^2[ 4p^4q^2 \\;+\\; 2pq(1 - 4pq + 4p^2q^2) \\;+\\; 4p^2q^4] \\\\\n        &= d^2[ 4p^2q^2(p^2+q^2) \\;+\\; 2pq(1 - 4pq + 4p^2q^2)] \\\\\n        &= 2pq[ 2pq(p^2+q^2) \\;+\\; (1 - 4pq + 4p^2q^2)]d^2\\\\\n        &= 2pq[ 2pq(1-2pq) \\;+\\; 1 - 4pq + 4p^2q^2 ] d^2\\\\\n        &= 2pq[ (2pq - 4p^2q^2) \\;+\\; (1 - 4pq + 4p^2q^2)d^2]d^2 \\\\\n        &= 2pq[ 1 - 2pq ]d^2 \\\\\n        \\end{align*}\n\\]",
    "crumbs": [
      "Parametrizations",
      "Breeding vs Genotypic"
    ]
  },
  {
    "objectID": "pages/08_Parametrizations.html#summary",
    "href": "pages/08_Parametrizations.html#summary",
    "title": "Parametrizations",
    "section": "3) Summary",
    "text": "3) Summary\n\nTheoryProofs\n\n\nBriefly, \\(\\operatorname{cov}(\\mathbf{u^*}) = \\operatorname{cov}(\\mathbf{u})\\), but \\(\\operatorname{cov}(\\mathbf{v^*}) \\neq \\operatorname{cov}(\\mathbf{v})\\), given that \\(\\mathbf{D^*} \\neq \\mathbf{D}\\). Besides, given that \\(\\sigma^2_A + \\sigma^2_D = \\sigma^2_{A^*} + \\sigma^2_{D^*}\\). THus, both approaches explains the data but their interpretation must be different.\nFinally, we wanted to highlight that from the practical point of view, t he conversion from the genotypic to the breeding values is easy, given that we can use \\(a\\), \\(d\\) estimated from the model and the allelic frequencies \\(p\\) and \\(q\\) from the original marker matrix to build \\(\\alpha = a + d(q - p)\\). Then we should simply multiply \\(Z\\alpha\\) to obtain \\(u\\)\n\n\n\nVariance correspondence\n\nTakin into account that\n\\[\n\\begin{align*}\n\\sigma^2_A &= 2pq \\, \\sigma_a^2 + 2pq (q - p)^2 \\, \\sigma_d^2 \\\\\n\\sigma^2_{A^*} &= 2pq \\, \\sigma_a^2 \\\\\n\\sigma^2_D &= (2pq)^2 \\, \\sigma_d^2 \\\\\n\\sigma^2_{D^*} &= 2pq (1 - 2pq) \\, \\sigma_d^2\n\\end{align*}\n\\]\nWe can then demonstrate that\n\\[\n\\begin{align*}\n\\sigma^2_A + \\sigma^2_D &= 2pq\\sigma^2_a + 2pq(q-p)^2\\sigma^2_d + (2pq)^2\\sigma^2_d \\\\\n&= 2pq\\sigma^2_a + \\sigma^2_d(2pq(q-p)^2 + (2pq)^2)\\\\\n&= 2pq\\sigma^2_a + \\sigma^2_d(2pq(q^2 + q^2 -2pq) + 4p^2q^2) \\\\\n&= 2pq\\sigma^2_a + \\sigma^2_d(2pq^3 + 2p^3q -4p^2q^2 + 4p^2q^2) \\\\\n&= 2pq\\sigma^2_a + \\sigma^2_d(2pq(q^2 + p^3))\\\\\n&= 2pq\\sigma^2_a + \\sigma^2_d(2pq(1-2pq)) \\\\\n&= 2pq(\\sigma^2_a + \\sigma^2_d(1-2pq)\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n\\sigma^2_{A^*} + \\sigma^2_{D^*} &= 2pq \\, \\sigma_a^2 + 2pq (1 - 2pq) \\, \\sigma_d^2 \\\\\n&= 2pq(\\sigma^2_a + \\sigma^2_d(1-2pq)\n\\end{align*}\n\\]\nAnd therefore\n\\[\n\\sigma^2_A + \\sigma^2_D = \\sigma^2_{A^*} + \\sigma^2_{D^*} = 2pq(\\sigma^2_a + \\sigma^2_d(1-2pq)\n\\]\nWhich means that both parametrizations are equivalents",
    "crumbs": [
      "Parametrizations",
      "Breeding vs Genotypic"
    ]
  },
  {
    "objectID": "pages/03.1_predict_LMM.html",
    "href": "pages/03.1_predict_LMM.html",
    "title": "Predict LMM",
    "section": "",
    "text": "This document illustrates prediction in a linear mixed model (LMM) for a simple RCBD example:\n\n4 genotypes (random)\n3 blocks (fixed)\n12 observations\n\nWe:\n\nRead the data and define design matrices\nEstimate variance components from ANOVA (method-of-moments)\nBuild Mixed Model Equations (MME)\nSolve for fixed effects and BLUPs\nCompute predicted values and standard errors for genotype means\nCompute reliability\n\n\ndata &lt;- read.csv(\"../data/example_1.csv\") |&gt;\n  mutate(gen = as.factor(gen), block = as.factor(block))\nhead(data)\n\n  block gen yield\n1     1  g1   7.4\n2     2  g1   6.5\n3     3  g1   5.6\n4     1  g2   9.8\n5     2  g2   6.8\n6     3  g2   6.2\n\nstr(data)\n\n'data.frame':   12 obs. of  3 variables:\n $ block: Factor w/ 3 levels \"1\",\"2\",\"3\": 1 2 3 1 2 3 1 2 3 1 ...\n $ gen  : Factor w/ 4 levels \"g1\",\"g2\",\"g3\",..: 1 1 1 2 2 2 3 3 3 4 ...\n $ yield: num  7.4 6.5 5.6 9.8 6.8 6.2 7.3 6.1 6.4 9.5 ...\n\nn &lt;- 12\nn_blks &lt;- 3\nn_gens &lt;- 4\n\n\n\n\nmod &lt;- lm(formula = yield ~ 1 + block + gen, data = data)\naov_table &lt;- as.data.frame(anova(mod))\naov_table\n\n          Df Sum Sq Mean Sq F value      Pr(&gt;F)\nblock      2   9.78    4.89  12.225 0.007650536\ngen        3   6.63    2.21   5.525 0.036730328\nResiduals  6   2.40    0.40      NA          NA\n\nmse_g &lt;- aov_table[\"gen\", \"Mean Sq\"]\nvar_e &lt;- aov_table[\"Residuals\", \"Mean Sq\"]\nvar_g &lt;- (mse_g - var_e) / n_blks\nvar_g\n\n[1] 0.6033333\n\n\n\n\n\n\nX &lt;- model.matrix(yield ~ 1 + block, data)\nZ &lt;- model.matrix(yield ~ -1 + gen, data)\ny &lt;- matrix(data[, \"yield\"]) |&gt; na.omit()\nprint(X)\n\n   (Intercept) block2 block3\n1            1      0      0\n2            1      1      0\n3            1      0      1\n4            1      0      0\n5            1      1      0\n6            1      0      1\n7            1      0      0\n8            1      1      0\n9            1      0      1\n10           1      0      0\n11           1      1      0\n12           1      0      1\nattr(,\"assign\")\n[1] 0 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$block\n[1] \"contr.treatment\"\n\nprint(y)\n\n      [,1]\n [1,]  7.4\n [2,]  6.5\n [3,]  5.6\n [4,]  9.8\n [5,]  6.8\n [6,]  6.2\n [7,]  7.3\n [8,]  6.1\n [9,]  6.4\n[10,]  9.5\n[11,]  8.0\n[12,]  7.4\n\nG &lt;- diag(x = var_g, nrow = n_gens)\nR &lt;- diag(x = var_e, nrow = n)\nV &lt;- Z %*% G %*% t(Z) + R\n\n# Mixed Model Equations\nC11 &lt;- t(X) %*% chol2inv(chol(R)) %*% X\nC12 &lt;- t(X) %*% chol2inv(chol(R)) %*% Z\nC21 &lt;- t(Z) %*% chol2inv(chol(R)) %*% X\nC22 &lt;- t(Z) %*% chol2inv(chol(R)) %*% Z + solve(G)\n\n# Coefficient matrix (LHS)\nC &lt;- as.matrix(\n  rbind(\n    cbind(C11, C12),\n    cbind(C21, C22)\n  )\n)\n\n# RHS\nrhs &lt;- rbind(\n  t(X) %*% solve(R) %*% y,\n  t(Z) %*% solve(R) %*% y\n)\n\n# Solution\nC_inv &lt;- chol2inv(chol(C))\nrownames(C_inv) &lt;- colnames(C_inv) &lt;- rownames(C)\nprint(round(C_inv, 4))\n\n            (Intercept) block2 block3   geng1   geng2   geng3   geng4\n(Intercept)      0.2508   -0.1   -0.1 -0.1508 -0.1508 -0.1508 -0.1508\nblock2          -0.1000    0.2    0.1  0.0000  0.0000  0.0000  0.0000\nblock3          -0.1000    0.1    0.2  0.0000  0.0000  0.0000  0.0000\ngeng1           -0.1508    0.0    0.0  0.2327  0.1235  0.1235  0.1235\ngeng2           -0.1508    0.0    0.0  0.1235  0.2327  0.1235  0.1235\ngeng3           -0.1508    0.0    0.0  0.1235  0.1235  0.2327  0.1235\ngeng4           -0.1508    0.0    0.0  0.1235  0.1235  0.1235  0.2327\n\nans &lt;- C_inv %*% rhs\nans\n\n                  [,1]\n(Intercept)  8.5000000\nblock2      -1.6500000\nblock3      -2.1000000\ngeng1       -0.6142534\ngeng2        0.2866516\ngeng3       -0.5323529\ngeng4        0.8599548\n\n\n\n\n\n\nPEV &lt;- C_inv[4:7, 4:7]\nr2_u &lt;- 1 - diag(PEV) / var_g\nr2_u\n\n    geng1     geng2     geng3     geng4 \n0.6142534 0.6142534 0.6142534 0.6142534 \n\n\n\n\n\n\n# L\nL &lt;- cbind(\n  matrix(1, nrow = n_gens, ncol = 1),                   # Intercept\n  matrix(1 / n_blks, nrow = n_gens, ncol = n_blks - 1), # Average block\n  diag(n_gens)                                          # Identity for gens\n)\nL\n\n     [,1]      [,2]      [,3] [,4] [,5] [,6] [,7]\n[1,]    1 0.3333333 0.3333333    1    0    0    0\n[2,]    1 0.3333333 0.3333333    0    1    0    0\n[3,]    1 0.3333333 0.3333333    0    0    1    0\n[4,]    1 0.3333333 0.3333333    0    0    0    1\n\n# Prediction\npv &lt;- L %*% ans\npv\n\n         [,1]\n[1,] 6.635747\n[2,] 7.536652\n[3,] 6.717647\n[4,] 8.109955\n\n# Standard Error\nsse2 &lt;- L %*% C_inv %*% t(L)\nsse2\n\n            [,1]        [,2]        [,3]        [,4]\n[1,] 0.115233786 0.006033183 0.006033183 0.006033183\n[2,] 0.006033183 0.115233786 0.006033183 0.006033183\n[3,] 0.006033183 0.006033183 0.115233786 0.006033183\n[4,] 0.006033183 0.006033183 0.006033183 0.115233786\n\nstd &lt;- sqrt(diag(sse2))\nstd\n\n[1] 0.339461 0.339461 0.339461 0.339461\n\ndata.frame(\"predicted.values\" = pv, std)\n\n  predicted.values      std\n1         6.635747 0.339461\n2         7.536652 0.339461\n3         6.717647 0.339461\n4         8.109955 0.339461\n\n\n\n\n\n\n# w = Lb + Mu\nvar_uhat &lt;- G - PEV\nvar_beta &lt;- C_inv[1:3, 1:3]\ncov_ubeta &lt;- -C_inv[4:7, 1:3]\n\nL &lt;- cbind(\n  matrix(1, nrow = n_gens, ncol = 1),                   # Intercept\n  matrix(1 / n_blks, nrow = n_gens, ncol = n_blks - 1)  # Average block\n)\nM &lt;- diag(n_gens)\nvar_what &lt;- L %*% var_beta %*% t(L) + M %*% var_uhat %*% t(M)\nvar_w &lt;- M %*% G %*% t(M)\ncov_wwhat &lt;- M %*% cov_ubeta %*% t(L) + M %*% var_uhat %*% t(M)\nr2_w &lt;- diag(cov_wwhat)^2 / (diag(var_w) * diag(var_what))\nr2_w\n\n[1] 0.8123244 0.8123244 0.8123244 0.8123244\n\ndata.frame(\"predicted.values\" = pv, std, r2_w, r2_u) |&gt;\n  mutate_if(is.numeric, round, 3) |&gt; \n  gt()\n\n\n\n\n\n\n\npredicted.values\nstd\nr2_w\nr2_u\n\n\n\n\n6.636\n0.339\n0.812\n0.614\n\n\n7.537\n0.339\n0.812\n0.614\n\n\n6.718\n0.339\n0.812\n0.614\n\n\n8.110\n0.339\n0.812\n0.614",
    "crumbs": [
      "Starting LMM",
      "Predictions in LMM"
    ]
  },
  {
    "objectID": "pages/03.1_predict_LMM.html#overview",
    "href": "pages/03.1_predict_LMM.html#overview",
    "title": "Predict LMM",
    "section": "",
    "text": "This document illustrates prediction in a linear mixed model (LMM) for a simple RCBD example:\n\n4 genotypes (random)\n3 blocks (fixed)\n12 observations\n\nWe:\n\nRead the data and define design matrices\nEstimate variance components from ANOVA (method-of-moments)\nBuild Mixed Model Equations (MME)\nSolve for fixed effects and BLUPs\nCompute predicted values and standard errors for genotype means\nCompute reliability\n\n\ndata &lt;- read.csv(\"../data/example_1.csv\") |&gt;\n  mutate(gen = as.factor(gen), block = as.factor(block))\nhead(data)\n\n  block gen yield\n1     1  g1   7.4\n2     2  g1   6.5\n3     3  g1   5.6\n4     1  g2   9.8\n5     2  g2   6.8\n6     3  g2   6.2\n\nstr(data)\n\n'data.frame':   12 obs. of  3 variables:\n $ block: Factor w/ 3 levels \"1\",\"2\",\"3\": 1 2 3 1 2 3 1 2 3 1 ...\n $ gen  : Factor w/ 4 levels \"g1\",\"g2\",\"g3\",..: 1 1 1 2 2 2 3 3 3 4 ...\n $ yield: num  7.4 6.5 5.6 9.8 6.8 6.2 7.3 6.1 6.4 9.5 ...\n\nn &lt;- 12\nn_blks &lt;- 3\nn_gens &lt;- 4\n\n\n\n\nmod &lt;- lm(formula = yield ~ 1 + block + gen, data = data)\naov_table &lt;- as.data.frame(anova(mod))\naov_table\n\n          Df Sum Sq Mean Sq F value      Pr(&gt;F)\nblock      2   9.78    4.89  12.225 0.007650536\ngen        3   6.63    2.21   5.525 0.036730328\nResiduals  6   2.40    0.40      NA          NA\n\nmse_g &lt;- aov_table[\"gen\", \"Mean Sq\"]\nvar_e &lt;- aov_table[\"Residuals\", \"Mean Sq\"]\nvar_g &lt;- (mse_g - var_e) / n_blks\nvar_g\n\n[1] 0.6033333\n\n\n\n\n\n\nX &lt;- model.matrix(yield ~ 1 + block, data)\nZ &lt;- model.matrix(yield ~ -1 + gen, data)\ny &lt;- matrix(data[, \"yield\"]) |&gt; na.omit()\nprint(X)\n\n   (Intercept) block2 block3\n1            1      0      0\n2            1      1      0\n3            1      0      1\n4            1      0      0\n5            1      1      0\n6            1      0      1\n7            1      0      0\n8            1      1      0\n9            1      0      1\n10           1      0      0\n11           1      1      0\n12           1      0      1\nattr(,\"assign\")\n[1] 0 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$block\n[1] \"contr.treatment\"\n\nprint(y)\n\n      [,1]\n [1,]  7.4\n [2,]  6.5\n [3,]  5.6\n [4,]  9.8\n [5,]  6.8\n [6,]  6.2\n [7,]  7.3\n [8,]  6.1\n [9,]  6.4\n[10,]  9.5\n[11,]  8.0\n[12,]  7.4\n\nG &lt;- diag(x = var_g, nrow = n_gens)\nR &lt;- diag(x = var_e, nrow = n)\nV &lt;- Z %*% G %*% t(Z) + R\n\n# Mixed Model Equations\nC11 &lt;- t(X) %*% chol2inv(chol(R)) %*% X\nC12 &lt;- t(X) %*% chol2inv(chol(R)) %*% Z\nC21 &lt;- t(Z) %*% chol2inv(chol(R)) %*% X\nC22 &lt;- t(Z) %*% chol2inv(chol(R)) %*% Z + solve(G)\n\n# Coefficient matrix (LHS)\nC &lt;- as.matrix(\n  rbind(\n    cbind(C11, C12),\n    cbind(C21, C22)\n  )\n)\n\n# RHS\nrhs &lt;- rbind(\n  t(X) %*% solve(R) %*% y,\n  t(Z) %*% solve(R) %*% y\n)\n\n# Solution\nC_inv &lt;- chol2inv(chol(C))\nrownames(C_inv) &lt;- colnames(C_inv) &lt;- rownames(C)\nprint(round(C_inv, 4))\n\n            (Intercept) block2 block3   geng1   geng2   geng3   geng4\n(Intercept)      0.2508   -0.1   -0.1 -0.1508 -0.1508 -0.1508 -0.1508\nblock2          -0.1000    0.2    0.1  0.0000  0.0000  0.0000  0.0000\nblock3          -0.1000    0.1    0.2  0.0000  0.0000  0.0000  0.0000\ngeng1           -0.1508    0.0    0.0  0.2327  0.1235  0.1235  0.1235\ngeng2           -0.1508    0.0    0.0  0.1235  0.2327  0.1235  0.1235\ngeng3           -0.1508    0.0    0.0  0.1235  0.1235  0.2327  0.1235\ngeng4           -0.1508    0.0    0.0  0.1235  0.1235  0.1235  0.2327\n\nans &lt;- C_inv %*% rhs\nans\n\n                  [,1]\n(Intercept)  8.5000000\nblock2      -1.6500000\nblock3      -2.1000000\ngeng1       -0.6142534\ngeng2        0.2866516\ngeng3       -0.5323529\ngeng4        0.8599548\n\n\n\n\n\n\nPEV &lt;- C_inv[4:7, 4:7]\nr2_u &lt;- 1 - diag(PEV) / var_g\nr2_u\n\n    geng1     geng2     geng3     geng4 \n0.6142534 0.6142534 0.6142534 0.6142534 \n\n\n\n\n\n\n# L\nL &lt;- cbind(\n  matrix(1, nrow = n_gens, ncol = 1),                   # Intercept\n  matrix(1 / n_blks, nrow = n_gens, ncol = n_blks - 1), # Average block\n  diag(n_gens)                                          # Identity for gens\n)\nL\n\n     [,1]      [,2]      [,3] [,4] [,5] [,6] [,7]\n[1,]    1 0.3333333 0.3333333    1    0    0    0\n[2,]    1 0.3333333 0.3333333    0    1    0    0\n[3,]    1 0.3333333 0.3333333    0    0    1    0\n[4,]    1 0.3333333 0.3333333    0    0    0    1\n\n# Prediction\npv &lt;- L %*% ans\npv\n\n         [,1]\n[1,] 6.635747\n[2,] 7.536652\n[3,] 6.717647\n[4,] 8.109955\n\n# Standard Error\nsse2 &lt;- L %*% C_inv %*% t(L)\nsse2\n\n            [,1]        [,2]        [,3]        [,4]\n[1,] 0.115233786 0.006033183 0.006033183 0.006033183\n[2,] 0.006033183 0.115233786 0.006033183 0.006033183\n[3,] 0.006033183 0.006033183 0.115233786 0.006033183\n[4,] 0.006033183 0.006033183 0.006033183 0.115233786\n\nstd &lt;- sqrt(diag(sse2))\nstd\n\n[1] 0.339461 0.339461 0.339461 0.339461\n\ndata.frame(\"predicted.values\" = pv, std)\n\n  predicted.values      std\n1         6.635747 0.339461\n2         7.536652 0.339461\n3         6.717647 0.339461\n4         8.109955 0.339461\n\n\n\n\n\n\n# w = Lb + Mu\nvar_uhat &lt;- G - PEV\nvar_beta &lt;- C_inv[1:3, 1:3]\ncov_ubeta &lt;- -C_inv[4:7, 1:3]\n\nL &lt;- cbind(\n  matrix(1, nrow = n_gens, ncol = 1),                   # Intercept\n  matrix(1 / n_blks, nrow = n_gens, ncol = n_blks - 1)  # Average block\n)\nM &lt;- diag(n_gens)\nvar_what &lt;- L %*% var_beta %*% t(L) + M %*% var_uhat %*% t(M)\nvar_w &lt;- M %*% G %*% t(M)\ncov_wwhat &lt;- M %*% cov_ubeta %*% t(L) + M %*% var_uhat %*% t(M)\nr2_w &lt;- diag(cov_wwhat)^2 / (diag(var_w) * diag(var_what))\nr2_w\n\n[1] 0.8123244 0.8123244 0.8123244 0.8123244\n\ndata.frame(\"predicted.values\" = pv, std, r2_w, r2_u) |&gt;\n  mutate_if(is.numeric, round, 3) |&gt; \n  gt()\n\n\n\n\n\n\n\npredicted.values\nstd\nr2_w\nr2_u\n\n\n\n\n6.636\n0.339\n0.812\n0.614\n\n\n7.537\n0.339\n0.812\n0.614\n\n\n6.718\n0.339\n0.812\n0.614\n\n\n8.110\n0.339\n0.812\n0.614",
    "crumbs": [
      "Starting LMM",
      "Predictions in LMM"
    ]
  },
  {
    "objectID": "pages/07_PBLUP.html",
    "href": "pages/07_PBLUP.html",
    "title": "Pedigree BLUP",
    "section": "",
    "text": "# RCBD\n# 4 gens\n# 3 blocks\n# 12 observations\ndata &lt;- read.csv(\"../data/example_1.csv\") |&gt;\n  mutate(gen = as.factor(gen), block = as.factor(block))\nhead(data)\n\n  block gen yield\n1     1  g1   7.4\n2     2  g1   6.5\n3     3  g1   5.6\n4     1  g2   9.8\n5     2  g2   6.8\n6     3  g2   6.2\n\nn &lt;- 12\nn_blks &lt;- 3\nn_gens &lt;- 4\ng_lvls &lt;- levels(data$gen)\n\n# Pedigree\nped &lt;- data.frame(\n  id = g_lvls,\n  p1 = c(NA, NA, \"g2\", \"g3\"),\n  p2 = c(NA, NA, \"g1\", \"g1\")\n)\n\n\n\n\n\n\n\n\n\n\n\nAdditive relationship matrix\n\nA &lt;- A_mat(ped, ploidy = 2)\nA\n\n     g1   g2   g3   g4\ng1 1.00 0.00 0.50 0.75\ng2 0.00 1.00 0.50 0.25\ng3 0.50 0.50 1.00 0.75\ng4 0.75 0.25 0.75 1.25\n\n\n\n\nVariance components\n\n\nOnline License checked out Tue Feb 17 21:45:48 2026\n\n\n\nasreml.options(Cfixed = TRUE, maxit = 50, trace = 0)\nmme &lt;- asreml(fixed = yield ~ 1, random = ~ vm(gen, A) + block, data = data)\nmme$coefficients$random\n\n                  effect\nvm(gen, A)_g1 -0.4594011\nvm(gen, A)_g2  0.4594011\nvm(gen, A)_g3 -0.3201192\nvm(gen, A)_g4  1.0022245\nblock_1        1.1439320\nblock_2       -0.3660582\nblock_3       -0.7778737\nattr(,\"terms\")\n                tname n\nvm(gen, A) vm(gen, A) 4\nblock           block 3\n\nvar_comps &lt;- summary(mme)$varcomp\nvar_g &lt;- var_comps[2, 1]\nvar_b &lt;- var_comps[1, 1]\nvar_e &lt;- var_comps[3, 1]\n\n\n\nReconstructing MME\n\nX &lt;- model.matrix(yield ~ 1, data)\nZb &lt;- model.matrix(yield ~ -1 + block, data)\nZg &lt;- model.matrix(yield ~ -1 + gen, data)\nZ &lt;- cbind(Zb, Zg)\ny &lt;- matrix(data[, \"yield\"])\n\nGb &lt;- diag(x = var_b, nrow = n_blks)\nGg &lt;- A * var_g\nG &lt;- bdiag(Gb, Gg)\nR &lt;- diag(x = var_e, nrow = n)\nV &lt;- Z %*% G %*% t(Z) + R\n\n# Mixed Model Equations\nC11 &lt;- t(X) %*% chol2inv(chol(R)) %*% X\nC12 &lt;- t(X) %*% chol2inv(chol(R)) %*% Z\nC21 &lt;- t(Z) %*% chol2inv(chol(R)) %*% X\nC22 &lt;- t(Z) %*% chol2inv(chol(R)) %*% Z + solve(G)\n\n# Coefficient matrix (LHS)\nC &lt;- as.matrix(\n  rbind(\n    cbind(C11, C12),\n    cbind(C21, C22)\n  )\n)\n\n# RHS\nrhs &lt;- rbind(\n  t(X) %*% solve(R) %*% y,\n  t(Z) %*% solve(R) %*% y\n)\n\n# Solution\nC_inv &lt;- ginv(C)\nrownames(C_inv) &lt;- colnames(C_inv) &lt;- rownames(C)\nprint(round(C_inv, 3))\n\n            (Intercept) block1 block2 block3  geng1  geng2  geng3  geng4\n(Intercept)       1.321 -0.373 -0.373 -0.373 -0.885 -0.883 -0.941 -0.943\nblock1           -0.373  0.437  0.342  0.342  0.000  0.000  0.000  0.000\nblock2           -0.373  0.342  0.437  0.342  0.000  0.000  0.000  0.000\nblock3           -0.373  0.342  0.342  0.437  0.000  0.000  0.000  0.000\ngeng1            -0.885  0.000  0.000  0.000  0.947  0.821  0.883  0.889\ngeng2            -0.883  0.000  0.000  0.000  0.821  0.947  0.885  0.879\ngeng3            -0.941  0.000  0.000  0.000  0.883  0.885  1.051  0.945\ngeng4            -0.943  0.000  0.000  0.000  0.889  0.879  0.945  1.059\n\nans &lt;- C_inv %*% rhs\nans\n\n                  [,1]\n(Intercept)  7.0794023\nblock1       1.1440635\nblock2      -0.3661003\nblock3      -0.7779632\ngeng1       -0.4596044\ngeng2        0.4596044\ngeng3       -0.3203845\ngeng4        1.0027751\n\n# PEV = C22_g\nC22_g &lt;- C_inv[5:8, 5:8]\nC22_g\n\n          geng1     geng2     geng3     geng4\ngeng1 0.9472216 0.8208755 0.8832245 0.8892003\ngeng2 0.8208755 0.9472216 0.8848727 0.8788968\ngeng3 0.8832245 0.8848727 1.0513126 0.9445423\ngeng4 0.8892003 0.8788968 0.9445423 1.0589367\n\n\n\n\nReliability\n\nPEV_gi &lt;- diag(C22_g)\nvar_gi &lt;- diag(Gg)\nr2_g &lt;- 1 - PEV_gi / var_gi\nr2_g\n\n    geng1     geng2     geng3     geng4 \n0.4642706 0.4642706 0.4053989 0.5208695 \n\n\n\n# Linear combination w = Lb + Mu\nvar_uhat &lt;- G - C_inv[-1, -1]\nvar_beta &lt;- C_inv[1, 1]\ncov_ubeta &lt;- -matrix(C_inv[-1, 1], nrow = 7, ncol = 1)\n\nL &lt;- cbind(matrix(1, nrow = n_gens, ncol = 1)) # Intercept\nM &lt;- cbind(\n  matrix(0, nrow = 4, ncol = 3),               # No block\n  diag(n_gens)                                 # Genotype \n)\nvar_what &lt;- L %*% var_beta %*% t(L) + M %*% var_uhat %*% t(M)\nvar_w &lt;- M %*% G %*% t(M)\ncov_wwhat &lt;- M %*% cov_ubeta %*% t(L) + M %*% var_uhat %*% t(M)\nr2_w &lt;- diag(cov_wwhat)^2 / (diag(var_w) * diag(var_what))\nr2_w\n\n[1] 0.7685843 0.7666359 0.7628126 0.8026305\n\n\n\n\nPredictions\n\n# g\npp &lt;- predict(mme, classify = \"vm(gen, A)\", only = \"vm(gen, A)\", sed = TRUE, vcov = TRUE)\nas.data.frame(pp$pvals) # Solution\n\n  vm.gen..A. predicted.value std.error    status\n1         g1      -0.4596291 0.9729385 Estimable\n2         g2       0.4596291 0.9729385 Estimable\n3         g3      -0.3204166 1.0249924 Estimable\n4         g4       1.0028419 1.0287003 Estimable\n\npp$vcov # C22_g\n\n4 x 4 Matrix of class \"dspMatrix\"\n          [,1]      [,2]      [,3]      [,4]\n[1,] 0.9466094 0.8203812 0.8826721 0.8886408\n[2,] 0.8203812 0.9466094 0.8843185 0.8783498\n[3,] 0.8826721 0.8843185 1.0506093 0.9439335\n[4,] 0.8886408 0.8783498 0.9439335 1.0582244\n\npp$sed^2 # vd_BLUP_mat\n\n4 x 4 Matrix of class \"dspMatrix\"\n          [,1]      [,2]      [,3]      [,4]\n[1,]        NA 0.2524565 0.2318745 0.2275522\n[2,] 0.2524565        NA 0.2285818 0.2481342\n[3,] 0.2318745 0.2285818        NA 0.2209668\n[4,] 0.2275522 0.2481342 0.2209668        NA\n\n# mu + g\nlc &lt;- predict(mme, classify = \"gen\", sed = TRUE, vcov = TRUE)\nas.data.frame(lc$pvals)\n\n  gen predicted.value std.error    status\n1  g1        6.619765 0.7052467 Estimable\n2  g2        7.539023 0.7083045 Estimable\n3  g3        6.758977 0.6998271 Estimable\n4  g4        8.082236 0.7025421 Estimable\n\nlc$vcov\n\n4 x 4 Matrix of class \"dspMatrix\"\n          [,1]      [,2]      [,3]      [,4]\n[1,] 0.4973729 0.3733059 0.3776282 0.3816931\n[2,] 0.3733059 0.5016952 0.3814357 0.3735632\n[3,] 0.3776282 0.3814357 0.4897579 0.3811783\n[4,] 0.3816931 0.3735632 0.3811783 0.4935654\n\nlc$sed^2\n\n4 x 4 Matrix of class \"dspMatrix\"\n          [,1]      [,2]      [,3]      [,4]\n[1,]        NA 0.2524565 0.2318745 0.2275522\n[2,] 0.2524565        NA 0.2285818 0.2481342\n[3,] 0.2318745 0.2285818        NA 0.2209668\n[4,] 0.2275522 0.2481342 0.2209668        NA",
    "crumbs": [
      "Starting LMM",
      "Pedigree BLUP"
    ]
  }
]