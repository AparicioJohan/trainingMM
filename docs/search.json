[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This webpage is under development.\nOur goal is to build a collection of resources that use R code and mathematical explanations to make concepts in statistical modeling easier to understand.\nWe will begin with the basics of linear models, and progressively extend toward more advanced mixed models, which are widely applied in plant breeding, animal breeding, and many other fields.\n\n\n\nHands-on examples: R scripts that you can run and adapt.\n\nStep-by-step math: showing how model equations connect to code.\n\nConceptual explanations: to make sense of sums of squares, design matrices, likelihood, variance components, mixed models equations, heritability, and more in the context of plant breeding.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "pages/00_start_lm.html",
    "href": "pages/00_start_lm.html",
    "title": "Understanding a linear model",
    "section": "",
    "text": "With this article we want to analyze a randomized complete block design (RCBD) to show how a traditional full fixed effects model and ANOVA work.",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html#setup-data",
    "href": "pages/00_start_lm.html#setup-data",
    "title": "Understanding a linear model",
    "section": "1) Setup & data",
    "text": "1) Setup & data\nClewer and Scarisbrick (2001) present a yield trial (t/ha) conducted using a randomized complete block design. The design included 3 blocks and 4 cultivars, resulting in 12 experimental plots.\n\nlibrary(tidyverse)\nlibrary(emmeans)\nlibrary(gt)\nlibrary(kableExtra)\n\n# Read and coerce factors\ndata &lt;- read.csv(\"../data/example_1.csv\") |&gt;\n  mutate(gen = as.factor(gen), block = as.factor(block))\nhead(data)\n\n  block gen yield\n1     1  g1   7.4\n2     2  g1   6.5\n3     3  g1   5.6\n4     1  g2   9.8\n5     2  g2   6.8\n6     3  g2   6.2\n\nstr(data)\n\n'data.frame':   12 obs. of  3 variables:\n $ block: Factor w/ 3 levels \"1\",\"2\",\"3\": 1 2 3 1 2 3 1 2 3 1 ...\n $ gen  : Factor w/ 4 levels \"g1\",\"g2\",\"g3\",..: 1 1 1 2 2 2 3 3 3 4 ...\n $ yield: num  7.4 6.5 5.6 9.8 6.8 6.2 7.3 6.1 6.4 9.5 ...\n\n\n\nQuick visualization\n\ndata |&gt;\n  ggplot(aes(x = gen, y = yield, color = block)) +\n  geom_point(size = 3) +\n  theme_classic(base_size = 15)\n\n\n\n\nYield by genotype colored by block.",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html#linear-model-building-blocks",
    "href": "pages/00_start_lm.html#linear-model-building-blocks",
    "title": "Understanding a linear model",
    "section": "2) Linear model building blocks",
    "text": "2) Linear model building blocks\nWe will progressively build the RCBD model using model.frame() and model.matrix() to see the design matrices explicitly, then solve normal equations. We use:\n\n\\(\\boldsymbol{y} = \\boldsymbol{X\\beta} + \\boldsymbol{\\varepsilon}\\): linear model\n\\(\\boldsymbol{y}\\): response vector (yield)\n\\(\\boldsymbol{X}\\): model matrix\n\\(\\boldsymbol{\\varepsilon} \\sim MVN(0, \\boldsymbol{I}\\sigma^2)\\)\n\\(E(\\boldsymbol{y}) = \\boldsymbol{X\\beta}; \\; V(\\boldsymbol{y}) = \\boldsymbol{I}\\sigma^2\\)\n\\(\\boldsymbol{\\hat\\beta} = (\\boldsymbol{X^\\top X})^{-1} \\boldsymbol{X^\\top} \\boldsymbol{y}\\)\n\\(V(\\boldsymbol{\\hat{\\beta}}) = (\\boldsymbol{X^\\top X})^{-1} \\sigma^2\\)\nFitted values \\(\\boldsymbol{\\hat y} = \\boldsymbol{X\\hat\\beta}\\)\nErrors \\(\\boldsymbol{\\varepsilon} = \\boldsymbol{y} - \\boldsymbol{\\hat y}\\)\nSum of squared errors \\(\\text{SSE} = \\boldsymbol{\\varepsilon^\\top \\varepsilon}\\)\n\nLet n = 12 observations, n_blks = 3 blocks, n_gens = 4 genotypes.\n\nn &lt;- 12\nn_blks &lt;- 3\nn_gens &lt;- 4\n\n\n2.1 Intercept-only model (overall mean)\n\nMatricesCode\n\n\n\nModel:\n\n\\[\n\\small\n\\boldsymbol{y} = \\boldsymbol{X\\beta} + \\boldsymbol{\\varepsilon} =&gt;\n\\begin{bmatrix}\n7.4 \\\\\n6.5 \\\\\n5.6 \\\\\n9.8 \\\\\n6.8 \\\\\n6.2 \\\\\n7.3 \\\\\n6.1 \\\\\n6.4 \\\\\n9.5 \\\\\n8.0 \\\\\n7.4 \\\\\n\\end{bmatrix} =\n\\overset{\\text{mean}}{\n\\begin{bmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1\n\\end{bmatrix}}\n\\mu + \\boldsymbol{\\varepsilon}\n\\]\n\nBLUE(s):\n\n\\[\n\\small\n\\boldsymbol{\\beta} =\n\\underbrace{\\left(\\begin{bmatrix}\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n\\end{bmatrix}\\right)^{-1}}_{(\\boldsymbol{X'X})^{-1}}\n\\underbrace{\\begin{bmatrix}\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n1 \\\n\\end{bmatrix}\n\\begin{bmatrix}\n7.4 \\\\\n6.5 \\\\\n5.6 \\\\\n9.8 \\\\\n6.8 \\\\\n6.2 \\\\\n7.3 \\\\\n6.1 \\\\\n6.4 \\\\\n9.5 \\\\\n8.0 \\\\\n7.4 \\\\\n\\end{bmatrix}}_{\\boldsymbol{X'y}} =\n\\underbrace{\\frac{1}{12}}_{(\\boldsymbol{X'X})^{-1}}\\cdot \\underbrace{87}_{\\boldsymbol{X'y}} = 7.25\n\\]\n\n\n\n\n\n\nNote\n\n\n\nNotice that the first term, \\(\\left(\\boldsymbol{X'X}\\right)^{-1}\\), corresponds to the inverse of number of observations, while the second term, \\(\\boldsymbol{X'y}\\), gives the sum of phenotypic values.\n\n\nIn this example, there is only a single level, \\(\\mu\\). Therefore, the entire expression simplifies to the sum of the phenotypic values divided by the number of observations, which is simply the mean, as shown below:\n\ndata.frame(mean = \"mean\", beta = mean(data$yield)) |&gt;\n  gt()\n\n\n\n\n\n\n\nmean\nbeta\n\n\n\n\nmean\n7.25\n\n\n\n\n\n\n\n\n\n\nff &lt;- yield ~ 1\nm &lt;- model.frame(ff, data)\nX &lt;- model.matrix(ff, m)\ny &lt;- matrix(data$yield)\n\n# Normal equations components\nXty &lt;- t(X) %*% y\nXtX &lt;- t(X) %*% X\nrank_X &lt;- qr(XtX)$rank\nXtX_inv &lt;- solve(XtX)\n\nbeta_mu &lt;- XtX_inv %*% Xty # overall mean (mu)\ny_hat &lt;- X %*% beta_mu\nerrors &lt;- y - y_hat\nSSE_mu &lt;- t(errors) %*% errors\nSSE_mu &lt;- as.numeric(SSE_mu)\n\nlist(rank = rank_X, beta_mu = drop(beta_mu), SSE_mu = SSE_mu)\n\n$rank\n[1] 1\n\n$beta_mu\n(Intercept) \n       7.25 \n\n$SSE_mu\n[1] 18.81\n\n\n\n\n\n\n\n2.2 Add blocks\nWe will now illustrate what happens if we only include the block factor.\n\nMatricesCode\n\n\n\nModel: \\[\n\\small\n\\boldsymbol{y} = \\boldsymbol{X\\beta} + \\boldsymbol{\\varepsilon} =&gt;\n\\begin{bmatrix}\n7.4 \\\\\n6.5 \\\\\n5.6 \\\\\n9.8 \\\\\n6.8 \\\\\n6.2 \\\\\n7.3 \\\\\n6.1 \\\\\n6.4 \\\\\n9.5 \\\\\n8.0 \\\\\n7.4 \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\overset{\\text{Block 1}}{1} & \\overset{\\text{Block 2}}{0} & \\overset{\\text{Block 3}}{0} \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\nb_3 \\\\\n\\end{bmatrix} + \\boldsymbol{\\varepsilon}\n\\]\nBLUE(s): \\[\n\\small\n\\boldsymbol{\\beta} =\n\\underbrace{\\left(\\begin{bmatrix}\n1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 1 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 1 \\ 0 \\ 0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\\\\n1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\\\\n1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\\\\n1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\\\\n\\end{bmatrix}\\right)^{-1}}_{(\\boldsymbol{X'X})^{-1}}\n\\underbrace{\\begin{bmatrix}\n1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 1 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 1 \\ 0 \\ 0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n7.4 \\\\\n6.5 \\\\\n5.6 \\\\\n9.8 \\\\\n6.8 \\\\\n6.2 \\\\\n7.3 \\\\\n6.1 \\\\\n6.4 \\\\\n9.5 \\\\\n8.0 \\\\\n7.4 \\\\\n\\end{bmatrix}}_{\\boldsymbol{X'y}} =\n\\underbrace{\\begin{bmatrix}\n\\frac{1}{4} \\ 0 \\ 0 \\\\\n0 \\ \\frac{1}{4} \\ 0 \\\\\n0 \\ 0 \\ \\frac{1}{4} \\\\\n\\end{bmatrix}}_{(\\boldsymbol{X'X})^{-1}}\n\\underbrace{\\begin{bmatrix}\n34 \\\\\n27.4 \\\\\n25.6 \\\\\n\\end{bmatrix}}_{\\boldsymbol{X'y}} =\n\\begin{bmatrix}\n8.50 \\\\\n6.85 \\\\\n6.40 \\\\\n\\end{bmatrix}\n\\]\n\nIn this model, there are 3 levels (\\(Block1\\), \\(Block2\\) and \\(Block3\\)). Therefore, the entire expression simplifies to the sum of the phenotypic values divided by the number of observations, which is simply the mean per block.\n\ndata |&gt;\n  group_by(block) |&gt;\n  summarise(beta = mean(yield, na.rm = TRUE)) |&gt;\n  gt()\n\n\n\n\n\n\n\nblock\nbeta\n\n\n\n\n1\n8.50\n\n\n2\n6.85\n\n\n3\n6.40\n\n\n\n\n\n\n\n\n\n\nff &lt;- yield ~ -1 + block\nm &lt;- model.frame(ff, data)\nX &lt;- model.matrix(ff, m)\ny &lt;- matrix(data$yield)\n\nXty &lt;- t(X) %*% y\nXtX &lt;- t(X) %*% X\nrank_X &lt;- qr(XtX)$rank\nXtX_inv &lt;- solve(XtX)\n\nbeta_blk &lt;- XtX_inv %*% Xty\nSSE_blk &lt;- t(y - X %*% beta_blk) %*% (y - X %*% beta_blk)\nSSE_blk &lt;- as.numeric(SSE_blk)\n\nlist(rank = rank_X, beta_blk = drop(beta_blk), SSE_blk = SSE_blk)\n\n$rank\n[1] 3\n\n$beta_blk\nblock1 block2 block3 \n  8.50   6.85   6.40 \n\n$SSE_blk\n[1] 9.03\n\n\n\n\n\n\n\n2.3 Genotype main effects (no intercept)\n\nMatricesCode\n\n\n\nModel:\n\n\\[\n\\small\n\\boldsymbol{y} = \\boldsymbol{X\\beta} + \\boldsymbol{\\varepsilon} =&gt;\n\\begin{bmatrix}\n7.4 \\\\\n6.5 \\\\\n5.6 \\\\\n9.8 \\\\\n6.8 \\\\\n6.2 \\\\\n7.3 \\\\\n6.1 \\\\\n6.4 \\\\\n9.5 \\\\\n8.0 \\\\\n7.4 \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\overset{gen1}{1} & \\overset{gen2}{0} & \\overset{gen3}{0} & \\overset{gen4}{0} \\\\\n1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\ng_1 \\\\\ng_2 \\\\\ng_3 \\\\\ng_4 \\\\\n\\end{bmatrix} + \\boldsymbol{\\varepsilon}\n\\]\n\nBLUE(s):\n\n\\[\n\\small\n\\boldsymbol{\\beta} =\n\\underbrace{\n\\left(\\begin{bmatrix}\n1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\ 0 \\ 0 \\ 0 \\\\\n1 \\ 0 \\ 0 \\ 0 \\\\\n1 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\ 0 \\\\\n0 \\ 1 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 1 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 1 \\\\\n0 \\ 0 \\ 0 \\ 1 \\\\\n0 \\ 0 \\ 0 \\ 1 \\\\\n\\end{bmatrix}\\right)^{-1}}_{(\\boldsymbol{X'X})^{-1}}\n\\underbrace{\n\\begin{bmatrix}\n1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n7.4 \\\\\n6.5 \\\\\n5.6 \\\\\n9.8 \\\\\n6.8 \\\\\n6.2 \\\\\n7.3 \\\\\n6.1 \\\\\n6.4 \\\\\n9.5 \\\\\n8.0 \\\\\n7.4 \\\\\n\\end{bmatrix}}_{\\boldsymbol{X'y}} =\n\\underbrace{\\begin{bmatrix}\n\\frac{1}{3}\\ 0 \\ 0 \\ 0 \\\\\n0 \\ \\frac{1}{3} \\ 0 \\ 0 \\\\\n0 \\ 0 \\ \\frac{1}{3} \\ 0 \\\\\n0 \\ 0 \\ 0 \\ \\frac{1}{3} \\\\\n\\end{bmatrix}}_{(X'X)^{-1}}\n\\underbrace{\n\\begin{bmatrix}\n19.5 \\\\\n22.8 \\\\\n19.8 \\\\\n24.9 \\\\\n\\end{bmatrix}\n}_{\\boldsymbol{X'y}} =\n\\begin{bmatrix}\n6.5 \\\\\n7.6 \\\\\n6.6 \\\\\n8.3 \\\\\n\\end{bmatrix}\n\\]\nIn this model, there are 4 levels (\\(g1\\), \\(g2\\), \\(g3\\) and \\(g4\\)). Therefore, the entire expression simplifies to the sum of the phenotypic values divided by the number of observations, which is simply the mean per genotype.\n\ndata |&gt;\n  group_by(gen) |&gt;\n  summarise(beta = mean(yield, na.rm = TRUE)) |&gt;\n  gt()\n\n\n\n\n\n\n\ngen\nbeta\n\n\n\n\ng1\n6.5\n\n\ng2\n7.6\n\n\ng3\n6.6\n\n\ng4\n8.3\n\n\n\n\n\n\n\n\n\n\nff &lt;- yield ~ -1 + gen\nm &lt;- model.frame(ff, data)\nX &lt;- model.matrix(ff, m)\ny &lt;- matrix(data$yield)\n\nXty &lt;- t(X) %*% y\nXtX &lt;- t(X) %*% X\nrank_X &lt;- qr(XtX)$rank\nXtX_inv &lt;- solve(XtX)\n\nbeta_gen &lt;- XtX_inv %*% Xty\nSSE_gen &lt;- t(y - X %*% beta_gen) %*% (y - X %*% beta_gen)\nSSE_gen &lt;- as.numeric(SSE_gen)\n\nm2 &lt;- list(rank = rank_X, beta_gen = drop(beta_gen), SSE_gen = SSE_gen)\n\n\n\n\n\n\n2.4 Genotype main effects (Intercept)\n\nMatricesCode\n\n\n\nModel:\n\n\\[\n\\small\n\\boldsymbol{y} = \\boldsymbol{X\\beta} + \\boldsymbol{\\varepsilon} =&gt;\n\\begin{bmatrix}\n7.4 \\\\\n6.5 \\\\\n5.6 \\\\\n9.8 \\\\\n6.8 \\\\\n6.2 \\\\\n7.3 \\\\\n6.1 \\\\\n6.4 \\\\\n9.5 \\\\\n8.0 \\\\\n7.4 \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\overset{intercept}{1} & \\overset{gen2}{0} & \\overset{gen3}{0} & \\overset{gen4}{0} \\\\\n1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 1 \\\\\n1 & 0 & 0 & 1 \\\\\n1 & 0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mu \\\\\ng_2 \\\\\ng_3 \\\\\ng_4 \\\\\n\\end{bmatrix} + \\boldsymbol{\\varepsilon}\n\\]\n\nBLUE(s):\n\n\\[\n\\small\n\\boldsymbol{\\beta} =\n\\underbrace{\n\\left(\\begin{bmatrix}\n1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\\\\n0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\ 0 \\ 0 \\ 0 \\\\\n1 \\ 0 \\ 0 \\ 0 \\\\\n1 \\ 0 \\ 0 \\ 0 \\\\\n1 \\ 1 \\ 0 \\ 0 \\\\\n1 \\ 1 \\ 0 \\ 0 \\\\\n1 \\ 1 \\ 0 \\ 0 \\\\\n1 \\ 0 \\ 1 \\ 0 \\\\\n1 \\ 0 \\ 1 \\ 0 \\\\\n1 \\ 0 \\ 1 \\ 0 \\\\\n1 \\ 0 \\ 0 \\ 1 \\\\\n1 \\ 0 \\ 0 \\ 1 \\\\\n1 \\ 0 \\ 0 \\ 1 \\\\\n\\end{bmatrix}\\right)^{-1}}_{(\\boldsymbol{X'X})^{-1}}\n\\underbrace{\n\\begin{bmatrix}\n1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\\\\n0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \\\\\n0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n7.4 \\\\\n6.5 \\\\\n5.6 \\\\\n9.8 \\\\\n6.8 \\\\\n6.2 \\\\\n7.3 \\\\\n6.1 \\\\\n6.4 \\\\\n9.5 \\\\\n8.0 \\\\\n7.4 \\\\\n\\end{bmatrix}}_{\\boldsymbol{X'y}} =\n\\underbrace{\\begin{bmatrix}\n\\frac{1}{3}\\ 0 \\ 0 \\ 0 \\\\\n0 \\ \\frac{1}{3} \\ 0 \\ 0 \\\\\n0 \\ 0 \\ \\frac{1}{3} \\ 0 \\\\\n0 \\ 0 \\ 0 \\ \\frac{1}{3} \\\\\n\\end{bmatrix}}_{(X'X)^{-1}}\n\\underbrace{\n\\begin{bmatrix}\n19.5 \\\\\n22.8 \\\\\n19.8 \\\\\n24.9 \\\\\n\\end{bmatrix}\n}_{\\boldsymbol{X'y}} =\n\\begin{bmatrix}\n6.5 \\\\\n7.6 \\\\\n6.6 \\\\\n8.3 \\\\\n\\end{bmatrix}\n\\]\n\n\n\nff &lt;- yield ~ 1 + gen\nm &lt;- model.frame(ff, data)\nX &lt;- model.matrix(ff, m)\ny &lt;- matrix(data$yield)\n\nXty &lt;- t(X) %*% y\nXtX &lt;- t(X) %*% X\nrank_X &lt;- qr(XtX)$rank\nXtX_inv &lt;- solve(XtX)\n\nbeta_gen &lt;- XtX_inv %*% Xty\nSSE_gen &lt;- t(y - X %*% beta_gen) %*% (y - X %*% beta_gen)\nSSE_gen &lt;- as.numeric(SSE_gen)\n\nm2 &lt;- list(rank = rank_X, beta_gen = drop(beta_gen), SSE_gen = SSE_gen)\n\nIn progress …\n\n\n\n\n\n2.5 Full model: intercept + blocks + genotypes\n\nMatricesCode\n\n\n\nModel:\n\n\\[\n\\small\n\\boldsymbol{y} = \\boldsymbol{X\\beta} + \\boldsymbol{\\varepsilon} \\;\\;\\;\\Rightarrow\\;\\;\\;\n\\begin{bmatrix}\n7.4 \\\\\n6.5 \\\\\n5.6 \\\\\n9.8 \\\\\n6.8 \\\\\n6.2 \\\\\n7.3 \\\\\n6.1 \\\\\n6.4 \\\\\n9.5 \\\\\n8.0 \\\\\n7.4 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\overset{\\text{intercept}}{1} &\n\\overset{\\text{block2}}{0} &\n\\overset{\\text{block3}}{0} &\n\\overset{\\text{gen2}}{0} &\n\\overset{\\text{gen3}}{0} &\n\\overset{\\text{gen4}}{0} \\\\\n1 & 1 & 0 & 0 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 1 & 0 \\\\\n1 & 1 & 0 & 0 & 1 & 0 \\\\\n1 & 0 & 1 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 0 & 0 & 1 \\\\\n1 & 1 & 0 & 0 & 0 & 1 \\\\\n1 & 0 & 1 & 0 & 0 & 1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mu \\\\\nb_2 \\\\\nb_3 \\\\\ng_2 \\\\\ng_3 \\\\\ng_4 \\\\\n\\end{bmatrix} + \\boldsymbol{\\varepsilon}\n\\]\n\nBLUE(s):\n\n\\[\n\\small\n\\begin{align*}\n\\boldsymbol{\\beta} &=\n\\underbrace{\n\\left(\n\\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 0 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 1 & 0 \\\\\n1 & 1 & 0 & 0 & 1 & 0 \\\\\n1 & 0 & 1 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 0 & 0 & 1 \\\\\n1 & 1 & 0 & 0 & 0 & 1 \\\\\n1 & 0 & 1 & 0 & 0 & 1\n\\end{bmatrix}\n\\right)^{-1}\n}_{(\\boldsymbol{X'X})^{-1}}\n\\underbrace{\n\\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n7.4 \\\\\n6.5 \\\\\n5.6 \\\\\n9.8 \\\\\n6.8 \\\\\n6.2 \\\\\n7.3 \\\\\n6.1 \\\\\n6.4 \\\\\n9.5 \\\\\n8.0 \\\\\n7.4\n\\end{bmatrix}\n}_{\\boldsymbol{X'y}} \\\\\n&=\n\\underbrace{\n\\begin{bmatrix}\n\\frac{1}{2} & \\frac{-1}{4} & \\frac{-1}{4} & \\frac{-1}{3} & \\frac{-1}{3} & \\frac{-1}{3} \\\\\n\\frac{-1}{4} & \\frac{1}{2} & \\frac{1}{4} & 0 & 0 & 0 \\\\\n\\frac{-1}{4}  & \\frac{1}{4} & \\frac{1}{2} & 1 & 1 & 1 \\\\\n\\frac{-1}{3}  & 0 & 0 & \\frac{2}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\\n\\frac{-1}{3}  & 0 & 0 & \\frac{1}{3} & \\frac{2}{3} & \\frac{1}{3} \\\\\n\\frac{-1}{3}  & 0 & 0 & \\frac{1}{3} & \\frac{1}{3} & \\frac{2}{3}\n\\end{bmatrix}\n}_{(\\boldsymbol{X'X})^{-1}}\n\\underbrace{\n\\begin{bmatrix}\n87.0 \\\\\n27.4 \\\\\n25.6 \\\\\n22.8 \\\\\n19.8 \\\\\n24.9\n\\end{bmatrix}\n}_{\\boldsymbol{X'y}}\n=\n\\begin{bmatrix}\n7.75 \\\\\n-1.65 \\\\\n-2.10 \\\\\n1.10 \\\\\n0.10 \\\\\n1.80\n\\end{bmatrix}\n\\end{align*}\n\\]\n\n\n\nff &lt;- yield ~ 1 + block + gen\nm &lt;- model.frame(ff, data)\nX &lt;- model.matrix(ff, m)\ny &lt;- matrix(data$yield)\n\nXty &lt;- t(X) %*% y\nXtX &lt;- t(X) %*% X\nrank_X &lt;- qr(XtX)$rank\nXtX_inv &lt;- solve(XtX)\n\nbeta &lt;- XtX_inv %*% Xty\nSSE &lt;- t(y - X %*% beta) %*% (y - X %*% beta)\nSSE &lt;- as.numeric(SSE)\n\nlist(rank = rank_X, betas = drop(beta), SSE = SSE)\n\n$rank\n[1] 6\n\n$betas\n(Intercept)      block2      block3       geng2       geng3       geng4 \n       7.75       -1.65       -2.10        1.10        0.10        1.80 \n\n$SSE\n[1] 2.4",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html#reduction-in-ss-logic-and-anova-table-by-hand",
    "href": "pages/00_start_lm.html#reduction-in-ss-logic-and-anova-table-by-hand",
    "title": "Understanding a linear model",
    "section": "3) Reduction-in-SS logic and ANOVA table (by hand)",
    "text": "3) Reduction-in-SS logic and ANOVA table (by hand)\nThe partial sums of squares for block and gen can be obtained as reductions from the intercept-only model:\n\nSSE_mu               # intercept-only\n\n[1] 18.81\n\nSSE_mu - SSE_blk     # SS for blocks\n\n[1] 9.78\n\nSSE_mu - SSE_gen     # SS for genotypes\n\n[1] 6.63\n\nSSE                  # residual SS from full model\n\n[1] 2.4\n\n\nWe now assemble an ANOVA table. Residual df is n - p, where p is the number of coefficients in the full model.\n\n# Sigma^2 estimate using the full model (p = length(beta))\nsigma_2 &lt;- SSE / (n - length(beta))\nsigma_2\n\n[1] 0.4\n\nanova_dt &lt;- data.frame(\n  Source = c(\"block\", \"gen\", \"residuals\"),\n  Df     = c(n_blks - 1, n_gens - 1, n - length(beta)),\n  SSq    = c(SSE_mu - SSE_blk, SSE_mu - SSE_gen, SSE)\n) |&gt;\n  mutate(\n    MSq    = SSq / Df,\n    F.value = MSq / MSq[3],\n    `Pr(&gt;F)` = pf(q = F.value, df1 = Df, df2 = Df[3], lower.tail = FALSE),\n    F.value = ifelse(Source == \"residuals\", NA, F.value),\n    `Pr(&gt;F)` = ifelse(Source == \"residuals\", NA, `Pr(&gt;F)`)\n  )\n\nanova_dt\n\n     Source Df  SSq  MSq F.value      Pr(&gt;F)\n1     block  2 9.78 4.89  12.225 0.007650536\n2       gen  3 6.63 2.21   5.525 0.036730328\n3 residuals  6 2.40 0.40      NA          NA\n\n\n::: {.callout-note} Why does this work? In fixed-effects ANOVA, Type-I SS for adding a factor equals the reduction in SSE between nested models. Here we use the intercept-only model as the baseline; adding block or gen reduces SSE by their respective SS. :::",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html#verify-with-lm",
    "href": "pages/00_start_lm.html#verify-with-lm",
    "title": "Understanding a linear model",
    "section": "4) Verify with lm()",
    "text": "4) Verify with lm()\n\nmod &lt;- lm(yield ~ 1 + block + gen, data = data)\nsummary(mod)\n\n\nCall:\nlm(formula = yield ~ 1 + block + gen, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5500 -0.3625 -0.0500  0.1750  0.9500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   7.7500     0.4472  17.330 2.37e-06 ***\nblock2       -1.6500     0.4472  -3.690  0.01021 *  \nblock3       -2.1000     0.4472  -4.696  0.00334 ** \ngeng2         1.1000     0.5164   2.130  0.07719 .  \ngeng3         0.1000     0.5164   0.194  0.85284    \ngeng4         1.8000     0.5164   3.486  0.01305 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6325 on 6 degrees of freedom\nMultiple R-squared:  0.8724,    Adjusted R-squared:  0.7661 \nF-statistic: 8.205 on 5 and 6 DF,  p-value: 0.01173\n\ncoef(mod)\n\n(Intercept)      block2      block3       geng2       geng3       geng4 \n       7.75       -1.65       -2.10        1.10        0.10        1.80 \n\nanova(mod)\n\nAnalysis of Variance Table\n\nResponse: yield\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nblock      2   9.78    4.89  12.225 0.007651 **\ngen        3   6.63    2.21   5.525 0.036730 * \nResiduals  6   2.40    0.40                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nround(vcov(mod), 5)\n\n            (Intercept) block2 block3    geng2    geng3    geng4\n(Intercept)     0.20000   -0.1   -0.1 -0.13333 -0.13333 -0.13333\nblock2         -0.10000    0.2    0.1  0.00000  0.00000  0.00000\nblock3         -0.10000    0.1    0.2  0.00000  0.00000  0.00000\ngeng2          -0.13333    0.0    0.0  0.26667  0.13333  0.13333\ngeng3          -0.13333    0.0    0.0  0.13333  0.26667  0.13333\ngeng4          -0.13333    0.0    0.0  0.13333  0.13333  0.26667\n\n# Check that (X'X)^{-1} * sigma^2 matches vcov\n# Sigma^2 estimate using the full model (p = length(beta))\nsigma_2 &lt;- SSE / (n - length(beta))\nvcov_betas &lt;- XtX_inv * sigma_2\nround(vcov_betas, 5)\n\n            (Intercept) block2 block3    geng2    geng3    geng4\n(Intercept)     0.20000   -0.1   -0.1 -0.13333 -0.13333 -0.13333\nblock2         -0.10000    0.2    0.1  0.00000  0.00000  0.00000\nblock3         -0.10000    0.1    0.2  0.00000  0.00000  0.00000\ngeng2          -0.13333    0.0    0.0  0.26667  0.13333  0.13333\ngeng3          -0.13333    0.0    0.0  0.13333  0.26667  0.13333\ngeng4          -0.13333    0.0    0.0  0.13333  0.13333  0.26667",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html#interpreting-coefficients-under-treatment-coding",
    "href": "pages/00_start_lm.html#interpreting-coefficients-under-treatment-coding",
    "title": "Understanding a linear model",
    "section": "5) Interpreting coefficients under treatment coding",
    "text": "5) Interpreting coefficients under treatment coding\nWith block and gen as factors and an intercept present, R uses treatment (reference-cell) coding by default. The printed beta therefore contains:\n\nbeta[1]: the intercept (no longer overall mean)\nbeta[2:3]: effects for non-reference blocks\nbeta[4:6]: effects for non-reference genotypes\n\nYou can reconstruct overall mean, genotype cell means, and block means as follows.\n\n# Number of coefficients in full model\nn_coef &lt;- length(beta)\n\n# Overall mean reconstructed from coefficients\nmu_recon &lt;- beta[1] + sum(beta[2:3]) / n_blks + sum(beta[4:6]) / n_gens\n\n# Compare with the intercept-only estimate\nmu_recon - beta_mu[1]\n\n[1] 7.993606e-15\n\n\n\n5.1 Genotype means including the missing (reference) level\n\n# beta currently has: (Intercept), block2, block3, gen2, gen3, gen4\n# Create a named vector for gen effects including the reference level set to 0\nprint(beta)\n\n             [,1]\n(Intercept)  7.75\nblock2      -1.65\nblock3      -2.10\ngeng2        1.10\ngeng3        0.10\ngeng4        1.80\n\ngens &lt;- c(\"geng1\" = 0, beta[4:6, ])\n# Add back the intercept and average block effect\ngens &lt;- beta[1] + sum(beta[2:3]) / n_blks + gens\ngens\n\ngeng1 geng2 geng3 geng4 \n  6.5   7.6   6.6   8.3 \n\n\n\n\n5.2 Block means including the missing (reference) level\n\nprint(beta)\n\n             [,1]\n(Intercept)  7.75\nblock2      -1.65\nblock3      -2.10\ngeng2        1.10\ngeng3        0.10\ngeng4        1.80\n\nblks &lt;- c(\"block1\" = 0, beta[2:3, ])\nblks &lt;- beta[1] + sum(beta[4:6]) / n_gens + blks\nblks\n\nblock1 block2 block3 \n  8.50   6.85   6.40",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html#estimated-marginal-means-and-pairwise-comparisons",
    "href": "pages/00_start_lm.html#estimated-marginal-means-and-pairwise-comparisons",
    "title": "Understanding a linear model",
    "section": "6) Estimated marginal means and pairwise comparisons",
    "text": "6) Estimated marginal means and pairwise comparisons\nemmeans provides adjusted means (marginal over the other factors) and convenient contrasts.\n\n# Genotype adjusted means\nemm_gen &lt;- emmeans(mod, ~gen)\nemm_gen\n\n gen emmean    SE df lower.CL upper.CL\n g1     6.5 0.365  6     5.61     7.39\n g2     7.6 0.365  6     6.71     8.49\n g3     6.6 0.365  6     5.71     7.49\n g4     8.3 0.365  6     7.41     9.19\n\nResults are averaged over the levels of: block \nConfidence level used: 0.95 \n\n# Standard errors of genotype via (X'X)^{-1} * sigma2\nsqrt(diag(vcov_betas)[4:6])\n\n    geng2     geng3     geng4 \n0.5163978 0.5163978 0.5163978 \n\n# Pairwise genotype comparisons\npairs(emm_gen)\n\n contrast estimate    SE df t.ratio p.value\n g1 - g2      -1.1 0.516  6  -2.130  0.2447\n g1 - g3      -0.1 0.516  6  -0.194  0.9971\n g1 - g4      -1.8 0.516  6  -3.486  0.0486\n g2 - g3       1.0 0.516  6   1.936  0.3066\n g2 - g4      -0.7 0.516  6  -1.356  0.5656\n g3 - g4      -1.7 0.516  6  -3.292  0.0609\n\nResults are averaged over the levels of: block \nP value adjustment: tukey method for comparing a family of 4 estimates \n\n# For a quick hand-check of a simple pairwise SE when balanced:\nsqrt(sigma_2 / 3 + sigma_2 / 3)\n\n[1] 0.5163978\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNotice that the standard errors via \\((\\boldsymbol{X' X})^{-1}\\sigma^2\\) are different to the ones return by emmeans. Interestingly those from \\((\\boldsymbol{X' X})^{-1}\\sigma^2\\) look exactly like the pairwise genotype comparison Standard Errors. Why? We will see why in the next article.",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html#what-you-learned",
    "href": "pages/00_start_lm.html#what-you-learned",
    "title": "Understanding a linear model",
    "section": "7) What you learned",
    "text": "7) What you learned\n\nHow to construct \\(X\\), solve \\((X^\\top X)^{-1} X^\\top y\\), and compute SSE.\nHow reductions in SSE across nested models yield sums of squares for factors.\nHow to rebuild means for reference and non-reference levels under treatment coding.\nHow to validate results with lm() and extract adjusted means and pairwise tests using emmeans.",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html#reproducibility",
    "href": "pages/00_start_lm.html#reproducibility",
    "title": "Understanding a linear model",
    "section": "Reproducibility",
    "text": "Reproducibility\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] kableExtra_1.4.0 emmeans_1.10.7   lubridate_1.9.4  forcats_1.0.0   \n [5] stringr_1.5.1    dplyr_1.1.4      purrr_1.0.2      readr_2.1.5     \n [9] tidyr_1.3.1      tibble_3.3.0     ggplot2_3.5.2    tidyverse_2.0.0 \n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.6         sandwich_3.1-1     generics_0.1.4     xml2_1.3.7        \n [5] stringi_1.8.7      lattice_0.22-6     hms_1.1.3          digest_0.6.37     \n [9] magrittr_2.0.3     evaluate_1.0.3     grid_4.4.2         timechange_0.3.0  \n[13] estimability_1.5.1 RColorBrewer_1.1-3 mvtnorm_1.3-3      fastmap_1.2.0     \n[17] jsonlite_2.0.0     Matrix_1.7-1       survival_3.7-0     multcomp_1.4-28   \n[21] viridisLite_0.4.2  scales_1.4.0       TH.data_1.1-3      codetools_0.2-20  \n[25] cli_3.6.3          rlang_1.1.4        splines_4.4.2      withr_3.0.2       \n[29] yaml_2.3.10        tools_4.4.2        tzdb_0.4.0         coda_0.19-4.1     \n[33] vctrs_0.6.5        R6_2.6.1           zoo_1.8-14         lifecycle_1.0.4   \n[37] htmlwidgets_1.6.4  MASS_7.3-61        pkgconfig_2.0.3    pillar_1.11.0     \n[41] gtable_0.3.6       glue_1.8.0         systemfonts_1.2.3  xfun_0.51         \n[45] tidyselect_1.2.1   rstudioapi_0.17.1  knitr_1.49         xtable_1.8-4      \n[49] farver_2.1.2       htmltools_0.5.8.1  labeling_0.4.3     svglite_2.1.3     \n[53] rmarkdown_2.29     compiler_4.4.2    \n\n\n::: {.callout-tip} Common pitfalls - Mixing character vectors and factors: always coerce to factors for design matrices. - Forgetting that with an intercept, one level per factor is the reference (not shown explicitly in model.matrix()). - Using the wrong residual df: it is n - p where p is the number of estimated coefficients in the full model. :::",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html#reduction-in-ss-and-anova-table-by-hand",
    "href": "pages/00_start_lm.html#reduction-in-ss-and-anova-table-by-hand",
    "title": "Understanding a linear model",
    "section": "3) Reduction-in-SS and ANOVA table (by hand)",
    "text": "3) Reduction-in-SS and ANOVA table (by hand)\nThe partial sums of squares for block and gen can be obtained as reductions from the intercept-only model:\n\nSSE_mu               # intercept-only\n\n[1] 18.81\n\nSSE_mu - SSE_blk     # SS for blocks\n\n[1] 9.78\n\nSSE_mu - SSE_gen     # SS for genotypes\n\n[1] 6.63\n\nSSE                  # residual SS from full model\n\n[1] 2.4\n\n\nWe now assemble an ANOVA table. Residual df is n - p, where p is the number of coefficients in the full model.\n\n# Sigma^2 estimate using the full model (p = length(beta))\nsigma_2 &lt;- SSE / (n - length(beta))\nsigma_2\n\n[1] 0.4\n\nanova_dt &lt;- data.frame(\n  Source = c(\"block\", \"gen\", \"residuals\"),\n  Df     = c(n_blks - 1, n_gens - 1, n - length(beta)),\n  SSq    = c(SSE_mu - SSE_blk, SSE_mu - SSE_gen, SSE)\n) |&gt;\n  mutate(\n    MSq    = SSq / Df,\n    F.value = MSq / MSq[3],\n    `Pr(&gt;F)` = pf(q = F.value, df1 = Df, df2 = Df[3], lower.tail = FALSE),\n    F.value = ifelse(Source == \"residuals\", NA, F.value),\n    `Pr(&gt;F)` = ifelse(Source == \"residuals\", NA, `Pr(&gt;F)`)\n  )\n\nanova_dt\n\n     Source Df  SSq  MSq F.value      Pr(&gt;F)\n1     block  2 9.78 4.89  12.225 0.007650536\n2       gen  3 6.63 2.21   5.525 0.036730328\n3 residuals  6 2.40 0.40      NA          NA\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhy does this work? In fixed-effects ANOVA, Type-I SS for adding a factor equals the reduction in SSE between nested models. Here we use the intercept-only model as the baseline; adding block or gen reduces SSE by their respective SS.",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html#reduction-in-ss-and-anova-table",
    "href": "pages/00_start_lm.html#reduction-in-ss-and-anova-table",
    "title": "Understanding a linear model",
    "section": "3) Reduction-in-SS and ANOVA table",
    "text": "3) Reduction-in-SS and ANOVA table\nPartial Sum of Squares (SS) measures the unique contribution of a predictor variable to a model, accounting for all other variables. It is calculated by comparing the error sum of squares (SSE) of the model without the variable to the SSE of the model with it. A larger reduction in SSE indicates a greater unique contribution of that variable.\n\nsse_df &lt;- data.frame(\n  Model = factor(\n    c(\"Intercept\", \"Add Blocks\", \"Add Genotypes\", \"Full\"),\n    levels = c(\"Intercept\", \"Add Blocks\", \"Add Genotypes\", \"Full\")\n  ),\n  SS = c(SSE_mu, SSE_mu - SSE_blk, SSE_mu - SSE_gen, SSE)\n)\n\nggplot(sse_df, mapping = aes(x = Model, y = SS, group = 0)) +\n  geom_line() +\n  geom_point(size = 3, color = \"#00BFC4\") +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nWe now assemble an ANOVA table. Residual df is\n\nanova_dt &lt;- data.frame(\n  Source = c(\"block\", \"gen\", \"residuals\"), # factors included in the model\n  Df     = c(n_blks - 1, n_gens - 1, n - length(beta)), # `n - p`, where `p` is the number of coefficients\n  SSq    = c(SSE_mu - SSE_blk, SSE_mu - SSE_gen, SSE) # differences between baseline model and nested models\n) |&gt;\n  mutate(\n    MSq = SSq / Df,\n    F.value = MSq / MSq[3],\n    `Pr(&gt;F)` = pf(q = F.value, df1 = Df, df2 = Df[3], lower.tail = FALSE),\n    F.value = ifelse(Source == \"residuals\", NA, F.value),\n    `Pr(&gt;F)` = ifelse(Source == \"residuals\", NA, `Pr(&gt;F)`)\n  )\n\nanova_dt |&gt;\n  gt()\n\n\n\n\n\n\n\nSource\nDf\nSSq\nMSq\nF.value\nPr(&gt;F)\n\n\n\n\nblock\n2\n9.78\n4.89\n12.225\n0.007650536\n\n\ngen\n3\n6.63\n2.21\n5.525\n0.036730328\n\n\nresiduals\n6\n2.40\n0.40\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhy does this work? In fixed-effects ANOVA, Type-I SS for adding a factor equals the reduction in SSE between nested models. Here we use the intercept-only model as the baseline; adding block or gen reduces SSE by their respective SS.",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "pages/00_start_lm.html#what-we-learned",
    "href": "pages/00_start_lm.html#what-we-learned",
    "title": "Understanding a linear model",
    "section": "7) What we learned",
    "text": "7) What we learned\n\nHow to construct \\(X\\), solve \\((X^\\top X)^{-1} X^\\top y\\), and compute SSE.\nHow reductions in SSE across nested models yield sums of squares for factors.\nHow to rebuild means for reference and non-reference levels under treatment coding.\nHow to validate results with lm() and extract adjusted means and pairwise tests using emmeans.",
    "crumbs": [
      "Starting LM",
      "ANOVA and fixed effects"
    ]
  },
  {
    "objectID": "index.html#what-to-expect",
    "href": "index.html#what-to-expect",
    "title": "Home",
    "section": "",
    "text": "Hands-on examples: R scripts that you can run and adapt.\n\nStep-by-step math: showing how model equations connect to code.\n\nConceptual explanations: to make sense of sums of squares, design matrices, likelihood, variance components, mixed models equations, heritability, and more in the context of plant breeding.",
    "crumbs": [
      "Welcome"
    ]
  }
]