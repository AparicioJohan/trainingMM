---
title: "OLS via QR decomposition"
date: "last-modified"
echo: true 
warning: false 
message: false
---

## Data

We will analyze the same randomized complete block design (RCBD) (4 genotypes and 3 blocks).

```{r}
library(dplyr)
library(Matrix)
data <- read.csv("../data/example_1.csv") |>
  mutate(gen = as.factor(gen),
         block = as.factor(block))
head(data)
str(data)
```

## Model Matrix

We fit the additive model

$$\texttt{yield} \sim 1 + \texttt{block} + \texttt{gen}$$

```{r}
X <- model.matrix(yield ~ 1 + block + gen, data = data)
y <- data[["yield"]]
dim(X)
X[1:6, ]
y
```
## Coefficients via Normal Equations

The OLS solution solves $(X^\top X)\beta = X^\top y$.

```{r}
XtX <- crossprod(X)      # X'X
Xty <- crossprod(X, y)   # X'y
qr(XtX)$rank             # check rank of X'X
beta_normal <- solve(XtX, Xty)
beta_normal
```

## Coefficients via QR Decomposition

::: callout-important
The QR decomposition (also called the QR factorization) of a matrix is a decomposition
of the matrix into an orthogonal matrix and a triangular matrix. A QR decomposition of
a real square matrix $A$ is a decomposition of $A$ as $A = QR$, where $Q$ is an orthogonal
matrix (i.e. $Q^\top Q = I$) and $R$ is an upper triangular matrix. If $A$ is nonsingular,
then this factorization is unique. By Yanovsky (UCLA) (PDF) [link](https://www.math.ucla.edu/~yanovsky/Teaching/Math151B/handouts/GramSchmidt.pdf)
:::

We start by factoring the design matrix $X$, such that $$X = QR$$

Substituting this factorization into the normal equations, $$X^\top X \beta = X^\top y,$$
we obtain  $$(R^\top Q^\top)(Q R)\beta = R^\top Q^\top y$$
Because $Q^\top Q = I$, this simplifies to $$R^\top R \beta = R^\top Q^\top y$$
By pre-multiplying both sides by $(R^\top)^{-1}$, we get $$R \beta = Q^\top y$$
Finally, since $R$ is upper triangular, we can solve for $\beta$ efficiently using **back substitution**.

```{r}
qrX <- qr(X)
Q   <- qr.Q(qrX)
R   <- qr.R(qrX)
beta_qr <- backsolve(R, t(Q) %*% y)
beta_qr
```

::: columns
::: {.column width="49%"}
```{r}
round(Q, 3)
```
:::

::: {.column width="2%"}
:::

::: {.column width="49%"}
```{r}
round(R, 3)
```
:::
:::


## Check: Same Fit Either Way

```{r}
all.equal(as.vector(beta_normal), as.vector(beta_qr))
```

## Notes

* The **normal-equations** method uses an explicit $(X^\top X)^{-1}$; itâ€™s concise but can be numerically less stable if $X^\top X$ is ill-conditioned.
* The **QR** method avoids forming $(X^\top X)^{-1}$ directly and is typically more stable; most linear model software uses QR (or related decompositions) under the hood.

