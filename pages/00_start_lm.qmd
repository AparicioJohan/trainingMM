---
title: "Understanding a linear model"
date: "last-modified"
echo: true 
warning: false 
message: false
---

> With this article we want to analyze a randomized complete block design (RCBD) to show how a traditional full fixed effects model and ANOVA work. 

## 1) Setup & data

Clewer and Scarisbrick (2001) present a yield trial (t/ha) conducted using a randomized complete block design. The design included 3 blocks and 4 cultivars, resulting in 12 experimental plots.

```{r}
library(tidyverse)
library(emmeans)
library(gt)
library(kableExtra)

# Read and coerce factors
data <- read.csv("../data/example_1.csv") |>
  mutate(gen = as.factor(gen), block = as.factor(block))
head(data)
str(data)
```

### Quick visualization


```{r}
#| fig-cap: "Yield by genotype colored by block."
#| fig-width: 7
#| fig-height: 4

data |>
  ggplot(aes(x = gen, y = yield, color = block)) +
  geom_point(size = 3) +
  theme_classic(base_size = 15)
```

<!-- ### Descriptive means -->

<!-- ```{r} -->
<!-- # Mean by genotype -->
<!-- by_gen <- data |> -->
<!--   group_by(gen) |> -->
<!--   summarise(mean = mean(yield), .groups = "drop") -->
<!-- by_gen |> gt() -->
<!-- # Mean by block -->
<!-- by_blk <- data |> -->
<!--   group_by(block) |> -->
<!--   summarise(mean = mean(yield), .groups = "drop") -->
<!-- by_blk |> gt() -->
<!-- # Overall mean -->
<!-- overall <- data |> -->
<!--   summarise(mean = mean(yield)) -->
<!-- overall -->
<!-- ``` -->

## 2) Linear model building blocks

We will progressively build the RCBD model using `model.frame()` and `model.matrix()` to see the design matrices explicitly, then solve normal equations. We use:

-   $\boldsymbol{y} = \boldsymbol{X\beta} + \boldsymbol{\varepsilon}$: linear model
-   $\boldsymbol{y}$: response vector (yield)
-   $\boldsymbol{X}$: model matrix
-   $\boldsymbol{\varepsilon} \sim MVN(0, \boldsymbol{I}\sigma^2)$
-   $E(\boldsymbol{y}) = \boldsymbol{X\beta}; \; V(\boldsymbol{y}) = \boldsymbol{I}\sigma^2$
-   $\boldsymbol{\hat\beta} = (\boldsymbol{X^\top X})^{-1} \boldsymbol{X^\top} \boldsymbol{y}$
-   $V(\boldsymbol{\hat{\beta}}) = (\boldsymbol{X^\top X})^{-1} \sigma^2$
-   Fitted values $\boldsymbol{\hat y} = \boldsymbol{X\hat\beta}$
-   Errors $\boldsymbol{\varepsilon} = \boldsymbol{y} - \boldsymbol{\hat y}$
-   Sum of squared errors $\text{SSE} = \boldsymbol{\varepsilon^\top \varepsilon}$

Let `n = 12` observations, `n_blks = 3` blocks, `n_gens = 4` genotypes.

```{r}
n <- 12
n_blks <- 3
n_gens <- 4
```

### 2.1 Intercept-only model (overall mean)

::: {.panel-tabset}

# Matrices

-   Model:

$$
\small
\boldsymbol{y} = \boldsymbol{X\beta} + \boldsymbol{\varepsilon} => 
\begin{bmatrix}
7.4 \\
6.5 \\
5.6 \\
9.8 \\
6.8 \\
6.2 \\
7.3 \\
6.1 \\
6.4 \\
9.5 \\
8.0 \\
7.4 \\
\end{bmatrix} = 
\overset{\text{mean}}{
\begin{bmatrix}
1 \\
1 \\
1 \\
1 \\
1 \\
1 \\
1 \\
1 \\
1 \\
1 \\
1 \\
1
\end{bmatrix}}
\mu + \boldsymbol{\varepsilon}
$$

-   BLUE(s): 

$$
\small
\boldsymbol{\beta} = 
\underbrace{\left(\begin{bmatrix}
1 \
1 \
1 \
1 \
1 \
1 \
1 \
1 \
1 \
1 \
1 \
1 \
\end{bmatrix}
\begin{bmatrix}
1 \\
1 \\
1 \\
1 \\
1 \\
1 \\
1 \\
1 \\
1 \\
1 \\
1 \\
1 \\
\end{bmatrix}\right)^{-1}}_{(\boldsymbol{X'X})^{-1}} 
\underbrace{\begin{bmatrix}
1 \
1 \
1 \
1 \
1 \
1 \
1 \
1 \
1 \
1 \
1 \
1 \
\end{bmatrix}
\begin{bmatrix}
7.4 \\
6.5 \\
5.6 \\
9.8 \\
6.8 \\
6.2 \\
7.3 \\
6.1 \\
6.4 \\
9.5 \\
8.0 \\
7.4 \\
\end{bmatrix}}_{\boldsymbol{X'y}} = 
\underbrace{\frac{1}{12}}_{(\boldsymbol{X'X})^{-1}}\cdot \underbrace{87}_{\boldsymbol{X'y}} = 7.25
$$

::: callout-note
Notice that the first term, $\left(\boldsymbol{X'X}\right)^{-1}$, corresponds to the inverse of
number of observations, while the second term, $\boldsymbol{X'y}$, 
gives the sum of phenotypic values. 
:::

In this example, there is only a single level, $\mu$. 
Therefore, the entire expression simplifies to the sum of the phenotypic 
values divided by the number of observations, which is simply the mean, as shown below:

```{r}
data.frame(mean = "mean", beta = mean(data$yield)) |>
  gt()
```


# Code

```{r}
ff <- yield ~ 1
m <- model.frame(ff, data)
X <- model.matrix(ff, m)
y <- matrix(data$yield)

# Normal equations components
Xty <- t(X) %*% y
XtX <- t(X) %*% X
rank_X <- qr(XtX)$rank
XtX_inv <- solve(XtX)

beta_mu <- XtX_inv %*% Xty # overall mean (mu)
y_hat <- X %*% beta_mu
errors <- y - y_hat
SSE_mu <- t(errors) %*% errors
SSE_mu <- as.numeric(SSE_mu)

list(rank = rank_X, beta_mu = drop(beta_mu), SSE_mu = SSE_mu)
```

:::

### 2.2 Add blocks

We will now illustrate what happens if we only include the block factor.

::: {.panel-tabset}

# Matrices

-   Model: 
$$
\small
\boldsymbol{y} = \boldsymbol{X\beta} + \boldsymbol{\varepsilon} => 
\begin{bmatrix}
7.4 \\
6.5 \\
5.6 \\
9.8 \\
6.8 \\
6.2 \\
7.3 \\
6.1 \\
6.4 \\
9.5 \\
8.0 \\
7.4 \\
\end{bmatrix} = 
\begin{bmatrix}
\overset{\text{Block 1}}{1} & \overset{\text{Block 2}}{0} & \overset{\text{Block 3}}{0} \\
0 & 1 & 0 \\
0 & 0 & 1 \\
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
b_1 \\
b_2 \\
b_3 \\
\end{bmatrix} + \boldsymbol{\varepsilon}
$$

- BLUE(s): 
$$
\small
\boldsymbol{\beta} = 
\underbrace{\left(\begin{bmatrix}
1 \ 0 \ 0 \ 1 \ 0 \ 0 \ 1 \ 0 \ 0 \ 1 \ 0 \ 0 \\
0 \ 1 \ 0 \ 0 \ 1 \ 0 \ 0 \ 1 \ 0 \ 1 \ 0 \ 0 \\
0 \ 0 \ 1 \ 0 \ 0 \ 1 \ 0 \ 0 \ 1 \ 1 \ 0 \ 0 \\
\end{bmatrix}
\begin{bmatrix}
1 \ 0 \ 0 \\
0 \ 1 \ 0 \\
0 \ 0 \ 1 \\
1 \ 0 \ 0 \\
0 \ 1 \ 0 \\
0 \ 0 \ 1 \\
1 \ 0 \ 0 \\
0 \ 1 \ 0 \\
0 \ 0 \ 1 \\
1 \ 0 \ 0 \\
0 \ 1 \ 0 \\
0 \ 0 \ 1 \\
\end{bmatrix}\right)^{-1}}_{(\boldsymbol{X'X})^{-1}} 
\underbrace{\begin{bmatrix}
1 \ 0 \ 0 \ 1 \ 0 \ 0 \ 1 \ 0 \ 0 \ 1 \ 0 \ 0 \\
0 \ 1 \ 0 \ 0 \ 1 \ 0 \ 0 \ 1 \ 0 \ 1 \ 0 \ 0 \\
0 \ 0 \ 1 \ 0 \ 0 \ 1 \ 0 \ 0 \ 1 \ 1 \ 0 \ 0 \\
\end{bmatrix}
\begin{bmatrix}
7.4 \\
6.5 \\
5.6 \\
9.8 \\
6.8 \\
6.2 \\
7.3 \\
6.1 \\
6.4 \\
9.5 \\
8.0 \\
7.4 \\
\end{bmatrix}}_{\boldsymbol{X'y}} = 
\underbrace{\begin{bmatrix}
\frac{1}{4} \ 0 \ 0 \\
0 \ \frac{1}{4} \ 0 \\
0 \ 0 \ \frac{1}{4} \\
\end{bmatrix}}_{(\boldsymbol{X'X})^{-1}}
\underbrace{\begin{bmatrix}
34 \\
27.4 \\
25.6 \\
\end{bmatrix}}_{\boldsymbol{X'y}} = 
\begin{bmatrix}
8.50 \\
6.85 \\
6.40 \\
\end{bmatrix}
$$

In this model, there are 3 levels ($Block1$, $Block2$ and $Block3$). 
Therefore, the entire expression simplifies to the sum of the phenotypic 
values divided by the number of observations, which is simply the mean per block.

```{r}
data |>
  group_by(block) |>
  summarise(beta = mean(yield, na.rm = TRUE)) |>
  gt()
```


# Code

```{r}
ff <- yield ~ -1 + block
m <- model.frame(ff, data)
X <- model.matrix(ff, m)
y <- matrix(data$yield)

Xty <- t(X) %*% y
XtX <- t(X) %*% X
rank_X <- qr(XtX)$rank
XtX_inv <- solve(XtX)

beta_blk <- XtX_inv %*% Xty
SSE_blk <- t(y - X %*% beta_blk) %*% (y - X %*% beta_blk)
SSE_blk <- as.numeric(SSE_blk)

list(rank = rank_X, beta_blk = drop(beta_blk), SSE_blk = SSE_blk)
```

:::

### 2.3 Add genotypes

::: {.panel-tabset}

# Matrices

-   Model: 

$$
\small
\boldsymbol{y} = \boldsymbol{X\beta} + \boldsymbol{\varepsilon} => 
\begin{bmatrix}
7.4 \\
6.5 \\
5.6 \\
9.8 \\
6.8 \\
6.2 \\
7.3 \\
6.1 \\
6.4 \\
9.5 \\
8.0 \\
7.4 \\
\end{bmatrix} = 
\begin{bmatrix}
\overset{gen1}{1} & \overset{gen2}{0} & \overset{gen3}{0} & \overset{gen4}{0} \\
1 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
g_1 \\
g_2 \\
g_3 \\
g_4 \\
\end{bmatrix} + \boldsymbol{\varepsilon}
$$

-   BLUE(s):


$$
\small
\boldsymbol{\beta} = 
\underbrace{
\left(\begin{bmatrix}
1 \ 1 \ 1 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \\
0 \ 0 \ 0 \ 1 \ 1 \ 1 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \\
0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 1 \ 1 \ 1 \ 0 \ 0 \ 0 \\
0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 1 \ 1 \ 1 \\
\end{bmatrix}
\begin{bmatrix}
1 \ 0 \ 0 \ 0 \\
1 \ 0 \ 0 \ 0 \\
1 \ 0 \ 0 \ 0 \\
0 \ 1 \ 0 \ 0 \\
0 \ 1 \ 0 \ 0 \\
0 \ 1 \ 0 \ 0 \\
0 \ 0 \ 1 \ 0 \\
0 \ 0 \ 1 \ 0 \\
0 \ 0 \ 1 \ 0 \\
0 \ 0 \ 0 \ 1 \\
0 \ 0 \ 0 \ 1 \\
0 \ 0 \ 0 \ 1 \\
\end{bmatrix}\right)^{-1}}_{(\boldsymbol{X'X})^{-1}}
\underbrace{
\begin{bmatrix}
1 \ 1 \ 1 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \\
0 \ 0 \ 0 \ 1 \ 1 \ 1 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \\
0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 1 \ 1 \ 1 \ 0 \ 0 \ 0 \\
0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 1 \ 1 \ 1 \\
\end{bmatrix}
\begin{bmatrix}
7.4 \\
6.5 \\
5.6 \\
9.8 \\
6.8 \\
6.2 \\
7.3 \\
6.1 \\
6.4 \\
9.5 \\
8.0 \\
7.4 \\
\end{bmatrix}}_{\boldsymbol{X'y}} = 
\underbrace{\begin{bmatrix}
\frac{1}{3}\ 0 \ 0 \ 0 \\ 
0 \ \frac{1}{3} \ 0 \ 0 \\
0 \ 0 \ \frac{1}{3} \ 0 \\
0 \ 0 \ 0 \ \frac{1}{3} \\
\end{bmatrix}}_{(\boldsymbol{X'X})^{-1}}
\underbrace{
\begin{bmatrix}
19.5 \\
22.8 \\
19.8 \\
24.9 \\
\end{bmatrix}
}_{\boldsymbol{X'y}} = 
\begin{bmatrix}
6.5 \\
7.6 \\
6.6 \\
8.3 \\
\end{bmatrix}
$$

In this model, there are 4 levels ($g1$, $g2$, $g3$ and $g4$). 
Therefore, the entire expression simplifies to the sum of the phenotypic 
values divided by the number of observations, which is simply the mean per genotype.

```{r}
data |>
  group_by(gen) |>
  summarise(beta = mean(yield, na.rm = TRUE)) |>
  gt()
```

# Code

```{r}
ff <- yield ~ -1 + gen
m <- model.frame(ff, data)
X <- model.matrix(ff, m)
y <- matrix(data$yield)

Xty <- t(X) %*% y
XtX <- t(X) %*% X
rank_X <- qr(XtX)$rank
XtX_inv <- solve(XtX)

beta_gen <- XtX_inv %*% Xty
SSE_gen <- t(y - X %*% beta_gen) %*% (y - X %*% beta_gen)
SSE_gen <- as.numeric(SSE_gen)

m2 <- list(rank = rank_X, beta_gen = drop(beta_gen), SSE_gen = SSE_gen)
```

:::


### 2.4 Full model: intercept + blocks + genotypes

::: {.panel-tabset}

# Matrices 

-   Model: 

$$
\small
\boldsymbol{y} = \boldsymbol{X\beta} + \boldsymbol{\varepsilon} \;\;\;\Rightarrow\;\;\;
\begin{bmatrix}
7.4 \\
6.5 \\
5.6 \\
9.8 \\
6.8 \\
6.2 \\
7.3 \\
6.1 \\
6.4 \\
9.5 \\
8.0 \\
7.4 \\
\end{bmatrix}
=
\begin{bmatrix}
\overset{\text{intercept}}{1} & 
\overset{\text{block2}}{0} & 
\overset{\text{block3}}{0} & 
\overset{\text{gen2}}{0} & 
\overset{\text{gen3}}{0} & 
\overset{\text{gen4}}{0} \\
1 & 1 & 0 & 0 & 0 & 0 \\
1 & 0 & 1 & 0 & 0 & 0 \\
1 & 0 & 0 & 1 & 0 & 0 \\
1 & 1 & 0 & 1 & 0 & 0 \\
1 & 0 & 1 & 1 & 0 & 0 \\
1 & 0 & 0 & 0 & 1 & 0 \\
1 & 1 & 0 & 0 & 1 & 0 \\
1 & 0 & 1 & 0 & 1 & 0 \\
1 & 0 & 0 & 0 & 0 & 1 \\
1 & 1 & 0 & 0 & 0 & 1 \\
1 & 0 & 1 & 0 & 0 & 1 \\
\end{bmatrix}
\begin{bmatrix}
\mu \\
b_2 \\
b_3 \\
g_2 \\
g_3 \\
g_4 \\
\end{bmatrix} + \boldsymbol{\varepsilon}
$$

-   BLUE(s):

$$
\small
\begin{align*}
\boldsymbol{\beta} &= 
\underbrace{
\left(
\begin{bmatrix}
1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\
0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 \\
0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1
\end{bmatrix}
\begin{bmatrix}
1 & 0 & 0 & 0 & 0 & 0 \\
1 & 1 & 0 & 0 & 0 & 0 \\
1 & 0 & 1 & 0 & 0 & 0 \\
1 & 0 & 0 & 1 & 0 & 0 \\
1 & 1 & 0 & 1 & 0 & 0 \\
1 & 0 & 1 & 1 & 0 & 0 \\
1 & 0 & 0 & 0 & 1 & 0 \\
1 & 1 & 0 & 0 & 1 & 0 \\
1 & 0 & 1 & 0 & 1 & 0 \\
1 & 0 & 0 & 0 & 0 & 1 \\
1 & 1 & 0 & 0 & 0 & 1 \\
1 & 0 & 1 & 0 & 0 & 1
\end{bmatrix}
\right)^{-1}
}_{(\boldsymbol{X'X})^{-1}}
\underbrace{
\begin{bmatrix}
1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\
0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 \\
0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1
\end{bmatrix}
\begin{bmatrix}
7.4 \\
6.5 \\
5.6 \\
9.8 \\
6.8 \\
6.2 \\
7.3 \\
6.1 \\
6.4 \\
9.5 \\
8.0 \\
7.4
\end{bmatrix}
}_{\boldsymbol{X'y}} \\
&= 
\underbrace{
\begin{bmatrix}
\frac{1}{2} & \frac{-1}{4} & \frac{-1}{4} & \frac{-1}{3} & \frac{-1}{3} & \frac{-1}{3} \\
\frac{-1}{4} & \frac{1}{2} & \frac{1}{4} & 0 & 0 & 0 \\
\frac{-1}{4}  & \frac{1}{4} & \frac{1}{2} & 1 & 1 & 1 \\
\frac{-1}{3}  & 0 & 0 & \frac{2}{3} & \frac{1}{3} & \frac{1}{3} \\
\frac{-1}{3}  & 0 & 0 & \frac{1}{3} & \frac{2}{3} & \frac{1}{3} \\
\frac{-1}{3}  & 0 & 0 & \frac{1}{3} & \frac{1}{3} & \frac{2}{3}
\end{bmatrix}
}_{(\boldsymbol{X'X})^{-1}}
\underbrace{
\begin{bmatrix}
87.0 \\
27.4 \\
25.6 \\
22.8 \\
19.8 \\
24.9
\end{bmatrix}
}_{\boldsymbol{X'y}}
= 
\begin{bmatrix}
7.75 \\
-1.65 \\
-2.10 \\
1.10 \\
0.10 \\
1.80
\end{bmatrix}
\end{align*}
$$

# Code


```{r}
ff <- yield ~ 1 + block + gen
m <- model.frame(ff, data)
X <- model.matrix(ff, m)
y <- matrix(data$yield)

Xty <- t(X) %*% y
XtX <- t(X) %*% X
rank_X <- qr(XtX)$rank
XtX_inv <- solve(XtX)

beta <- XtX_inv %*% Xty
SSE <- t(y - X %*% beta) %*% (y - X %*% beta)
SSE <- as.numeric(SSE)

list(rank = rank_X, betas = drop(beta), SSE = SSE)
```

:::

Notice that the first level of each factor (baseline) is dropped in order to have a full rank $\boldsymbol{X}$ matrix.

### 2.5 Coefficients and variance-covariances

$$ \boldsymbol{\hat\beta} = (\boldsymbol{X^\top X})^{-1} \boldsymbol{X^\top} \boldsymbol{y} $$
$$V(\boldsymbol{\hat{\beta}}) = (\boldsymbol{X^\top X})^{-1} \sigma^2$$
 
```{r}
sigma_2 <- SSE / (n - length(beta))
vcov_betas <- XtX_inv * sigma_2
vcov_betas
```
```{r}
data.frame(
  coefficient = rownames(vcov_betas),
  solution = beta,
  variance = diag(vcov_betas),
  std.error = sqrt(diag(vcov_betas)),
  row.names = NULL
) |> gt()
```

::: callout-note
Let's be careful when interpreting the coefficients and the variance of these factor coefficients. In a linear model, when you include a categorical factor like "genotype", the model needs a baseline. In R it chooses g1 as the reference. Therefore:

* The coefficient for g2 is the estimated difference between g2 and g1 (7.6 - 6.5 = 1.1).
* The variance for the g2 coefficient is the uncertainty around that difference (g2 - g1).
:::


## 3) Reduction-in-SS and ANOVA table

Partial Sum of Squares (SS) measures the unique contribution of a predictor variable to a model, accounting for all other variables. It is calculated by comparing the error sum of squares (SSE) of the model without the variable to the SSE of the model with it. A larger reduction in SSE indicates a greater unique contribution of that variable.     

```{r, fig.width=8, fig.height=4}
sse_df <- data.frame(
  Model = factor(
    c("Intercept", "Add Blocks", "Add Genotypes", "Full"),
    levels = c("Intercept", "Add Blocks", "Add Genotypes", "Full")
  ),
  SS = c(SSE_mu, SSE_mu - SSE_blk, SSE_mu - SSE_gen, SSE)
)

ggplot(sse_df, mapping = aes(x = Model, y = SS, group = 0)) +
  geom_line() +
  geom_point(size = 3, color = "#00BFC4") +
  theme_minimal(base_size = 14)
```

We now assemble an ANOVA table. Residual df is 

```{r}
anova_dt <- data.frame(
  Source = c("block", "gen", "residuals"), # factors included in the model
  Df     = c(n_blks - 1, n_gens - 1, n - length(beta)), # `n - p`, where `p` is the number of coefficients
  SSq    = c(SSE_mu - SSE_blk, SSE_mu - SSE_gen, SSE) # differences between baseline model and nested models
) |>
  mutate(
    MSq = SSq / Df,
    F.value = MSq / MSq[3],
    `Pr(>F)` = pf(q = F.value, df1 = Df, df2 = Df[3], lower.tail = FALSE),
    F.value = ifelse(Source == "residuals", NA, F.value),
    `Pr(>F)` = ifelse(Source == "residuals", NA, `Pr(>F)`)
  )

anova_dt |>
  gt()
```

::: callout-note
**Why does this work?** In fixed-effects ANOVA, Type-I SS for adding a factor equals the reduction in SSE between nested models. Here we use the intercept-only model as the baseline; adding `block` or `gen` reduces SSE by their respective SS.
:::

## 4) Verify with `lm()`

```{r}
mod <- lm(yield ~ 1 + block + gen, data = data)
summary(mod)
coef(mod)
anova(mod)
round(vcov(mod), 5)

# Check that (X'X)^{-1} * sigma^2 matches vcov
round(vcov_betas, 5)
```

## 5) Reconstructing means

With `block` and `gen` as factors and an intercept present, R uses treatment (baseline) coding by default. The printed `beta` therefore contains:

-   `beta[1]`: the intercept (no longer overall mean)
-   `beta[2:3]`: effects for non-reference blocks
-   `beta[4:6]`: effects for non-reference genotypes

You can reconstruct **overall mean**, **genotype means**, and **block means** as follows.

```{r}
# Number of coefficients in full model
n_coef <- length(beta)

# Overall mean reconstructed from coefficients
mu_recon <- beta[1] + sum(beta[2:3]) / n_blks + sum(beta[4:6]) / n_gens

# Compare with the intercept-only estimate
mu_recon - beta_mu[1]
```

### 5.1 Genotype means including the missing (baseline) level

```{r}
# beta currently has: (Intercept), block2, block3, gen2, gen3, gen4
# Create a named vector for gen effects including the reference level set to 0
print(beta)
gens <- c("geng1" = 0, beta[4:6, ])
# Add back the intercept and average block effect
gens <- beta[1] + sum(beta[2:3]) / n_blks + gens
gens
```

### 5.2 Block means including the missing (baseline) level

```{r}
print(beta)
blks <- c("block1" = 0, beta[2:3, ])
blks <- beta[1] + sum(beta[4:6]) / n_gens + blks
blks
```

## 6) Estimated marginal means and pairwise comparisons

`emmeans` provides adjusted means (marginal over the other factors) and convenient contrasts.

```{r}
# Genotype adjusted means
emm_gen <- emmeans(mod, ~gen)
emm_gen

# Pairwise genotype comparisons
pairs(emm_gen)

# For a quick hand-check of a simple pairwise SE when balanced:
sqrt(sigma_2 / 3 + sigma_2 / 3)
```

::: callout-important
Notice that the standard errors via $(\boldsymbol{X' X})^{-1}\sigma^2$ are different to the ones return by `emmeans`, as expected.
:::
